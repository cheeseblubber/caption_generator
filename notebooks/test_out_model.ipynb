{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/workspace/ImageCaptioning.pytorch\")\n",
    "sys.path.append(\"/home/ubuntu/workspace/fastai\")\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai import *\n",
    "# import fastai *\n",
    "# from fastai.rnn_reg import *\n",
    "# from fastai.rnn_train import *\n",
    "# from fastai.nlp import *\n",
    "# from fastai.lm_rnn import *\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import dill as pickle\n",
    "import torch\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "import misc\n",
    "import pandas as pd\n",
    "\n",
    "from models import *\n",
    "from dataloaderraw import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('/home/ubuntu/workspace/FC/fc-model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "    def get(self, ele, default):\n",
    "        return self.__dict__.get(ele, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_path = '/home/ubuntu/workspace/ImageCaptioning.pytorch/data/asin_to_captions.json'\n",
    "import json\n",
    "file = open(label_path, encoding='ISO-8859-1')\n",
    "json_data = json.load(file)\n",
    "# pd.DataFrame(json_data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = set()\n",
    "a.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_seen = 5\n",
    "\n",
    "for key, label in json_data.items():\n",
    "    json_data[key] = label + \" <EOS>\"\n",
    "values = json_data.values()\n",
    "\n",
    "values = \" \".join(list(values))\n",
    "word_counter = {}\n",
    "\n",
    "words = values.split(\" \")\n",
    "for w in set(words):\n",
    "    if w == '':\n",
    "        next\n",
    "    word_counter[w] = 0\n",
    "    \n",
    "include_words = set()\n",
    "for word in words:\n",
    "    word_counter[word] += 1\n",
    "    if word_counter[word] > min_seen:\n",
    "        include_words.add(word)\n",
    "# idx_to_word = { for i, w in  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13015"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(include_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_to_word = {i: w for i, w in enumerate(include_words)}\n",
    "word_to_idx = {w: i for i, w in enumerate(include_words)}\n",
    "max_words = 15\n",
    "def label_to_i(word_to_idx, label):\n",
    "    indexes = []\n",
    "    for word in label.split(\" \"):\n",
    "        unk = '<UNKNOWN>'\n",
    "        index = word_to_idx.get(word, unk)\n",
    "        if index != unk:\n",
    "            indexes.append(index)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_json = {}\n",
    "for asin, label in json_data.items():\n",
    "    label_json[asin] = label_to_i(word_to_idx, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longest = 0\n",
    "longest_i = 0\n",
    "for k, v in label_json.items():\n",
    "    if len(v) > longest:\n",
    "        longest_i = k\n",
    "        longest = len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Namespace(\n",
    "    label_json=label_json,\n",
    "    idx_to_word=idx_to_word,\n",
    "    word_to_idx=word_to_idx,\n",
    "    att_feat_size=2048, \n",
    "    att_hid_size=512, \n",
    "    batch_size=batch_size, \n",
    "    beam_size=2, \n",
    "    caption_model='att2in', \n",
    "    checkpoint_path='log_a2i', \n",
    "    cnn_model='resnet101', \n",
    "    coco_json='', \n",
    "    current_lr=0.00016384000000000006, \n",
    "    drop_prob_lm=0.5, \n",
    "    dump_images=1, \n",
    "    dump_json=1, \n",
    "    dump_path=0, \n",
    "    labels_path=label_path,\n",
    "    fc_feat_size=2048, \n",
    "    grad_clip=0.1, \n",
    "    id='a2i', \n",
    "    image_folder='/home/ubuntu/workspace/ImageCaptioning.pytorch/images/', \n",
    "    folder_path='/home/ubuntu/workspace/ImageCaptioning.pytorch/images/',\n",
    "    valid_path='/home/ubuntu/workspace/ImageCaptioning.pytorch/valid/',\n",
    "    image_root='', \n",
    "    infos_path='../att2in/infos_a2i-best.pkl', \n",
    "    input_att_dir='data/cocotalk_att', \n",
    "    input_encoding_size=512, \n",
    "    input_fc_dir='data/cocotalk_fc', \n",
    "    input_json='data/cocotalk.json', \n",
    "    input_label_h5='data/cocotalk_label.h5', \n",
    "    language_eval=0, learning_rate=0.0005, \n",
    "    learning_rate_decay_every=3, \n",
    "    learning_rate_decay_rate=0.8, \n",
    "    learning_rate_decay_start=0, \n",
    "    load_best_score=1, \n",
    "    losses_log_every=25, \n",
    "    max_epochs=25, \n",
    "    model='../att2in/model-best.pth', \n",
    "    num_images=2, \n",
    "    num_layers=1, \n",
    "    optim='adam', \n",
    "    optim_alpha=0.9, \n",
    "    optim_beta=0.999, \n",
    "    optim_epsilon=1e-08, \n",
    "    rnn_size=512, \n",
    "    rnn_type='lstm', \n",
    "    sample_max=1, \n",
    "    save_checkpoint_every=3000, \n",
    "    scheduled_sampling_increase_every=5, \n",
    "    scheduled_sampling_increase_prob=0.05, \n",
    "    scheduled_sampling_max_prob=0.25, \n",
    "    scheduled_sampling_start=0, \n",
    "    self_critical_after=-1, \n",
    "    seq_length=30, \n",
    "    seq_per_img=5,\n",
    "    split='test',\n",
    "    ss_prob=0.15000000000000002,\n",
    "    temperature=1.0,\n",
    "    train_only=0,\n",
    "    val_images_use=5000,\n",
    "    vocab_size=len(include_words),\n",
    "    weight_decay=0,\n",
    "    start_from=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCModel(\n",
       "  (img_embed): Linear(in_features=2048, out_features=512)\n",
       "  (core): LSTMCore(\n",
       "    (i2h): Linear(in_features=512, out_features=2560)\n",
       "    (h2h): Linear(in_features=512, out_features=2560)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (embed): Embedding(13016, 512)\n",
       "  (logit): Linear(in_features=512, out_features=13016)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FCModel(opt)\n",
    "# model.load_state_dict(torch.load(os.path.join('/home/ubuntu/workspace/FC/fc-model.pth')))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaderRaw loading images from folder:  /home/ubuntu/workspace/ImageCaptioning.pytorch/images/\n",
      "0\n",
      "listing all images in directory /home/ubuntu/workspace/ImageCaptioning.pytorch/images/\n",
      "DataLoaderRaw found  80088  images\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoaderRaw(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not installed; No tensorboard logging.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "\n",
    "import opts\n",
    "import models\n",
    "from dataloader import *\n",
    "import eval_utils\n",
    "import misc.utils as utils\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    print(\"Tensorflow not installed; No tensorboard logging.\")\n",
    "    tf = None\n",
    "\n",
    "def add_summary_value(writer, key, value, iteration):\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag=key, simple_value=value)])\n",
    "    writer.add_summary(summary, iteration)\n",
    "\n",
    "def train(opt):\n",
    "    opt.use_att = utils.if_use_att(opt.caption_model)\n",
    "    loader = DataLoaderRaw(opt)\n",
    "#     opt.vocab_size = loader.vocab_size\n",
    "#     opt.seq_length = loader.seq_length\n",
    "\n",
    "    tf_summary_writer = tf and tf.summary.FileWriter(opt.checkpoint_path)\n",
    "\n",
    "    infos = {}\n",
    "    histories = {}\n",
    "    if opt.start_from is not None:\n",
    "        # open old infos and check if models are compatible\n",
    "        with open(os.path.join(opt.start_from, 'infos_'+opt.id+'.pkl')) as f:\n",
    "            infos = cPickle.load(f)\n",
    "            saved_model_opt = infos['opt']\n",
    "            need_be_same=[\"caption_model\", \"rnn_type\", \"rnn_size\", \"num_layers\"]\n",
    "            for checkme in need_be_same:\n",
    "                assert vars(saved_model_opt)[checkme] == vars(opt)[checkme], \"Command line argument and saved model disagree on '%s' \" % checkme\n",
    "\n",
    "        if os.path.isfile(os.path.join(opt.start_from, 'histories_'+opt.id+'.pkl')):\n",
    "            with open(os.path.join(opt.start_from, 'histories_'+opt.id+'.pkl')) as f:\n",
    "                histories = cPickle.load(f)\n",
    "\n",
    "    iteration = infos.get('iter', 0)\n",
    "    epoch = infos.get('epoch', 0)\n",
    "\n",
    "    val_result_history = histories.get('val_result_history', {})\n",
    "    loss_history = histories.get('loss_history', {})\n",
    "    lr_history = histories.get('lr_history', {})\n",
    "    ss_prob_history = histories.get('ss_prob_history', {})\n",
    "\n",
    "#     loader.iterators = infos.get('iterators', loader.iterators)\n",
    "#     loader.split_ix = infos.get('split_ix', loader.split_ix)\n",
    "    if opt.load_best_score == 1:\n",
    "        best_val_score = infos.get('best_val_score', None)\n",
    "\n",
    "    model = models.setup(opt)\n",
    "    model.cuda()\n",
    "\n",
    "    update_lr_flag = True\n",
    "    # Assure in training mode\n",
    "    model.train()\n",
    "\n",
    "    crit = utils.LanguageModelCriterion()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=opt.learning_rate, weight_decay=opt.weight_decay)\n",
    "\n",
    "    # Load the optimizer\n",
    "    if vars(opt).get('start_from', None) is not None:\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(opt.start_from, 'optimizer.pth')))\n",
    "\n",
    "    while True:\n",
    "        if update_lr_flag:\n",
    "                # Assign the learning rate\n",
    "            if epoch > opt.learning_rate_decay_start and opt.learning_rate_decay_start >= 0:\n",
    "                frac = (epoch - opt.learning_rate_decay_start) // opt.learning_rate_decay_every\n",
    "                decay_factor = opt.learning_rate_decay_rate  ** frac\n",
    "                opt.current_lr = opt.learning_rate * decay_factor\n",
    "                utils.set_lr(optimizer, opt.current_lr) # set the decayed rate\n",
    "            else:\n",
    "                opt.current_lr = opt.learning_rate\n",
    "            # Assign the scheduled sampling prob\n",
    "            if epoch > opt.scheduled_sampling_start and opt.scheduled_sampling_start >= 0:\n",
    "                frac = (epoch - opt.scheduled_sampling_start) // opt.scheduled_sampling_increase_every\n",
    "                opt.ss_prob = min(opt.scheduled_sampling_increase_prob  * frac, opt.scheduled_sampling_max_prob)\n",
    "                model.ss_prob = opt.ss_prob\n",
    "            update_lr_flag = False\n",
    "                \n",
    "        start = time.time()\n",
    "        # Load data from train split (0)\n",
    "        data = loader.get_batch('train')\n",
    "        print('Read data:', time.time() - start)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        tmp = [data['fc_feats'], data['att_feats'], data['labels']]\n",
    "        tmp = [Variable(torch.from_numpy(_), requires_grad=False).cuda() for _ in tmp]\n",
    "        fc_feats, att_feats, labels = tmp\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = crit(model(fc_feats, att_feats, labels), labels[:,1:])\n",
    "        loss.backward()\n",
    "        utils.clip_gradient(optimizer, opt.grad_clip)\n",
    "        optimizer.step()\n",
    "        train_loss = loss.data[0] / batch_size\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        print(\"iter {} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "            .format(iteration, epoch, train_loss, end - start))\n",
    "\n",
    "        # Update the iteration and epoch\n",
    "        iteration += 1\n",
    "        if data['bounds']['wrapped']:\n",
    "            epoch += 1\n",
    "            update_lr_flag = True\n",
    "\n",
    "        # Write the training loss summary\n",
    "        if (iteration % opt.losses_log_every == 0):\n",
    "            if tf is not None:\n",
    "                add_summary_value(tf_summary_writer, 'train_loss', train_loss, iteration)\n",
    "                add_summary_value(tf_summary_writer, 'learning_rate', opt.current_lr, iteration)\n",
    "                add_summary_value(tf_summary_writer, 'scheduled_sampling_prob', model.ss_prob, iteration)\n",
    "                tf_summary_writer.flush()\n",
    "\n",
    "            loss_history[iteration] = train_loss\n",
    "            lr_history[iteration] = opt.current_lr\n",
    "            ss_prob_history[iteration] = model.ss_prob\n",
    "\n",
    "        # make evaluation on validation set, and save model\n",
    "        if (iteration % opt.save_checkpoint_every == 0):\n",
    "            # eval model\n",
    "            eval_kwargs = {'split': 'val',\n",
    "                            'dataset': opt.input_json}\n",
    "            eval_kwargs.update(vars(opt))\n",
    "            val_loss, predictions, lang_stats = eval_utils.eval_split(model, crit, loader, eval_kwargs)\n",
    "\n",
    "            # Write validation result into summary\n",
    "            if tf is not None:\n",
    "                add_summary_value(tf_summary_writer, 'validation loss', val_loss, iteration)\n",
    "                for k,v in lang_stats.items():\n",
    "                    add_summary_value(tf_summary_writer, k, v, iteration)\n",
    "                tf_summary_writer.flush()\n",
    "            val_result_history[iteration] = {'loss': val_loss, 'lang_stats': lang_stats, 'predictions': predictions}\n",
    "\n",
    "            # Save model if is improving on validation result\n",
    "            if opt.language_eval == 1:\n",
    "                current_score = lang_stats['CIDEr']\n",
    "            else:\n",
    "                current_score = - val_loss\n",
    "\n",
    "            best_flag = False\n",
    "            if True: # if true\n",
    "                if best_val_score is None or current_score > best_val_score:\n",
    "                    best_val_score = current_score\n",
    "                    best_flag = True\n",
    "                checkpoint_path = os.path.join(opt.checkpoint_path, 'model.pth')\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(\"model saved to {}\".format(checkpoint_path))\n",
    "                optimizer_path = os.path.join(opt.checkpoint_path, 'optimizer.pth')\n",
    "                torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "                # Dump miscalleous informations\n",
    "                infos['iter'] = iteration\n",
    "                infos['epoch'] = epoch\n",
    "                infos['iterators'] = loader.iterators\n",
    "                infos['split_ix'] = loader.split_ix\n",
    "                infos['best_val_score'] = best_val_score\n",
    "                infos['opt'] = opt\n",
    "                infos['vocab'] = loader.get_vocab()\n",
    "\n",
    "                histories['val_result_history'] = val_result_history\n",
    "                histories['loss_history'] = loss_history\n",
    "                histories['lr_history'] = lr_history\n",
    "                histories['ss_prob_history'] = ss_prob_history\n",
    "                with open(os.path.join(opt.checkpoint_path, 'infos_'+opt.id+'.pkl'), 'wb') as f:\n",
    "                    cPickle.dump(infos, f)\n",
    "                with open(os.path.join(opt.checkpoint_path, 'histories_'+opt.id+'.pkl'), 'wb') as f:\n",
    "                    cPickle.dump(histories, f)\n",
    "\n",
    "                if best_flag:\n",
    "                    checkpoint_path = os.path.join(opt.checkpoint_path, 'model-best.pth')\n",
    "                    torch.save(model.state_dict(), checkpoint_path)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "                    with open(os.path.join(opt.checkpoint_path, 'infos_'+opt.id+'-best.pkl'), 'wb') as f:\n",
    "                        cPickle.dump(infos, f)\n",
    "\n",
    "        # Stop if reaching max epochs\n",
    "        if epoch >= opt.max_epochs and opt.max_epochs != -1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt.label_object = json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaderRaw loading images from folder:  /home/ubuntu/workspace/ImageCaptioning.pytorch/images/\n",
      "0\n",
      "listing all images in directory /home/ubuntu/workspace/ImageCaptioning.pytorch/images/\n",
      "DataLoaderRaw found  80088  images\n",
      "Read data: 5.235480308532715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/ImageCaptioning.pytorch/models/Att2inModel.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weight = F.softmax(dot)                             # batch * att_size\n",
      "/home/ubuntu/workspace/ImageCaptioning.pytorch/models/Att2inModel.py:144: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.logit(output))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 (epoch 0), train_loss = 252.773, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.305351495742798\n",
      "iter 1 (epoch 0), train_loss = 173.345, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 5.16870641708374\n",
      "iter 2 (epoch 0), train_loss = 123.652, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.037405967712402\n",
      "iter 3 (epoch 0), train_loss = 139.175, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.025844573974609\n",
      "iter 4 (epoch 0), train_loss = 121.159, time/batch = 0.446\n",
      "EVALUATING\n",
      "Read data: 5.059460401535034\n",
      "iter 5 (epoch 0), train_loss = 129.613, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 4.826989412307739\n",
      "iter 6 (epoch 0), train_loss = 118.567, time/batch = 0.406\n",
      "EVALUATING\n",
      "Read data: 4.968106746673584\n",
      "iter 7 (epoch 0), train_loss = 120.045, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 4.950570344924927\n",
      "iter 8 (epoch 0), train_loss = 113.053, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.174998760223389\n",
      "iter 9 (epoch 0), train_loss = 111.697, time/batch = 0.507\n",
      "EVALUATING\n",
      "Read data: 5.140204906463623\n",
      "iter 10 (epoch 0), train_loss = 112.592, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.03186821937561\n",
      "iter 11 (epoch 0), train_loss = 116.084, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.109048128128052\n",
      "iter 12 (epoch 0), train_loss = 114.459, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.244126558303833\n",
      "iter 13 (epoch 0), train_loss = 113.090, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 5.264964818954468\n",
      "iter 14 (epoch 0), train_loss = 109.779, time/batch = 0.389\n",
      "EVALUATING\n",
      "Read data: 4.9270710945129395\n",
      "iter 15 (epoch 0), train_loss = 113.945, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.055103063583374\n",
      "iter 16 (epoch 0), train_loss = 112.335, time/batch = 0.513\n",
      "EVALUATING\n",
      "Read data: 5.1248619556427\n",
      "iter 17 (epoch 0), train_loss = 111.061, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 4.986462354660034\n",
      "iter 18 (epoch 0), train_loss = 90.718, time/batch = 0.384\n",
      "EVALUATING\n",
      "Read data: 4.981245756149292\n",
      "iter 19 (epoch 0), train_loss = 105.704, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 4.9643731117248535\n",
      "iter 20 (epoch 0), train_loss = 94.545, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.120389699935913\n",
      "iter 21 (epoch 0), train_loss = 102.955, time/batch = 0.522\n",
      "EVALUATING\n",
      "Read data: 5.238000869750977\n",
      "iter 22 (epoch 0), train_loss = 106.763, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.147637128829956\n",
      "iter 23 (epoch 0), train_loss = 96.788, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.048083305358887\n",
      "iter 24 (epoch 0), train_loss = 94.510, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.224309682846069\n",
      "iter 25 (epoch 0), train_loss = 94.104, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.1071999073028564\n",
      "iter 26 (epoch 0), train_loss = 91.677, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.112383842468262\n",
      "iter 27 (epoch 0), train_loss = 99.270, time/batch = 0.450\n",
      "EVALUATING\n",
      "Read data: 4.994495153427124\n",
      "iter 28 (epoch 0), train_loss = 93.608, time/batch = 0.523\n",
      "EVALUATING\n",
      "Read data: 5.131410360336304\n",
      "iter 29 (epoch 0), train_loss = 92.958, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 4.921706676483154\n",
      "iter 30 (epoch 0), train_loss = 81.380, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.187077522277832\n",
      "iter 31 (epoch 0), train_loss = 100.582, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 5.214723348617554\n",
      "iter 32 (epoch 0), train_loss = 84.623, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.126080513000488\n",
      "iter 33 (epoch 0), train_loss = 89.371, time/batch = 0.363\n",
      "EVALUATING\n",
      "Read data: 4.868467807769775\n",
      "iter 34 (epoch 0), train_loss = 89.903, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 5.120887279510498\n",
      "iter 35 (epoch 0), train_loss = 86.371, time/batch = 0.400\n",
      "EVALUATING\n",
      "Read data: 5.1883745193481445\n",
      "iter 36 (epoch 0), train_loss = 88.160, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 4.943304061889648\n",
      "iter 37 (epoch 0), train_loss = 81.092, time/batch = 0.386\n",
      "EVALUATING\n",
      "Read data: 5.154568672180176\n",
      "iter 38 (epoch 0), train_loss = 77.663, time/batch = 0.295\n",
      "EVALUATING\n",
      "Read data: 4.9136528968811035\n",
      "iter 39 (epoch 0), train_loss = 88.375, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.0160510540008545\n",
      "iter 40 (epoch 0), train_loss = 82.889, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.064791917800903\n",
      "iter 41 (epoch 0), train_loss = 91.038, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.0716469287872314\n",
      "iter 42 (epoch 0), train_loss = 86.463, time/batch = 0.464\n",
      "EVALUATING\n",
      "Read data: 5.015343427658081\n",
      "iter 43 (epoch 0), train_loss = 94.770, time/batch = 0.610\n",
      "EVALUATING\n",
      "Read data: 4.9949631690979\n",
      "iter 44 (epoch 0), train_loss = 85.662, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.020258903503418\n",
      "iter 45 (epoch 0), train_loss = 86.279, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.311957597732544\n",
      "iter 46 (epoch 0), train_loss = 92.667, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.101929426193237\n",
      "iter 47 (epoch 0), train_loss = 88.037, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.107881307601929\n",
      "iter 48 (epoch 0), train_loss = 91.766, time/batch = 0.460\n",
      "EVALUATING\n",
      "Read data: 5.131290912628174\n",
      "iter 49 (epoch 0), train_loss = 90.239, time/batch = 0.458\n",
      "EVALUATING\n",
      "Read data: 4.97643780708313\n",
      "iter 50 (epoch 0), train_loss = 84.621, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 5.164088010787964\n",
      "iter 51 (epoch 0), train_loss = 88.891, time/batch = 0.379\n",
      "EVALUATING\n",
      "Read data: 5.119720935821533\n",
      "iter 52 (epoch 0), train_loss = 85.417, time/batch = 0.519\n",
      "EVALUATING\n",
      "Read data: 5.065999269485474\n",
      "iter 53 (epoch 0), train_loss = 79.880, time/batch = 0.560\n",
      "EVALUATING\n",
      "Read data: 5.0892653465271\n",
      "iter 54 (epoch 0), train_loss = 93.413, time/batch = 0.419\n",
      "EVALUATING\n",
      "Read data: 5.163609981536865\n",
      "iter 55 (epoch 0), train_loss = 94.600, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.140699148178101\n",
      "iter 56 (epoch 0), train_loss = 93.269, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.012770175933838\n",
      "iter 57 (epoch 0), train_loss = 79.767, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.170464515686035\n",
      "iter 58 (epoch 0), train_loss = 89.295, time/batch = 0.472\n",
      "EVALUATING\n",
      "Read data: 5.081836223602295\n",
      "iter 59 (epoch 0), train_loss = 76.794, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.169519424438477\n",
      "iter 60 (epoch 0), train_loss = 85.170, time/batch = 0.406\n",
      "EVALUATING\n",
      "Read data: 5.105384111404419\n",
      "iter 61 (epoch 0), train_loss = 80.665, time/batch = 0.522\n",
      "EVALUATING\n",
      "Read data: 5.008974313735962\n",
      "iter 62 (epoch 0), train_loss = 85.269, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.203902006149292\n",
      "iter 63 (epoch 0), train_loss = 85.651, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.080806732177734\n",
      "iter 64 (epoch 0), train_loss = 81.322, time/batch = 0.524\n",
      "EVALUATING\n",
      "Read data: 5.052722692489624\n",
      "iter 65 (epoch 0), train_loss = 90.151, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.13482666015625\n",
      "iter 66 (epoch 0), train_loss = 85.030, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.2363646030426025\n",
      "iter 67 (epoch 0), train_loss = 82.865, time/batch = 0.377\n",
      "EVALUATING\n",
      "Read data: 4.944917917251587\n",
      "iter 68 (epoch 0), train_loss = 81.556, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.042485237121582\n",
      "iter 69 (epoch 0), train_loss = 84.687, time/batch = 0.459\n",
      "EVALUATING\n",
      "Read data: 5.294936656951904\n",
      "iter 70 (epoch 0), train_loss = 88.845, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.057062864303589\n",
      "iter 71 (epoch 0), train_loss = 86.199, time/batch = 0.435\n",
      "EVALUATING\n",
      "Read data: 5.149470329284668\n",
      "iter 72 (epoch 0), train_loss = 95.389, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 5.136435508728027\n",
      "iter 73 (epoch 0), train_loss = 82.763, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.19553017616272\n",
      "iter 74 (epoch 0), train_loss = 78.677, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.231801271438599\n",
      "iter 75 (epoch 0), train_loss = 84.674, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.141753911972046\n",
      "iter 76 (epoch 0), train_loss = 83.418, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.175528526306152\n",
      "iter 77 (epoch 0), train_loss = 81.020, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.182833671569824\n",
      "iter 78 (epoch 0), train_loss = 85.391, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.101918935775757\n",
      "iter 79 (epoch 0), train_loss = 86.063, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.050904989242554\n",
      "iter 80 (epoch 0), train_loss = 79.788, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.212400674819946\n",
      "iter 81 (epoch 0), train_loss = 80.024, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.0642194747924805\n",
      "iter 82 (epoch 0), train_loss = 89.360, time/batch = 0.437\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.106816530227661\n",
      "iter 83 (epoch 0), train_loss = 89.259, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.153535842895508\n",
      "iter 84 (epoch 0), train_loss = 85.855, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.166828393936157\n",
      "iter 85 (epoch 0), train_loss = 77.648, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.004393577575684\n",
      "iter 86 (epoch 0), train_loss = 76.256, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 4.9457783699035645\n",
      "iter 87 (epoch 0), train_loss = 77.785, time/batch = 0.359\n",
      "EVALUATING\n",
      "Read data: 5.1712424755096436\n",
      "iter 88 (epoch 0), train_loss = 78.522, time/batch = 0.361\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 7.37626838684082\n",
      "iter 89 (epoch 0), train_loss = 82.694, time/batch = 0.433\n",
      "EVALUATING\n",
      "Read data: 5.152551889419556\n",
      "iter 90 (epoch 0), train_loss = 82.649, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.107722043991089\n",
      "iter 91 (epoch 0), train_loss = 81.059, time/batch = 0.445\n",
      "EVALUATING\n",
      "Read data: 5.0071985721588135\n",
      "iter 92 (epoch 0), train_loss = 84.396, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.113517761230469\n",
      "iter 93 (epoch 0), train_loss = 85.373, time/batch = 0.499\n",
      "EVALUATING\n",
      "Read data: 5.155101776123047\n",
      "iter 94 (epoch 0), train_loss = 84.047, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.064254999160767\n",
      "iter 95 (epoch 0), train_loss = 78.533, time/batch = 0.566\n",
      "EVALUATING\n",
      "Read data: 5.0463902950286865\n",
      "iter 96 (epoch 0), train_loss = 80.370, time/batch = 0.474\n",
      "EVALUATING\n",
      "Read data: 5.081309795379639\n",
      "iter 97 (epoch 0), train_loss = 71.659, time/batch = 0.337\n",
      "EVALUATING\n",
      "Read data: 5.169619798660278\n",
      "iter 98 (epoch 0), train_loss = 83.353, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.146052360534668\n",
      "iter 99 (epoch 0), train_loss = 82.956, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.097563743591309\n",
      "iter 100 (epoch 0), train_loss = 83.288, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.053466081619263\n",
      "iter 101 (epoch 0), train_loss = 72.645, time/batch = 0.306\n",
      "EVALUATING\n",
      "Read data: 5.1228508949279785\n",
      "iter 102 (epoch 0), train_loss = 77.918, time/batch = 0.365\n",
      "EVALUATING\n",
      "Read data: 5.190079212188721\n",
      "iter 103 (epoch 0), train_loss = 83.144, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.1033570766448975\n",
      "iter 104 (epoch 0), train_loss = 71.087, time/batch = 0.377\n",
      "EVALUATING\n",
      "Read data: 5.079641580581665\n",
      "iter 105 (epoch 0), train_loss = 80.292, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.129171848297119\n",
      "iter 106 (epoch 0), train_loss = 83.787, time/batch = 0.385\n",
      "EVALUATING\n",
      "Read data: 5.26138162612915\n",
      "iter 107 (epoch 0), train_loss = 87.142, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 5.064090251922607\n",
      "iter 108 (epoch 0), train_loss = 83.197, time/batch = 0.537\n",
      "EVALUATING\n",
      "Read data: 4.966817140579224\n",
      "iter 109 (epoch 0), train_loss = 74.462, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.162289619445801\n",
      "iter 110 (epoch 0), train_loss = 76.393, time/batch = 0.460\n",
      "EVALUATING\n",
      "Read data: 5.116592645645142\n",
      "iter 111 (epoch 0), train_loss = 88.388, time/batch = 0.492\n",
      "EVALUATING\n",
      "Read data: 5.082765102386475\n",
      "iter 112 (epoch 0), train_loss = 83.051, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.206201791763306\n",
      "iter 113 (epoch 0), train_loss = 86.515, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.083765029907227\n",
      "iter 114 (epoch 0), train_loss = 88.458, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 4.964266538619995\n",
      "iter 115 (epoch 0), train_loss = 78.661, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.1623170375823975\n",
      "iter 116 (epoch 0), train_loss = 79.805, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.283589601516724\n",
      "iter 117 (epoch 0), train_loss = 84.356, time/batch = 0.499\n",
      "EVALUATING\n",
      "Read data: 4.90887713432312\n",
      "iter 118 (epoch 0), train_loss = 81.337, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 4.983279228210449\n",
      "iter 119 (epoch 0), train_loss = 82.358, time/batch = 0.336\n",
      "EVALUATING\n",
      "Read data: 4.991446256637573\n",
      "iter 120 (epoch 0), train_loss = 81.511, time/batch = 0.346\n",
      "EVALUATING\n",
      "Read data: 5.175062656402588\n",
      "iter 121 (epoch 0), train_loss = 79.226, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 5.156246900558472\n",
      "iter 122 (epoch 0), train_loss = 84.555, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.267944812774658\n",
      "iter 123 (epoch 0), train_loss = 83.849, time/batch = 0.794\n",
      "EVALUATING\n",
      "Read data: 5.1397340297698975\n",
      "iter 124 (epoch 0), train_loss = 79.532, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 4.961589097976685\n",
      "iter 125 (epoch 0), train_loss = 75.511, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 5.058449983596802\n",
      "iter 126 (epoch 0), train_loss = 84.298, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.208245038986206\n",
      "iter 127 (epoch 0), train_loss = 94.074, time/batch = 0.496\n",
      "EVALUATING\n",
      "Read data: 5.014219760894775\n",
      "iter 128 (epoch 0), train_loss = 78.489, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.06311821937561\n",
      "iter 129 (epoch 0), train_loss = 72.512, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.216624975204468\n",
      "iter 130 (epoch 0), train_loss = 73.542, time/batch = 0.531\n",
      "EVALUATING\n",
      "Read data: 5.113400936126709\n",
      "iter 131 (epoch 0), train_loss = 72.335, time/batch = 0.389\n",
      "EVALUATING\n",
      "Read data: 5.273045778274536\n",
      "iter 132 (epoch 0), train_loss = 95.064, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.1139607429504395\n",
      "iter 133 (epoch 0), train_loss = 75.080, time/batch = 0.448\n",
      "EVALUATING\n",
      "Read data: 5.211503267288208\n",
      "iter 134 (epoch 0), train_loss = 84.595, time/batch = 0.509\n",
      "EVALUATING\n",
      "Read data: 5.331990480422974\n",
      "iter 135 (epoch 0), train_loss = 76.316, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.260385036468506\n",
      "iter 136 (epoch 0), train_loss = 86.050, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.308673143386841\n",
      "iter 137 (epoch 0), train_loss = 80.670, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.0755205154418945\n",
      "iter 138 (epoch 0), train_loss = 75.204, time/batch = 0.445\n",
      "EVALUATING\n",
      "Read data: 5.075671434402466\n",
      "iter 139 (epoch 0), train_loss = 69.640, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 5.218497276306152\n",
      "iter 140 (epoch 0), train_loss = 79.273, time/batch = 0.366\n",
      "EVALUATING\n",
      "Read data: 4.925242185592651\n",
      "iter 141 (epoch 0), train_loss = 78.342, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.0003814697265625\n",
      "iter 142 (epoch 0), train_loss = 77.464, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.079636573791504\n",
      "iter 143 (epoch 0), train_loss = 77.739, time/batch = 0.367\n",
      "EVALUATING\n",
      "Read data: 5.131759166717529\n",
      "iter 144 (epoch 0), train_loss = 69.009, time/batch = 0.548\n",
      "EVALUATING\n",
      "Read data: 5.196344614028931\n",
      "iter 145 (epoch 0), train_loss = 71.444, time/batch = 0.419\n",
      "EVALUATING\n",
      "Read data: 5.080806255340576\n",
      "iter 146 (epoch 0), train_loss = 95.835, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.135799407958984\n",
      "iter 147 (epoch 0), train_loss = 73.969, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.046990633010864\n",
      "iter 148 (epoch 0), train_loss = 83.412, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 4.982928037643433\n",
      "iter 149 (epoch 0), train_loss = 85.297, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 4.934994220733643\n",
      "iter 150 (epoch 0), train_loss = 73.123, time/batch = 0.318\n",
      "EVALUATING\n",
      "Read data: 5.088361740112305\n",
      "iter 151 (epoch 0), train_loss = 80.481, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.260048151016235\n",
      "iter 152 (epoch 0), train_loss = 77.689, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.273000001907349\n",
      "iter 153 (epoch 0), train_loss = 75.728, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.049858808517456\n",
      "iter 154 (epoch 0), train_loss = 85.668, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.190679311752319\n",
      "iter 155 (epoch 0), train_loss = 79.640, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 4.96372389793396\n",
      "iter 156 (epoch 0), train_loss = 72.513, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.15654993057251\n",
      "iter 157 (epoch 0), train_loss = 84.177, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.094225645065308\n",
      "iter 158 (epoch 0), train_loss = 77.113, time/batch = 0.434\n",
      "EVALUATING\n",
      "Read data: 5.060928106307983\n",
      "iter 159 (epoch 0), train_loss = 70.944, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.206475496292114\n",
      "iter 160 (epoch 0), train_loss = 72.091, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 4.975531339645386\n",
      "iter 161 (epoch 0), train_loss = 79.251, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.058580160140991\n",
      "iter 162 (epoch 0), train_loss = 70.120, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.130157709121704\n",
      "iter 163 (epoch 0), train_loss = 77.353, time/batch = 0.599\n",
      "EVALUATING\n",
      "Read data: 5.160124063491821\n",
      "iter 164 (epoch 0), train_loss = 82.058, time/batch = 0.455\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.147178888320923\n",
      "iter 165 (epoch 0), train_loss = 86.370, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.277986288070679\n",
      "iter 166 (epoch 0), train_loss = 74.878, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 4.986137390136719\n",
      "iter 167 (epoch 0), train_loss = 81.677, time/batch = 0.533\n",
      "EVALUATING\n",
      "Read data: 5.2268385887146\n",
      "iter 168 (epoch 0), train_loss = 76.058, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.198153734207153\n",
      "iter 169 (epoch 0), train_loss = 81.351, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.240711450576782\n",
      "iter 170 (epoch 0), train_loss = 72.093, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.086417198181152\n",
      "iter 171 (epoch 0), train_loss = 74.289, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.017797470092773\n",
      "iter 172 (epoch 0), train_loss = 75.421, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 4.868059396743774\n",
      "iter 173 (epoch 0), train_loss = 77.905, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.064418315887451\n",
      "iter 174 (epoch 0), train_loss = 73.906, time/batch = 0.434\n",
      "EVALUATING\n",
      "Read data: 4.993848562240601\n",
      "iter 175 (epoch 0), train_loss = 74.351, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.140758514404297\n",
      "iter 176 (epoch 0), train_loss = 81.780, time/batch = 0.475\n",
      "EVALUATING\n",
      "Read data: 5.059349536895752\n",
      "iter 177 (epoch 0), train_loss = 82.125, time/batch = 0.536\n",
      "EVALUATING\n",
      "Read data: 5.2306365966796875\n",
      "iter 178 (epoch 0), train_loss = 72.007, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.188728332519531\n",
      "iter 179 (epoch 0), train_loss = 74.882, time/batch = 0.429\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 9.821606397628784\n",
      "iter 180 (epoch 0), train_loss = 106.299, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.060962200164795\n",
      "iter 181 (epoch 0), train_loss = 81.220, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.173056125640869\n",
      "iter 182 (epoch 0), train_loss = 76.279, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.1928088665008545\n",
      "iter 183 (epoch 0), train_loss = 81.482, time/batch = 0.582\n",
      "EVALUATING\n",
      "Read data: 5.133798360824585\n",
      "iter 184 (epoch 0), train_loss = 85.343, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.31605863571167\n",
      "iter 185 (epoch 0), train_loss = 84.271, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.057241678237915\n",
      "iter 186 (epoch 0), train_loss = 74.311, time/batch = 0.351\n",
      "EVALUATING\n",
      "Read data: 5.03874659538269\n",
      "iter 187 (epoch 0), train_loss = 82.414, time/batch = 0.523\n",
      "EVALUATING\n",
      "Read data: 5.095309734344482\n",
      "iter 188 (epoch 0), train_loss = 73.691, time/batch = 0.487\n",
      "EVALUATING\n",
      "Read data: 5.00783109664917\n",
      "iter 189 (epoch 0), train_loss = 86.619, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 4.999615907669067\n",
      "iter 190 (epoch 0), train_loss = 80.746, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.21265983581543\n",
      "iter 191 (epoch 0), train_loss = 77.605, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.217042446136475\n",
      "iter 192 (epoch 0), train_loss = 76.572, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.20628809928894\n",
      "iter 193 (epoch 0), train_loss = 86.902, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.0306923389434814\n",
      "iter 194 (epoch 0), train_loss = 77.540, time/batch = 0.458\n",
      "EVALUATING\n",
      "Read data: 5.096261739730835\n",
      "iter 195 (epoch 0), train_loss = 73.719, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.226677179336548\n",
      "iter 196 (epoch 0), train_loss = 74.510, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.0596137046813965\n",
      "iter 197 (epoch 0), train_loss = 75.926, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.103275775909424\n",
      "iter 198 (epoch 0), train_loss = 76.818, time/batch = 0.571\n",
      "EVALUATING\n",
      "Read data: 5.127924203872681\n",
      "iter 199 (epoch 0), train_loss = 83.062, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.224675416946411\n",
      "iter 200 (epoch 0), train_loss = 94.121, time/batch = 0.507\n",
      "EVALUATING\n",
      "Read data: 5.046932697296143\n",
      "iter 201 (epoch 0), train_loss = 70.837, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.170456886291504\n",
      "iter 202 (epoch 0), train_loss = 77.754, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.238399505615234\n",
      "iter 203 (epoch 0), train_loss = 72.260, time/batch = 0.511\n",
      "EVALUATING\n",
      "Read data: 5.271776437759399\n",
      "iter 204 (epoch 0), train_loss = 88.122, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.158480882644653\n",
      "iter 205 (epoch 0), train_loss = 80.920, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.217547178268433\n",
      "iter 206 (epoch 0), train_loss = 82.558, time/batch = 0.526\n",
      "EVALUATING\n",
      "Read data: 5.254497528076172\n",
      "iter 207 (epoch 0), train_loss = 76.858, time/batch = 0.392\n",
      "EVALUATING\n",
      "Read data: 5.016658306121826\n",
      "iter 208 (epoch 0), train_loss = 66.815, time/batch = 0.446\n",
      "EVALUATING\n",
      "Read data: 5.146393775939941\n",
      "iter 209 (epoch 0), train_loss = 76.464, time/batch = 0.363\n",
      "EVALUATING\n",
      "Read data: 5.053558111190796\n",
      "iter 210 (epoch 0), train_loss = 73.679, time/batch = 0.400\n",
      "EVALUATING\n",
      "Read data: 5.137551307678223\n",
      "iter 211 (epoch 0), train_loss = 75.380, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.107269763946533\n",
      "iter 212 (epoch 0), train_loss = 81.725, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.204814434051514\n",
      "iter 213 (epoch 0), train_loss = 81.432, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.065494537353516\n",
      "iter 214 (epoch 0), train_loss = 70.288, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.099024772644043\n",
      "iter 215 (epoch 0), train_loss = 74.560, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.142758369445801\n",
      "iter 216 (epoch 0), train_loss = 77.346, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.1943864822387695\n",
      "iter 217 (epoch 0), train_loss = 80.674, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.194689989089966\n",
      "iter 218 (epoch 0), train_loss = 76.792, time/batch = 0.557\n",
      "EVALUATING\n",
      "Read data: 5.1752028465271\n",
      "iter 219 (epoch 0), train_loss = 70.041, time/batch = 0.419\n",
      "EVALUATING\n",
      "Read data: 5.308723211288452\n",
      "iter 220 (epoch 0), train_loss = 76.553, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 4.981014251708984\n",
      "iter 221 (epoch 0), train_loss = 70.477, time/batch = 0.502\n",
      "EVALUATING\n",
      "Read data: 5.09200382232666\n",
      "iter 222 (epoch 0), train_loss = 75.454, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 4.989726543426514\n",
      "iter 223 (epoch 0), train_loss = 75.280, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.223341226577759\n",
      "iter 224 (epoch 0), train_loss = 81.929, time/batch = 0.460\n",
      "EVALUATING\n",
      "Read data: 5.159989833831787\n",
      "iter 225 (epoch 0), train_loss = 76.583, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.071895599365234\n",
      "iter 226 (epoch 0), train_loss = 85.575, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.270914077758789\n",
      "iter 227 (epoch 0), train_loss = 79.549, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 5.189663410186768\n",
      "iter 228 (epoch 0), train_loss = 82.495, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.238335609436035\n",
      "iter 229 (epoch 0), train_loss = 77.118, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.170714855194092\n",
      "iter 230 (epoch 0), train_loss = 80.576, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.078993558883667\n",
      "iter 231 (epoch 0), train_loss = 86.012, time/batch = 0.535\n",
      "EVALUATING\n",
      "Read data: 4.923231840133667\n",
      "iter 232 (epoch 0), train_loss = 87.189, time/batch = 0.477\n",
      "EVALUATING\n",
      "Read data: 5.210579872131348\n",
      "iter 233 (epoch 0), train_loss = 78.254, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.193492412567139\n",
      "iter 234 (epoch 0), train_loss = 72.389, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.278076171875\n",
      "iter 235 (epoch 0), train_loss = 76.452, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.032202959060669\n",
      "iter 236 (epoch 0), train_loss = 72.813, time/batch = 0.397\n",
      "EVALUATING\n",
      "Read data: 5.197905778884888\n",
      "iter 237 (epoch 0), train_loss = 67.333, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.123516082763672\n",
      "iter 238 (epoch 0), train_loss = 78.482, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.024053335189819\n",
      "iter 239 (epoch 0), train_loss = 71.062, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 5.14197564125061\n",
      "iter 240 (epoch 0), train_loss = 80.819, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.219360828399658\n",
      "iter 241 (epoch 0), train_loss = 73.602, time/batch = 0.385\n",
      "EVALUATING\n",
      "Read data: 4.9365081787109375\n",
      "iter 242 (epoch 0), train_loss = 72.010, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.197474479675293\n",
      "iter 243 (epoch 0), train_loss = 81.718, time/batch = 0.453\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 6.05529260635376\n",
      "iter 244 (epoch 0), train_loss = 76.812, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.036617279052734\n",
      "iter 245 (epoch 0), train_loss = 71.804, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.193367958068848\n",
      "iter 246 (epoch 0), train_loss = 76.425, time/batch = 0.438\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.2166829109191895\n",
      "iter 247 (epoch 0), train_loss = 76.661, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.094869136810303\n",
      "iter 248 (epoch 0), train_loss = 79.598, time/batch = 0.516\n",
      "EVALUATING\n",
      "Read data: 5.084155082702637\n",
      "iter 249 (epoch 0), train_loss = 76.426, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.064160585403442\n",
      "iter 250 (epoch 0), train_loss = 70.142, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.038931846618652\n",
      "iter 251 (epoch 0), train_loss = 64.378, time/batch = 0.530\n",
      "EVALUATING\n",
      "Read data: 5.000235319137573\n",
      "iter 252 (epoch 0), train_loss = 69.107, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.205464124679565\n",
      "iter 253 (epoch 0), train_loss = 73.446, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.165551424026489\n",
      "iter 254 (epoch 0), train_loss = 73.706, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.112659215927124\n",
      "iter 255 (epoch 0), train_loss = 73.572, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.054959058761597\n",
      "iter 256 (epoch 0), train_loss = 74.211, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.095096111297607\n",
      "iter 257 (epoch 0), train_loss = 74.275, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.000466346740723\n",
      "iter 258 (epoch 0), train_loss = 72.624, time/batch = 0.511\n",
      "EVALUATING\n",
      "Read data: 5.071457624435425\n",
      "iter 259 (epoch 0), train_loss = 80.359, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.267337322235107\n",
      "iter 260 (epoch 0), train_loss = 76.185, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.1391191482543945\n",
      "iter 261 (epoch 0), train_loss = 68.926, time/batch = 0.337\n",
      "EVALUATING\n",
      "Read data: 5.178860664367676\n",
      "iter 262 (epoch 0), train_loss = 68.883, time/batch = 0.348\n",
      "EVALUATING\n",
      "Read data: 5.045902729034424\n",
      "iter 263 (epoch 0), train_loss = 67.931, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.103438377380371\n",
      "iter 264 (epoch 0), train_loss = 79.425, time/batch = 0.506\n",
      "EVALUATING\n",
      "Read data: 5.306109428405762\n",
      "iter 265 (epoch 0), train_loss = 88.925, time/batch = 0.406\n",
      "EVALUATING\n",
      "Read data: 5.092706918716431\n",
      "iter 266 (epoch 0), train_loss = 67.334, time/batch = 0.369\n",
      "EVALUATING\n",
      "Read data: 5.064980506896973\n",
      "iter 267 (epoch 0), train_loss = 77.856, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.060508489608765\n",
      "iter 268 (epoch 0), train_loss = 76.930, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.1572511196136475\n",
      "iter 269 (epoch 0), train_loss = 72.803, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.101899862289429\n",
      "iter 270 (epoch 0), train_loss = 72.169, time/batch = 0.406\n",
      "EVALUATING\n",
      "Read data: 5.1264808177948\n",
      "iter 271 (epoch 0), train_loss = 73.874, time/batch = 0.392\n",
      "EVALUATING\n",
      "Read data: 5.079622030258179\n",
      "iter 272 (epoch 0), train_loss = 71.696, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.249174118041992\n",
      "iter 273 (epoch 0), train_loss = 76.381, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.285968780517578\n",
      "iter 274 (epoch 0), train_loss = 76.627, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 4.974003076553345\n",
      "iter 275 (epoch 0), train_loss = 66.959, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.049648761749268\n",
      "iter 276 (epoch 0), train_loss = 75.537, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.05220103263855\n",
      "iter 277 (epoch 0), train_loss = 75.761, time/batch = 0.383\n",
      "EVALUATING\n",
      "Read data: 5.0414698123931885\n",
      "iter 278 (epoch 0), train_loss = 64.613, time/batch = 0.369\n",
      "EVALUATING\n",
      "Read data: 4.90777587890625\n",
      "iter 279 (epoch 0), train_loss = 63.274, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.0794548988342285\n",
      "iter 280 (epoch 0), train_loss = 75.681, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.135673999786377\n",
      "iter 281 (epoch 0), train_loss = 65.931, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 5.140720367431641\n",
      "iter 282 (epoch 0), train_loss = 73.988, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.235383749008179\n",
      "iter 283 (epoch 0), train_loss = 68.684, time/batch = 0.392\n",
      "EVALUATING\n",
      "Read data: 5.096182107925415\n",
      "iter 284 (epoch 0), train_loss = 82.074, time/batch = 0.545\n",
      "EVALUATING\n",
      "Read data: 5.098446607589722\n",
      "iter 285 (epoch 0), train_loss = 69.293, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 5.1617443561553955\n",
      "iter 286 (epoch 0), train_loss = 73.150, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.056102991104126\n",
      "iter 287 (epoch 0), train_loss = 78.198, time/batch = 0.445\n",
      "EVALUATING\n",
      "Read data: 5.095307111740112\n",
      "iter 288 (epoch 0), train_loss = 68.015, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 5.121165752410889\n",
      "iter 289 (epoch 0), train_loss = 73.103, time/batch = 0.547\n",
      "EVALUATING\n",
      "Read data: 5.079258918762207\n",
      "iter 290 (epoch 0), train_loss = 67.271, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.052998304367065\n",
      "iter 291 (epoch 0), train_loss = 74.166, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.0794172286987305\n",
      "iter 292 (epoch 0), train_loss = 78.433, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 4.928999423980713\n",
      "iter 293 (epoch 0), train_loss = 68.225, time/batch = 0.376\n",
      "EVALUATING\n",
      "Read data: 5.136399984359741\n",
      "iter 294 (epoch 0), train_loss = 67.452, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 5.328149080276489\n",
      "iter 295 (epoch 0), train_loss = 77.536, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.169059753417969\n",
      "iter 296 (epoch 0), train_loss = 64.334, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.185842275619507\n",
      "iter 297 (epoch 0), train_loss = 75.390, time/batch = 0.577\n",
      "EVALUATING\n",
      "Read data: 5.1412413120269775\n",
      "iter 298 (epoch 0), train_loss = 81.590, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.234379053115845\n",
      "iter 299 (epoch 0), train_loss = 81.747, time/batch = 0.380\n",
      "EVALUATING\n",
      "Read data: 5.2417707443237305\n",
      "iter 300 (epoch 0), train_loss = 82.476, time/batch = 0.419\n",
      "EVALUATING\n",
      "Read data: 5.090587377548218\n",
      "iter 301 (epoch 0), train_loss = 86.303, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 4.954272270202637\n",
      "iter 302 (epoch 0), train_loss = 70.312, time/batch = 0.362\n",
      "EVALUATING\n",
      "Read data: 5.1264824867248535\n",
      "iter 303 (epoch 0), train_loss = 76.897, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.091225862503052\n",
      "iter 304 (epoch 0), train_loss = 70.571, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 4.939359426498413\n",
      "iter 305 (epoch 0), train_loss = 69.528, time/batch = 0.487\n",
      "EVALUATING\n",
      "Read data: 5.17711329460144\n",
      "iter 306 (epoch 0), train_loss = 80.655, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 5.053105115890503\n",
      "iter 307 (epoch 0), train_loss = 75.002, time/batch = 0.376\n",
      "EVALUATING\n",
      "Read data: 5.265457391738892\n",
      "iter 308 (epoch 0), train_loss = 79.543, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.3258984088897705\n",
      "iter 309 (epoch 0), train_loss = 75.000, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.026571035385132\n",
      "iter 310 (epoch 0), train_loss = 69.361, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.218019485473633\n",
      "iter 311 (epoch 0), train_loss = 81.267, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.173161029815674\n",
      "iter 312 (epoch 0), train_loss = 76.982, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.121792554855347\n",
      "iter 313 (epoch 0), train_loss = 76.581, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.168905973434448\n",
      "iter 314 (epoch 0), train_loss = 72.114, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.028985977172852\n",
      "iter 315 (epoch 0), train_loss = 79.920, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 5.153777360916138\n",
      "iter 316 (epoch 0), train_loss = 62.928, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 5.144400119781494\n",
      "iter 317 (epoch 0), train_loss = 76.799, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.273182392120361\n",
      "iter 318 (epoch 0), train_loss = 78.162, time/batch = 0.360\n",
      "EVALUATING\n",
      "Read data: 5.265066862106323\n",
      "iter 319 (epoch 0), train_loss = 68.809, time/batch = 0.330\n",
      "EVALUATING\n",
      "Read data: 5.020026206970215\n",
      "iter 320 (epoch 0), train_loss = 79.244, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.001180171966553\n",
      "iter 321 (epoch 0), train_loss = 79.901, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 4.989942789077759\n",
      "iter 322 (epoch 0), train_loss = 77.809, time/batch = 0.380\n",
      "EVALUATING\n",
      "Read data: 5.193145751953125\n",
      "iter 323 (epoch 0), train_loss = 80.592, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.171769857406616\n",
      "iter 324 (epoch 0), train_loss = 75.971, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 4.969520330429077\n",
      "iter 325 (epoch 0), train_loss = 69.021, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 4.958408355712891\n",
      "iter 326 (epoch 0), train_loss = 75.317, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.218733787536621\n",
      "iter 327 (epoch 0), train_loss = 77.244, time/batch = 0.459\n",
      "EVALUATING\n",
      "Read data: 5.302480220794678\n",
      "iter 328 (epoch 0), train_loss = 72.034, time/batch = 0.373\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.061717748641968\n",
      "iter 329 (epoch 0), train_loss = 66.247, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.1876795291900635\n",
      "iter 330 (epoch 0), train_loss = 82.321, time/batch = 0.531\n",
      "EVALUATING\n",
      "Read data: 4.974856615066528\n",
      "iter 331 (epoch 0), train_loss = 73.791, time/batch = 0.521\n",
      "EVALUATING\n",
      "Read data: 5.128743886947632\n",
      "iter 332 (epoch 0), train_loss = 77.406, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.068079948425293\n",
      "iter 333 (epoch 0), train_loss = 73.906, time/batch = 0.453\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 5.849058628082275\n",
      "iter 334 (epoch 0), train_loss = 77.575, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.025962591171265\n",
      "iter 335 (epoch 0), train_loss = 71.369, time/batch = 0.369\n",
      "EVALUATING\n",
      "Read data: 5.1995251178741455\n",
      "iter 336 (epoch 0), train_loss = 72.846, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.090246677398682\n",
      "iter 337 (epoch 0), train_loss = 71.731, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.228109359741211\n",
      "iter 338 (epoch 0), train_loss = 85.647, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 4.8796305656433105\n",
      "iter 339 (epoch 0), train_loss = 68.537, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.1157026290893555\n",
      "iter 340 (epoch 0), train_loss = 73.232, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.265594959259033\n",
      "iter 341 (epoch 0), train_loss = 71.445, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.038524150848389\n",
      "iter 342 (epoch 0), train_loss = 69.541, time/batch = 0.435\n",
      "EVALUATING\n",
      "Read data: 5.132402658462524\n",
      "iter 343 (epoch 0), train_loss = 72.602, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.208036422729492\n",
      "iter 344 (epoch 0), train_loss = 77.539, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.239393711090088\n",
      "iter 345 (epoch 0), train_loss = 70.887, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.000204086303711\n",
      "iter 346 (epoch 0), train_loss = 80.459, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 5.143224239349365\n",
      "iter 347 (epoch 0), train_loss = 76.566, time/batch = 0.555\n",
      "EVALUATING\n",
      "Read data: 5.271480560302734\n",
      "iter 348 (epoch 0), train_loss = 74.699, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.206941843032837\n",
      "iter 349 (epoch 0), train_loss = 77.064, time/batch = 0.506\n",
      "EVALUATING\n",
      "Read data: 5.068850755691528\n",
      "iter 350 (epoch 0), train_loss = 82.442, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.219189882278442\n",
      "iter 351 (epoch 0), train_loss = 66.590, time/batch = 0.339\n",
      "EVALUATING\n",
      "Read data: 5.000318288803101\n",
      "iter 352 (epoch 0), train_loss = 69.095, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 4.743509292602539\n",
      "iter 353 (epoch 0), train_loss = 69.890, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.2462897300720215\n",
      "iter 354 (epoch 0), train_loss = 70.516, time/batch = 0.571\n",
      "EVALUATING\n",
      "Read data: 5.041476249694824\n",
      "iter 355 (epoch 0), train_loss = 72.606, time/batch = 0.383\n",
      "EVALUATING\n",
      "Read data: 5.234846353530884\n",
      "iter 356 (epoch 0), train_loss = 70.063, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.043824195861816\n",
      "iter 357 (epoch 0), train_loss = 66.118, time/batch = 0.338\n",
      "EVALUATING\n",
      "Read data: 5.124670505523682\n",
      "iter 358 (epoch 0), train_loss = 78.680, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.002482652664185\n",
      "iter 359 (epoch 0), train_loss = 73.017, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.159598112106323\n",
      "iter 360 (epoch 0), train_loss = 70.146, time/batch = 0.359\n",
      "EVALUATING\n",
      "Read data: 4.991473197937012\n",
      "iter 361 (epoch 0), train_loss = 76.465, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 4.903478384017944\n",
      "iter 362 (epoch 0), train_loss = 78.698, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 5.181974411010742\n",
      "iter 363 (epoch 0), train_loss = 76.979, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.203987121582031\n",
      "iter 364 (epoch 0), train_loss = 76.444, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.0738606452941895\n",
      "iter 365 (epoch 0), train_loss = 72.189, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.208858251571655\n",
      "iter 366 (epoch 0), train_loss = 72.772, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.009714603424072\n",
      "iter 367 (epoch 0), train_loss = 65.779, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.103657484054565\n",
      "iter 368 (epoch 0), train_loss = 69.357, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.266813278198242\n",
      "iter 369 (epoch 0), train_loss = 75.915, time/batch = 0.524\n",
      "EVALUATING\n",
      "Read data: 4.995512008666992\n",
      "iter 370 (epoch 0), train_loss = 69.034, time/batch = 0.313\n",
      "EVALUATING\n",
      "Read data: 5.098479747772217\n",
      "iter 371 (epoch 0), train_loss = 84.006, time/batch = 0.421\n",
      "EVALUATING\n",
      "Read data: 8.495948076248169\n",
      "iter 372 (epoch 0), train_loss = 75.831, time/batch = 0.957\n",
      "EVALUATING\n",
      "Read data: 7.894580364227295\n",
      "iter 373 (epoch 0), train_loss = 76.440, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 4.97419548034668\n",
      "iter 374 (epoch 0), train_loss = 64.050, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.242573976516724\n",
      "iter 375 (epoch 0), train_loss = 70.336, time/batch = 0.317\n",
      "EVALUATING\n",
      "Read data: 5.25427508354187\n",
      "iter 376 (epoch 0), train_loss = 69.800, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.1536760330200195\n",
      "iter 377 (epoch 0), train_loss = 76.887, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.040058612823486\n",
      "iter 378 (epoch 0), train_loss = 67.378, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.213959217071533\n",
      "iter 379 (epoch 0), train_loss = 73.402, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.12595272064209\n",
      "iter 380 (epoch 0), train_loss = 70.367, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.190500020980835\n",
      "iter 381 (epoch 0), train_loss = 72.567, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 4.963680267333984\n",
      "iter 382 (epoch 0), train_loss = 68.233, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.0813398361206055\n",
      "iter 383 (epoch 0), train_loss = 79.764, time/batch = 0.496\n",
      "EVALUATING\n",
      "Read data: 5.096337556838989\n",
      "iter 384 (epoch 0), train_loss = 72.088, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.175738334655762\n",
      "iter 385 (epoch 0), train_loss = 75.732, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.10998272895813\n",
      "iter 386 (epoch 0), train_loss = 73.493, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 5.074942350387573\n",
      "iter 387 (epoch 0), train_loss = 78.211, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.115201950073242\n",
      "iter 388 (epoch 0), train_loss = 68.358, time/batch = 0.350\n",
      "EVALUATING\n",
      "Read data: 5.107457160949707\n",
      "iter 389 (epoch 0), train_loss = 84.077, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.099474668502808\n",
      "iter 390 (epoch 0), train_loss = 71.048, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.230946063995361\n",
      "iter 391 (epoch 0), train_loss = 64.539, time/batch = 0.376\n",
      "EVALUATING\n",
      "Read data: 5.164302825927734\n",
      "iter 392 (epoch 0), train_loss = 71.356, time/batch = 0.458\n",
      "EVALUATING\n",
      "Read data: 5.09275221824646\n",
      "iter 393 (epoch 0), train_loss = 78.420, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.196359395980835\n",
      "iter 394 (epoch 0), train_loss = 74.137, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.148073434829712\n",
      "iter 395 (epoch 0), train_loss = 66.941, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.229549884796143\n",
      "iter 396 (epoch 0), train_loss = 69.241, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.1506688594818115\n",
      "iter 397 (epoch 0), train_loss = 69.440, time/batch = 0.477\n",
      "EVALUATING\n",
      "Read data: 5.183255195617676\n",
      "iter 398 (epoch 0), train_loss = 68.199, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.23400616645813\n",
      "iter 399 (epoch 0), train_loss = 71.351, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 4.942246437072754\n",
      "iter 400 (epoch 0), train_loss = 71.032, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.165939569473267\n",
      "iter 401 (epoch 0), train_loss = 72.505, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.166925430297852\n",
      "iter 402 (epoch 0), train_loss = 74.890, time/batch = 0.384\n",
      "EVALUATING\n",
      "Read data: 5.151159763336182\n",
      "iter 403 (epoch 0), train_loss = 67.309, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.157415390014648\n",
      "iter 404 (epoch 0), train_loss = 69.574, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.1704747676849365\n",
      "iter 405 (epoch 0), train_loss = 75.518, time/batch = 0.446\n",
      "EVALUATING\n",
      "Read data: 9.925755977630615\n",
      "iter 406 (epoch 0), train_loss = 65.971, time/batch = 0.891\n",
      "EVALUATING\n",
      "Read data: 6.246127367019653\n",
      "iter 407 (epoch 0), train_loss = 75.682, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.146403551101685\n",
      "iter 408 (epoch 0), train_loss = 67.536, time/batch = 0.512\n",
      "EVALUATING\n",
      "Read data: 5.08236837387085\n",
      "iter 409 (epoch 0), train_loss = 75.889, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 7.667601108551025\n",
      "iter 410 (epoch 0), train_loss = 77.762, time/batch = 1.138\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 8.533636093139648\n",
      "iter 411 (epoch 0), train_loss = 82.103, time/batch = 0.465\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 7.3391149044036865\n",
      "iter 412 (epoch 0), train_loss = 82.719, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.118736028671265\n",
      "iter 413 (epoch 0), train_loss = 65.723, time/batch = 0.392\n",
      "EVALUATING\n",
      "Read data: 5.129425764083862\n",
      "iter 414 (epoch 0), train_loss = 68.292, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.173458099365234\n",
      "iter 415 (epoch 0), train_loss = 77.444, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.195315599441528\n",
      "iter 416 (epoch 0), train_loss = 82.370, time/batch = 0.945\n",
      "EVALUATING\n",
      "Read data: 5.287247180938721\n",
      "iter 417 (epoch 0), train_loss = 67.309, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.122030735015869\n",
      "iter 418 (epoch 0), train_loss = 76.885, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.198419094085693\n",
      "iter 419 (epoch 0), train_loss = 70.763, time/batch = 0.521\n",
      "EVALUATING\n",
      "Read data: 5.209815740585327\n",
      "iter 420 (epoch 0), train_loss = 77.410, time/batch = 0.458\n",
      "EVALUATING\n",
      "Read data: 5.0252602100372314\n",
      "iter 421 (epoch 0), train_loss = 80.447, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.15641975402832\n",
      "iter 422 (epoch 0), train_loss = 71.948, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.13163948059082\n",
      "iter 423 (epoch 0), train_loss = 77.190, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.0637617111206055\n",
      "iter 424 (epoch 0), train_loss = 79.078, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.178867340087891\n",
      "iter 425 (epoch 0), train_loss = 73.908, time/batch = 0.435\n",
      "EVALUATING\n",
      "Read data: 5.131103038787842\n",
      "iter 426 (epoch 0), train_loss = 70.338, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.166922330856323\n",
      "iter 427 (epoch 0), train_loss = 75.937, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.044452428817749\n",
      "iter 428 (epoch 0), train_loss = 67.225, time/batch = 0.338\n",
      "EVALUATING\n",
      "Read data: 5.168980836868286\n",
      "iter 429 (epoch 0), train_loss = 75.304, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 5.298011779785156\n",
      "iter 430 (epoch 0), train_loss = 82.483, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.050119161605835\n",
      "iter 431 (epoch 0), train_loss = 70.881, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.107762575149536\n",
      "iter 432 (epoch 0), train_loss = 79.322, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.101907253265381\n",
      "iter 433 (epoch 0), train_loss = 74.288, time/batch = 0.541\n",
      "EVALUATING\n",
      "Read data: 5.12085223197937\n",
      "iter 434 (epoch 0), train_loss = 72.244, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.064788579940796\n",
      "iter 435 (epoch 0), train_loss = 68.851, time/batch = 0.377\n",
      "EVALUATING\n",
      "Read data: 5.061667442321777\n",
      "iter 436 (epoch 0), train_loss = 83.500, time/batch = 0.503\n",
      "EVALUATING\n",
      "Read data: 5.326091766357422\n",
      "iter 437 (epoch 0), train_loss = 79.346, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.132026195526123\n",
      "iter 438 (epoch 0), train_loss = 73.305, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.076613903045654\n",
      "iter 439 (epoch 0), train_loss = 78.085, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 5.096228361129761\n",
      "iter 440 (epoch 0), train_loss = 70.704, time/batch = 0.373\n",
      "EVALUATING\n",
      "Read data: 5.074555158615112\n",
      "iter 441 (epoch 0), train_loss = 73.348, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.282259225845337\n",
      "iter 442 (epoch 0), train_loss = 76.873, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.082570314407349\n",
      "iter 443 (epoch 0), train_loss = 70.549, time/batch = 0.554\n",
      "EVALUATING\n",
      "Read data: 5.212586402893066\n",
      "iter 444 (epoch 0), train_loss = 79.035, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.066646575927734\n",
      "iter 445 (epoch 0), train_loss = 70.971, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.32023811340332\n",
      "iter 446 (epoch 0), train_loss = 82.252, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.305415630340576\n",
      "iter 447 (epoch 0), train_loss = 69.088, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 4.9247894287109375\n",
      "iter 448 (epoch 0), train_loss = 69.834, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.1486592292785645\n",
      "iter 449 (epoch 0), train_loss = 76.849, time/batch = 0.433\n",
      "EVALUATING\n",
      "Read data: 5.20034646987915\n",
      "iter 450 (epoch 0), train_loss = 83.191, time/batch = 0.521\n",
      "EVALUATING\n",
      "Read data: 5.001500129699707\n",
      "iter 451 (epoch 0), train_loss = 63.740, time/batch = 0.333\n",
      "EVALUATING\n",
      "Read data: 4.969532012939453\n",
      "iter 452 (epoch 0), train_loss = 71.565, time/batch = 0.459\n",
      "EVALUATING\n",
      "Read data: 5.146735429763794\n",
      "iter 453 (epoch 0), train_loss = 74.339, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 7.016473770141602\n",
      "iter 454 (epoch 0), train_loss = 70.476, time/batch = 1.106\n",
      "EVALUATING\n",
      "Read data: 10.083909273147583\n",
      "iter 455 (epoch 0), train_loss = 66.495, time/batch = 0.977\n",
      "EVALUATING\n",
      "Read data: 10.086606740951538\n",
      "iter 456 (epoch 0), train_loss = 73.126, time/batch = 0.373\n",
      "EVALUATING\n",
      "Read data: 5.161326885223389\n",
      "iter 457 (epoch 0), train_loss = 80.400, time/batch = 0.458\n",
      "EVALUATING\n",
      "Read data: 4.951029539108276\n",
      "iter 458 (epoch 0), train_loss = 81.843, time/batch = 0.510\n",
      "EVALUATING\n",
      "Read data: 5.030715465545654\n",
      "iter 459 (epoch 0), train_loss = 73.330, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.12326979637146\n",
      "iter 460 (epoch 0), train_loss = 76.734, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 5.012231826782227\n",
      "iter 461 (epoch 0), train_loss = 77.791, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.203156471252441\n",
      "iter 462 (epoch 0), train_loss = 81.036, time/batch = 0.507\n",
      "EVALUATING\n",
      "Read data: 5.143059492111206\n",
      "iter 463 (epoch 0), train_loss = 64.082, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 5.0463268756866455\n",
      "iter 464 (epoch 0), train_loss = 69.349, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.137622356414795\n",
      "iter 465 (epoch 0), train_loss = 68.938, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.236358642578125\n",
      "iter 466 (epoch 0), train_loss = 67.412, time/batch = 0.374\n",
      "EVALUATING\n",
      "Read data: 5.256166934967041\n",
      "iter 467 (epoch 0), train_loss = 86.096, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.191218137741089\n",
      "iter 468 (epoch 0), train_loss = 67.347, time/batch = 0.293\n",
      "EVALUATING\n",
      "Read data: 5.0893261432647705\n",
      "iter 469 (epoch 0), train_loss = 84.050, time/batch = 0.492\n",
      "EVALUATING\n",
      "Read data: 5.119983196258545\n",
      "iter 470 (epoch 0), train_loss = 66.031, time/batch = 0.487\n",
      "EVALUATING\n",
      "Read data: 5.041421175003052\n",
      "iter 471 (epoch 0), train_loss = 77.403, time/batch = 0.502\n",
      "EVALUATING\n",
      "Read data: 5.0891313552856445\n",
      "iter 472 (epoch 0), train_loss = 68.526, time/batch = 0.450\n",
      "EVALUATING\n",
      "Read data: 4.96744704246521\n",
      "iter 473 (epoch 0), train_loss = 70.593, time/batch = 0.477\n",
      "EVALUATING\n",
      "Read data: 5.154935359954834\n",
      "iter 474 (epoch 0), train_loss = 72.238, time/batch = 0.421\n",
      "EVALUATING\n",
      "Read data: 5.086423397064209\n",
      "iter 475 (epoch 0), train_loss = 82.938, time/batch = 0.492\n",
      "EVALUATING\n",
      "Read data: 5.0206427574157715\n",
      "iter 476 (epoch 0), train_loss = 66.953, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.225830078125\n",
      "iter 477 (epoch 0), train_loss = 64.552, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.171848297119141\n",
      "iter 478 (epoch 0), train_loss = 66.888, time/batch = 0.420\n",
      "EVALUATING\n",
      "Read data: 5.0386292934417725\n",
      "iter 479 (epoch 0), train_loss = 77.179, time/batch = 0.504\n",
      "EVALUATING\n",
      "Read data: 4.956895112991333\n",
      "iter 480 (epoch 0), train_loss = 73.447, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 4.9638144969940186\n",
      "iter 481 (epoch 0), train_loss = 68.924, time/batch = 0.362\n",
      "EVALUATING\n",
      "Read data: 5.156377553939819\n",
      "iter 482 (epoch 0), train_loss = 65.978, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.046184778213501\n",
      "iter 483 (epoch 0), train_loss = 75.476, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.214468240737915\n",
      "iter 484 (epoch 0), train_loss = 80.606, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.056368350982666\n",
      "iter 485 (epoch 0), train_loss = 61.182, time/batch = 0.299\n",
      "EVALUATING\n",
      "Read data: 5.189586162567139\n",
      "iter 486 (epoch 0), train_loss = 71.114, time/batch = 0.450\n",
      "EVALUATING\n",
      "Read data: 5.306173324584961\n",
      "iter 487 (epoch 0), train_loss = 71.253, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.030210256576538\n",
      "iter 488 (epoch 0), train_loss = 71.620, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.126584529876709\n",
      "iter 489 (epoch 0), train_loss = 63.916, time/batch = 0.450\n",
      "EVALUATING\n",
      "Read data: 5.176030158996582\n",
      "iter 490 (epoch 0), train_loss = 66.693, time/batch = 0.422\n",
      "EVALUATING\n",
      "Read data: 5.149495601654053\n",
      "iter 491 (epoch 0), train_loss = 75.313, time/batch = 0.515\n",
      "EVALUATING\n",
      "Read data: 4.897496700286865\n",
      "iter 492 (epoch 0), train_loss = 71.483, time/batch = 0.456\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.094802141189575\n",
      "iter 493 (epoch 0), train_loss = 67.269, time/batch = 0.512\n",
      "EVALUATING\n",
      "Read data: 5.23944091796875\n",
      "iter 494 (epoch 0), train_loss = 71.686, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.092511415481567\n",
      "iter 495 (epoch 0), train_loss = 74.891, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.067931175231934\n",
      "iter 496 (epoch 0), train_loss = 69.563, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.132164001464844\n",
      "iter 497 (epoch 0), train_loss = 73.196, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 4.989598035812378\n",
      "iter 498 (epoch 0), train_loss = 79.845, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.107976198196411\n",
      "iter 499 (epoch 0), train_loss = 70.099, time/batch = 0.561\n",
      "EVALUATING\n",
      "Read data: 5.157949924468994\n",
      "iter 500 (epoch 0), train_loss = 67.501, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.314004898071289\n",
      "iter 501 (epoch 0), train_loss = 67.473, time/batch = 0.436\n",
      "EVALUATING\n",
      "Read data: 5.173547744750977\n",
      "iter 502 (epoch 0), train_loss = 71.342, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.147392749786377\n",
      "iter 503 (epoch 0), train_loss = 77.766, time/batch = 0.558\n",
      "EVALUATING\n",
      "Read data: 5.100733041763306\n",
      "iter 504 (epoch 0), train_loss = 78.154, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.149662971496582\n",
      "iter 505 (epoch 0), train_loss = 69.866, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.126661777496338\n",
      "iter 506 (epoch 0), train_loss = 75.639, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 4.990432262420654\n",
      "iter 507 (epoch 0), train_loss = 72.705, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.3619115352630615\n",
      "iter 508 (epoch 0), train_loss = 78.816, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.205524444580078\n",
      "iter 509 (epoch 0), train_loss = 75.326, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.253207206726074\n",
      "iter 510 (epoch 0), train_loss = 64.101, time/batch = 0.311\n",
      "EVALUATING\n",
      "Read data: 4.989771604537964\n",
      "iter 511 (epoch 0), train_loss = 80.504, time/batch = 0.509\n",
      "EVALUATING\n",
      "Read data: 5.096410751342773\n",
      "iter 512 (epoch 0), train_loss = 71.002, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.06362247467041\n",
      "iter 513 (epoch 0), train_loss = 75.062, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 4.870687246322632\n",
      "iter 514 (epoch 0), train_loss = 62.001, time/batch = 0.377\n",
      "EVALUATING\n",
      "Read data: 5.1108152866363525\n",
      "iter 515 (epoch 0), train_loss = 67.184, time/batch = 0.377\n",
      "EVALUATING\n",
      "Read data: 5.038108825683594\n",
      "iter 516 (epoch 0), train_loss = 72.809, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 5.087536334991455\n",
      "iter 517 (epoch 0), train_loss = 73.931, time/batch = 0.509\n",
      "EVALUATING\n",
      "Read data: 5.288989543914795\n",
      "iter 518 (epoch 0), train_loss = 72.966, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 4.940977334976196\n",
      "iter 519 (epoch 0), train_loss = 66.243, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 5.0325939655303955\n",
      "iter 520 (epoch 0), train_loss = 74.727, time/batch = 0.560\n",
      "EVALUATING\n",
      "Read data: 5.067744970321655\n",
      "iter 521 (epoch 0), train_loss = 65.827, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.211532831192017\n",
      "iter 522 (epoch 0), train_loss = 77.676, time/batch = 0.492\n",
      "EVALUATING\n",
      "Read data: 5.199580192565918\n",
      "iter 523 (epoch 0), train_loss = 71.652, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.140382528305054\n",
      "iter 524 (epoch 0), train_loss = 79.465, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.25785231590271\n",
      "iter 525 (epoch 0), train_loss = 65.988, time/batch = 0.386\n",
      "EVALUATING\n",
      "Read data: 4.913349866867065\n",
      "iter 526 (epoch 0), train_loss = 77.943, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.186872720718384\n",
      "iter 527 (epoch 0), train_loss = 80.553, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.138650178909302\n",
      "iter 528 (epoch 0), train_loss = 71.847, time/batch = 0.349\n",
      "EVALUATING\n",
      "Read data: 5.2392377853393555\n",
      "iter 529 (epoch 0), train_loss = 73.027, time/batch = 0.506\n",
      "EVALUATING\n",
      "Read data: 5.170121431350708\n",
      "iter 530 (epoch 0), train_loss = 70.665, time/batch = 0.500\n",
      "EVALUATING\n",
      "Read data: 5.07953667640686\n",
      "iter 531 (epoch 0), train_loss = 64.551, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 4.966201305389404\n",
      "iter 532 (epoch 0), train_loss = 68.588, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.106863975524902\n",
      "iter 533 (epoch 0), train_loss = 71.459, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.26672887802124\n",
      "iter 534 (epoch 0), train_loss = 77.074, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.1658947467803955\n",
      "iter 535 (epoch 0), train_loss = 70.749, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.2512922286987305\n",
      "iter 536 (epoch 0), train_loss = 61.429, time/batch = 0.352\n",
      "EVALUATING\n",
      "Read data: 5.086951732635498\n",
      "iter 537 (epoch 0), train_loss = 73.235, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 5.209616422653198\n",
      "iter 538 (epoch 0), train_loss = 72.427, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 4.972143888473511\n",
      "iter 539 (epoch 0), train_loss = 68.571, time/batch = 0.420\n",
      "EVALUATING\n",
      "Read data: 5.037333965301514\n",
      "iter 540 (epoch 0), train_loss = 66.943, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 5.126513481140137\n",
      "iter 541 (epoch 0), train_loss = 75.841, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.103729963302612\n",
      "iter 542 (epoch 0), train_loss = 74.434, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.1732704639434814\n",
      "iter 543 (epoch 0), train_loss = 80.216, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.075260877609253\n",
      "iter 544 (epoch 0), train_loss = 64.412, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.124827146530151\n",
      "iter 545 (epoch 0), train_loss = 69.856, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 5.265022039413452\n",
      "iter 546 (epoch 0), train_loss = 67.354, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.1637139320373535\n",
      "iter 547 (epoch 0), train_loss = 68.803, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.10991096496582\n",
      "iter 548 (epoch 0), train_loss = 84.300, time/batch = 0.548\n",
      "EVALUATING\n",
      "Read data: 5.0852766036987305\n",
      "iter 549 (epoch 0), train_loss = 70.329, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.197030544281006\n",
      "iter 550 (epoch 0), train_loss = 72.004, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.300470590591431\n",
      "iter 551 (epoch 0), train_loss = 77.039, time/batch = 0.490\n",
      "EVALUATING\n",
      "Read data: 4.933323860168457\n",
      "iter 552 (epoch 0), train_loss = 69.204, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.064962863922119\n",
      "iter 553 (epoch 0), train_loss = 68.220, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.110305070877075\n",
      "iter 554 (epoch 0), train_loss = 78.998, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.108194589614868\n",
      "iter 555 (epoch 0), train_loss = 72.846, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.241050958633423\n",
      "iter 556 (epoch 0), train_loss = 70.782, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.0559186935424805\n",
      "iter 557 (epoch 0), train_loss = 64.226, time/batch = 0.488\n",
      "EVALUATING\n",
      "Read data: 5.121827125549316\n",
      "iter 558 (epoch 0), train_loss = 75.251, time/batch = 0.556\n",
      "EVALUATING\n",
      "Read data: 5.208587646484375\n",
      "iter 559 (epoch 0), train_loss = 68.124, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.0849769115448\n",
      "iter 560 (epoch 0), train_loss = 71.346, time/batch = 0.392\n",
      "EVALUATING\n",
      "Read data: 5.126995801925659\n",
      "iter 561 (epoch 0), train_loss = 64.675, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.143695831298828\n",
      "iter 562 (epoch 0), train_loss = 70.836, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.1575212478637695\n",
      "iter 563 (epoch 0), train_loss = 82.096, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.096741676330566\n",
      "iter 564 (epoch 0), train_loss = 81.475, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.107632637023926\n",
      "iter 565 (epoch 0), train_loss = 71.089, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 5.269414186477661\n",
      "iter 566 (epoch 0), train_loss = 70.094, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.17886209487915\n",
      "iter 567 (epoch 0), train_loss = 67.887, time/batch = 0.507\n",
      "EVALUATING\n",
      "Read data: 5.104469060897827\n",
      "iter 568 (epoch 0), train_loss = 67.957, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.147303581237793\n",
      "iter 569 (epoch 0), train_loss = 76.662, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.174326419830322\n",
      "iter 570 (epoch 0), train_loss = 73.798, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.221606969833374\n",
      "iter 571 (epoch 0), train_loss = 79.664, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.326777219772339\n",
      "iter 572 (epoch 0), train_loss = 70.996, time/batch = 0.510\n",
      "EVALUATING\n",
      "Read data: 5.007607936859131\n",
      "iter 573 (epoch 0), train_loss = 68.232, time/batch = 0.376\n",
      "EVALUATING\n",
      "Read data: 5.082567453384399\n",
      "iter 574 (epoch 0), train_loss = 72.274, time/batch = 0.408\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.220258951187134\n",
      "iter 575 (epoch 0), train_loss = 73.704, time/batch = 0.406\n",
      "EVALUATING\n",
      "Read data: 5.052279949188232\n",
      "iter 576 (epoch 0), train_loss = 72.001, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 5.01711368560791\n",
      "iter 577 (epoch 0), train_loss = 79.346, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.092884063720703\n",
      "iter 578 (epoch 0), train_loss = 66.498, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.246825933456421\n",
      "iter 579 (epoch 0), train_loss = 77.452, time/batch = 0.458\n",
      "EVALUATING\n",
      "Read data: 5.187680244445801\n",
      "iter 580 (epoch 0), train_loss = 76.371, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.1612324714660645\n",
      "iter 581 (epoch 0), train_loss = 69.406, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.145395755767822\n",
      "iter 582 (epoch 0), train_loss = 70.284, time/batch = 0.384\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 5.559663534164429\n",
      "iter 583 (epoch 0), train_loss = 75.185, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.051437854766846\n",
      "iter 584 (epoch 0), train_loss = 70.100, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.221678972244263\n",
      "iter 585 (epoch 0), train_loss = 68.999, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.218838930130005\n",
      "iter 586 (epoch 0), train_loss = 72.405, time/batch = 0.523\n",
      "EVALUATING\n",
      "Read data: 5.284693002700806\n",
      "iter 587 (epoch 0), train_loss = 68.177, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.155623435974121\n",
      "iter 588 (epoch 0), train_loss = 70.235, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.240936040878296\n",
      "iter 589 (epoch 0), train_loss = 69.877, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.058534383773804\n",
      "iter 590 (epoch 0), train_loss = 71.761, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.051386594772339\n",
      "iter 591 (epoch 0), train_loss = 68.783, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.225932598114014\n",
      "iter 592 (epoch 0), train_loss = 81.372, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.148144721984863\n",
      "iter 593 (epoch 0), train_loss = 71.620, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.236622333526611\n",
      "iter 594 (epoch 0), train_loss = 70.369, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.12409234046936\n",
      "iter 595 (epoch 0), train_loss = 65.731, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.036262035369873\n",
      "iter 596 (epoch 0), train_loss = 76.249, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.151704549789429\n",
      "iter 597 (epoch 0), train_loss = 74.516, time/batch = 0.522\n",
      "EVALUATING\n",
      "Read data: 4.992685556411743\n",
      "iter 598 (epoch 0), train_loss = 74.710, time/batch = 0.434\n",
      "EVALUATING\n",
      "Read data: 5.076797723770142\n",
      "iter 599 (epoch 0), train_loss = 64.700, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.124908924102783\n",
      "iter 600 (epoch 0), train_loss = 69.550, time/batch = 0.474\n",
      "EVALUATING\n",
      "Read data: 4.976680040359497\n",
      "iter 601 (epoch 0), train_loss = 62.030, time/batch = 0.300\n",
      "EVALUATING\n",
      "Read data: 5.1290154457092285\n",
      "iter 602 (epoch 0), train_loss = 70.948, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 5.113118886947632\n",
      "iter 603 (epoch 0), train_loss = 73.104, time/batch = 0.507\n",
      "EVALUATING\n",
      "Read data: 5.2857887744903564\n",
      "iter 604 (epoch 0), train_loss = 73.899, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.114932060241699\n",
      "iter 605 (epoch 0), train_loss = 79.390, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.0845232009887695\n",
      "iter 606 (epoch 0), train_loss = 68.203, time/batch = 0.362\n",
      "EVALUATING\n",
      "Read data: 5.214497804641724\n",
      "iter 607 (epoch 0), train_loss = 79.147, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.247773170471191\n",
      "iter 608 (epoch 0), train_loss = 65.865, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.191391229629517\n",
      "iter 609 (epoch 0), train_loss = 65.947, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.161915063858032\n",
      "iter 610 (epoch 0), train_loss = 68.453, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.112314224243164\n",
      "iter 611 (epoch 0), train_loss = 70.918, time/batch = 0.577\n",
      "EVALUATING\n",
      "Read data: 5.1400146484375\n",
      "iter 612 (epoch 0), train_loss = 78.897, time/batch = 0.374\n",
      "EVALUATING\n",
      "Read data: 5.060574054718018\n",
      "iter 613 (epoch 0), train_loss = 62.562, time/batch = 0.310\n",
      "EVALUATING\n",
      "Read data: 5.256829738616943\n",
      "iter 614 (epoch 0), train_loss = 77.896, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.1582934856414795\n",
      "iter 615 (epoch 0), train_loss = 75.120, time/batch = 0.492\n",
      "EVALUATING\n",
      "Read data: 5.199080944061279\n",
      "iter 616 (epoch 0), train_loss = 77.895, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 5.24864387512207\n",
      "iter 617 (epoch 0), train_loss = 78.337, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 4.990302562713623\n",
      "iter 618 (epoch 0), train_loss = 73.216, time/batch = 0.392\n",
      "EVALUATING\n",
      "Read data: 4.914257049560547\n",
      "iter 619 (epoch 0), train_loss = 65.989, time/batch = 0.504\n",
      "EVALUATING\n",
      "Read data: 5.013357877731323\n",
      "iter 620 (epoch 0), train_loss = 73.104, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 5.280794620513916\n",
      "iter 621 (epoch 0), train_loss = 68.675, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.03416895866394\n",
      "iter 622 (epoch 0), train_loss = 67.237, time/batch = 0.372\n",
      "EVALUATING\n",
      "Read data: 5.1698479652404785\n",
      "iter 623 (epoch 0), train_loss = 73.755, time/batch = 0.547\n",
      "EVALUATING\n",
      "Read data: 5.095069646835327\n",
      "iter 624 (epoch 0), train_loss = 71.407, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.224496603012085\n",
      "iter 625 (epoch 0), train_loss = 79.176, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.107177495956421\n",
      "iter 626 (epoch 0), train_loss = 58.877, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.170670747756958\n",
      "iter 627 (epoch 0), train_loss = 69.514, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.192230224609375\n",
      "iter 628 (epoch 0), train_loss = 74.073, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.1290764808654785\n",
      "iter 629 (epoch 0), train_loss = 80.052, time/batch = 0.833\n",
      "EVALUATING\n",
      "Read data: 5.075323820114136\n",
      "iter 630 (epoch 0), train_loss = 76.316, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 4.964961528778076\n",
      "iter 631 (epoch 0), train_loss = 73.972, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.04732608795166\n",
      "iter 632 (epoch 0), train_loss = 69.874, time/batch = 0.320\n",
      "EVALUATING\n",
      "Read data: 5.117345094680786\n",
      "iter 633 (epoch 0), train_loss = 65.050, time/batch = 0.358\n",
      "EVALUATING\n",
      "Read data: 4.981430768966675\n",
      "iter 634 (epoch 0), train_loss = 68.297, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.055408716201782\n",
      "iter 635 (epoch 0), train_loss = 76.943, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 4.982347249984741\n",
      "iter 636 (epoch 0), train_loss = 66.286, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.068880081176758\n",
      "iter 637 (epoch 0), train_loss = 69.871, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.067192792892456\n",
      "iter 638 (epoch 0), train_loss = 74.379, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.238551616668701\n",
      "iter 639 (epoch 0), train_loss = 59.641, time/batch = 0.284\n",
      "EVALUATING\n",
      "Read data: 4.906778812408447\n",
      "iter 640 (epoch 0), train_loss = 64.894, time/batch = 0.385\n",
      "EVALUATING\n",
      "Read data: 4.890536785125732\n",
      "iter 641 (epoch 0), train_loss = 65.568, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.188178539276123\n",
      "iter 642 (epoch 0), train_loss = 75.136, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.135544061660767\n",
      "iter 643 (epoch 0), train_loss = 76.441, time/batch = 0.550\n",
      "EVALUATING\n",
      "Read data: 5.289577484130859\n",
      "iter 644 (epoch 0), train_loss = 69.857, time/batch = 0.410\n",
      "EVALUATING\n",
      "Read data: 5.263495206832886\n",
      "iter 645 (epoch 0), train_loss = 64.033, time/batch = 0.335\n",
      "EVALUATING\n",
      "Read data: 5.015859842300415\n",
      "iter 646 (epoch 0), train_loss = 64.636, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.0569987297058105\n",
      "iter 647 (epoch 0), train_loss = 71.181, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.135288238525391\n",
      "iter 648 (epoch 0), train_loss = 69.842, time/batch = 0.322\n",
      "EVALUATING\n",
      "Read data: 5.2038047313690186\n",
      "iter 649 (epoch 0), train_loss = 69.275, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.074316501617432\n",
      "iter 650 (epoch 0), train_loss = 69.296, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.267137765884399\n",
      "iter 651 (epoch 0), train_loss = 67.647, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 4.988501310348511\n",
      "iter 652 (epoch 0), train_loss = 71.272, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.228178262710571\n",
      "iter 653 (epoch 0), train_loss = 61.944, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.221188068389893\n",
      "iter 654 (epoch 0), train_loss = 70.782, time/batch = 0.506\n",
      "EVALUATING\n",
      "Read data: 5.05970573425293\n",
      "iter 655 (epoch 0), train_loss = 74.183, time/batch = 0.506\n",
      "EVALUATING\n",
      "Read data: 5.012779474258423\n",
      "iter 656 (epoch 0), train_loss = 66.609, time/batch = 0.414\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.128607749938965\n",
      "iter 657 (epoch 0), train_loss = 77.391, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.215787649154663\n",
      "iter 658 (epoch 0), train_loss = 65.831, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.001181364059448\n",
      "iter 659 (epoch 0), train_loss = 63.244, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.241398811340332\n",
      "iter 660 (epoch 0), train_loss = 78.116, time/batch = 0.519\n",
      "EVALUATING\n",
      "Read data: 5.044753074645996\n",
      "iter 661 (epoch 0), train_loss = 75.531, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.1289002895355225\n",
      "iter 662 (epoch 0), train_loss = 71.647, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 5.064842224121094\n",
      "iter 663 (epoch 0), train_loss = 74.953, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.020463466644287\n",
      "iter 664 (epoch 0), train_loss = 71.448, time/batch = 0.509\n",
      "EVALUATING\n",
      "Read data: 5.063253879547119\n",
      "iter 665 (epoch 0), train_loss = 72.667, time/batch = 0.513\n",
      "EVALUATING\n",
      "Read data: 5.173713445663452\n",
      "iter 666 (epoch 0), train_loss = 72.940, time/batch = 0.584\n",
      "EVALUATING\n",
      "Read data: 4.991212844848633\n",
      "iter 667 (epoch 0), train_loss = 72.112, time/batch = 0.396\n",
      "EVALUATING\n",
      "Read data: 5.116306781768799\n",
      "iter 668 (epoch 0), train_loss = 70.827, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.090823650360107\n",
      "iter 669 (epoch 0), train_loss = 63.921, time/batch = 0.336\n",
      "EVALUATING\n",
      "Read data: 5.108384132385254\n",
      "iter 670 (epoch 0), train_loss = 70.491, time/batch = 0.536\n",
      "EVALUATING\n",
      "Read data: 5.0860631465911865\n",
      "iter 671 (epoch 0), train_loss = 72.612, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.023793697357178\n",
      "iter 672 (epoch 0), train_loss = 74.953, time/batch = 0.514\n",
      "EVALUATING\n",
      "Read data: 5.1825292110443115\n",
      "iter 673 (epoch 0), train_loss = 77.991, time/batch = 0.389\n",
      "EVALUATING\n",
      "Read data: 5.168025493621826\n",
      "iter 674 (epoch 0), train_loss = 73.842, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.179901599884033\n",
      "iter 675 (epoch 0), train_loss = 66.548, time/batch = 0.463\n",
      "EVALUATING\n",
      "Read data: 5.142185688018799\n",
      "iter 676 (epoch 0), train_loss = 69.239, time/batch = 0.579\n",
      "EVALUATING\n",
      "Read data: 5.135921239852905\n",
      "iter 677 (epoch 0), train_loss = 74.197, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.122084617614746\n",
      "iter 678 (epoch 0), train_loss = 68.577, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 5.190689325332642\n",
      "iter 679 (epoch 0), train_loss = 75.296, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.206954002380371\n",
      "iter 680 (epoch 0), train_loss = 69.593, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.1505348682403564\n",
      "iter 681 (epoch 0), train_loss = 75.832, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.134814739227295\n",
      "iter 682 (epoch 0), train_loss = 75.404, time/batch = 0.530\n",
      "EVALUATING\n",
      "Read data: 5.034977436065674\n",
      "iter 683 (epoch 0), train_loss = 60.699, time/batch = 0.338\n",
      "EVALUATING\n",
      "Read data: 5.148667335510254\n",
      "iter 684 (epoch 0), train_loss = 72.961, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.196834325790405\n",
      "iter 685 (epoch 0), train_loss = 72.342, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.237504482269287\n",
      "iter 686 (epoch 0), train_loss = 71.951, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.202499628067017\n",
      "iter 687 (epoch 0), train_loss = 72.261, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.0149619579315186\n",
      "iter 688 (epoch 0), train_loss = 69.152, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.2288336753845215\n",
      "iter 689 (epoch 0), train_loss = 69.871, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 5.038645505905151\n",
      "iter 690 (epoch 0), train_loss = 64.968, time/batch = 0.306\n",
      "EVALUATING\n",
      "Read data: 5.041022539138794\n",
      "iter 691 (epoch 0), train_loss = 67.062, time/batch = 0.460\n",
      "EVALUATING\n",
      "Read data: 5.105793476104736\n",
      "iter 692 (epoch 0), train_loss = 64.913, time/batch = 0.386\n",
      "EVALUATING\n",
      "Read data: 5.210614204406738\n",
      "iter 693 (epoch 0), train_loss = 72.480, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.099693059921265\n",
      "iter 694 (epoch 0), train_loss = 63.477, time/batch = 0.459\n",
      "EVALUATING\n",
      "Read data: 5.125720262527466\n",
      "iter 695 (epoch 0), train_loss = 68.380, time/batch = 0.577\n",
      "EVALUATING\n",
      "Read data: 5.154836654663086\n",
      "iter 696 (epoch 0), train_loss = 75.567, time/batch = 0.536\n",
      "EVALUATING\n",
      "Read data: 5.093715190887451\n",
      "iter 697 (epoch 0), train_loss = 74.427, time/batch = 0.524\n",
      "EVALUATING\n",
      "Read data: 5.262436389923096\n",
      "iter 698 (epoch 0), train_loss = 61.674, time/batch = 0.407\n",
      "EVALUATING\n",
      "Read data: 5.146661043167114\n",
      "iter 699 (epoch 0), train_loss = 66.471, time/batch = 0.360\n",
      "EVALUATING\n",
      "Read data: 5.162133455276489\n",
      "iter 700 (epoch 0), train_loss = 74.589, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 5.258087396621704\n",
      "iter 701 (epoch 0), train_loss = 74.606, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.156761169433594\n",
      "iter 702 (epoch 0), train_loss = 72.631, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.015121698379517\n",
      "iter 703 (epoch 0), train_loss = 66.574, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.011183261871338\n",
      "iter 704 (epoch 0), train_loss = 73.750, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.214818477630615\n",
      "iter 705 (epoch 0), train_loss = 76.580, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.21292781829834\n",
      "iter 706 (epoch 0), train_loss = 72.411, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.205644607543945\n",
      "iter 707 (epoch 0), train_loss = 66.226, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.184242486953735\n",
      "iter 708 (epoch 0), train_loss = 64.302, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.134053945541382\n",
      "iter 709 (epoch 0), train_loss = 75.061, time/batch = 0.510\n",
      "EVALUATING\n",
      "Read data: 4.923976421356201\n",
      "iter 710 (epoch 0), train_loss = 67.142, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.1373395919799805\n",
      "iter 711 (epoch 0), train_loss = 72.076, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.1958208084106445\n",
      "iter 712 (epoch 0), train_loss = 78.683, time/batch = 0.579\n",
      "EVALUATING\n",
      "Read data: 4.906949758529663\n",
      "iter 713 (epoch 0), train_loss = 60.219, time/batch = 0.352\n",
      "EVALUATING\n",
      "Read data: 5.220930814743042\n",
      "iter 714 (epoch 0), train_loss = 66.092, time/batch = 0.372\n",
      "EVALUATING\n",
      "Read data: 4.994478225708008\n",
      "iter 715 (epoch 0), train_loss = 77.833, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.100147008895874\n",
      "iter 716 (epoch 0), train_loss = 69.370, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.067898750305176\n",
      "iter 717 (epoch 0), train_loss = 69.660, time/batch = 0.529\n",
      "EVALUATING\n",
      "Read data: 5.17951226234436\n",
      "iter 718 (epoch 0), train_loss = 73.424, time/batch = 0.536\n",
      "EVALUATING\n",
      "Read data: 5.039191722869873\n",
      "iter 719 (epoch 0), train_loss = 61.958, time/batch = 0.322\n",
      "EVALUATING\n",
      "Read data: 5.138183832168579\n",
      "iter 720 (epoch 0), train_loss = 74.224, time/batch = 0.572\n",
      "EVALUATING\n",
      "Read data: 4.909618616104126\n",
      "iter 721 (epoch 0), train_loss = 71.178, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.073242902755737\n",
      "iter 722 (epoch 0), train_loss = 65.404, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.232908725738525\n",
      "iter 723 (epoch 0), train_loss = 73.732, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.11824631690979\n",
      "iter 724 (epoch 0), train_loss = 64.697, time/batch = 0.449\n",
      "EVALUATING\n",
      "Read data: 5.070887088775635\n",
      "iter 725 (epoch 0), train_loss = 69.086, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.133196592330933\n",
      "iter 726 (epoch 0), train_loss = 72.252, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.205469608306885\n",
      "iter 727 (epoch 0), train_loss = 73.713, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.002103328704834\n",
      "iter 728 (epoch 0), train_loss = 67.617, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 4.890823125839233\n",
      "iter 729 (epoch 0), train_loss = 70.949, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 4.869950294494629\n",
      "iter 730 (epoch 0), train_loss = 69.133, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.09391450881958\n",
      "iter 731 (epoch 0), train_loss = 64.151, time/batch = 0.362\n",
      "EVALUATING\n",
      "Read data: 5.04340934753418\n",
      "iter 732 (epoch 0), train_loss = 69.905, time/batch = 0.513\n",
      "EVALUATING\n",
      "Read data: 5.075170516967773\n",
      "iter 733 (epoch 0), train_loss = 75.559, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.126478910446167\n",
      "iter 734 (epoch 0), train_loss = 74.668, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.222062587738037\n",
      "iter 735 (epoch 0), train_loss = 77.447, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.224165201187134\n",
      "iter 736 (epoch 0), train_loss = 69.310, time/batch = 0.365\n",
      "EVALUATING\n",
      "Read data: 5.283156156539917\n",
      "iter 737 (epoch 0), train_loss = 71.311, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.133415699005127\n",
      "iter 738 (epoch 0), train_loss = 73.076, time/batch = 0.467\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.251756191253662\n",
      "iter 739 (epoch 0), train_loss = 65.026, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.132040023803711\n",
      "iter 740 (epoch 0), train_loss = 68.636, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.063517808914185\n",
      "iter 741 (epoch 0), train_loss = 65.076, time/batch = 0.408\n",
      "EVALUATING\n",
      "Read data: 5.083480358123779\n",
      "iter 742 (epoch 0), train_loss = 69.619, time/batch = 0.529\n",
      "EVALUATING\n",
      "Read data: 5.3004162311553955\n",
      "iter 743 (epoch 0), train_loss = 72.975, time/batch = 0.523\n",
      "EVALUATING\n",
      "Read data: 5.133348703384399\n",
      "iter 744 (epoch 0), train_loss = 70.116, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.275045871734619\n",
      "iter 745 (epoch 0), train_loss = 79.796, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.064539194107056\n",
      "iter 746 (epoch 0), train_loss = 67.817, time/batch = 0.511\n",
      "EVALUATING\n",
      "Read data: 5.217061281204224\n",
      "iter 747 (epoch 0), train_loss = 69.051, time/batch = 0.506\n",
      "EVALUATING\n",
      "Read data: 5.190019130706787\n",
      "iter 748 (epoch 0), train_loss = 76.997, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.193384647369385\n",
      "iter 749 (epoch 0), train_loss = 71.734, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.155295133590698\n",
      "iter 750 (epoch 0), train_loss = 73.189, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.079378366470337\n",
      "iter 751 (epoch 0), train_loss = 70.562, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.132675647735596\n",
      "iter 752 (epoch 0), train_loss = 74.103, time/batch = 0.427\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 9.183704853057861\n",
      "iter 753 (epoch 0), train_loss = 89.017, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.105096101760864\n",
      "iter 754 (epoch 0), train_loss = 71.994, time/batch = 0.516\n",
      "EVALUATING\n",
      "Read data: 5.008434534072876\n",
      "iter 755 (epoch 0), train_loss = 64.147, time/batch = 0.518\n",
      "EVALUATING\n",
      "Read data: 5.169696569442749\n",
      "iter 756 (epoch 0), train_loss = 74.779, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 5.134217977523804\n",
      "iter 757 (epoch 0), train_loss = 77.306, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 5.186177730560303\n",
      "iter 758 (epoch 0), train_loss = 78.952, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.1790244579315186\n",
      "iter 759 (epoch 0), train_loss = 64.428, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.085168838500977\n",
      "iter 760 (epoch 0), train_loss = 63.799, time/batch = 0.348\n",
      "EVALUATING\n",
      "Read data: 5.174175262451172\n",
      "iter 761 (epoch 0), train_loss = 69.383, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 5.052568435668945\n",
      "iter 762 (epoch 0), train_loss = 71.231, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 5.020435810089111\n",
      "iter 763 (epoch 0), train_loss = 70.604, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 4.954487562179565\n",
      "iter 764 (epoch 0), train_loss = 71.196, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.06757926940918\n",
      "iter 765 (epoch 0), train_loss = 75.468, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.25356650352478\n",
      "iter 766 (epoch 0), train_loss = 68.350, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.084502220153809\n",
      "iter 767 (epoch 0), train_loss = 69.268, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.234579563140869\n",
      "iter 768 (epoch 0), train_loss = 70.881, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.149364948272705\n",
      "iter 769 (epoch 0), train_loss = 76.408, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.102238893508911\n",
      "iter 770 (epoch 0), train_loss = 61.287, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 4.978822708129883\n",
      "iter 771 (epoch 0), train_loss = 74.190, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.158863306045532\n",
      "iter 772 (epoch 0), train_loss = 62.076, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 5.229140996932983\n",
      "iter 773 (epoch 0), train_loss = 76.134, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 4.9656407833099365\n",
      "iter 774 (epoch 0), train_loss = 75.580, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.170297145843506\n",
      "iter 775 (epoch 0), train_loss = 63.493, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.176681280136108\n",
      "iter 776 (epoch 0), train_loss = 66.622, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.051158905029297\n",
      "iter 777 (epoch 0), train_loss = 72.722, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.10055136680603\n",
      "iter 778 (epoch 0), train_loss = 65.116, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 4.927686452865601\n",
      "iter 779 (epoch 0), train_loss = 61.416, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.270148754119873\n",
      "iter 780 (epoch 0), train_loss = 72.669, time/batch = 0.490\n",
      "EVALUATING\n",
      "Read data: 5.044693231582642\n",
      "iter 781 (epoch 0), train_loss = 70.781, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.171928644180298\n",
      "iter 782 (epoch 0), train_loss = 73.685, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.209026098251343\n",
      "iter 783 (epoch 0), train_loss = 75.397, time/batch = 0.489\n",
      "EVALUATING\n",
      "Read data: 5.109671592712402\n",
      "iter 784 (epoch 0), train_loss = 72.340, time/batch = 0.556\n",
      "EVALUATING\n",
      "Read data: 5.276232004165649\n",
      "iter 785 (epoch 0), train_loss = 66.456, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.171356439590454\n",
      "iter 786 (epoch 0), train_loss = 67.016, time/batch = 0.507\n",
      "EVALUATING\n",
      "Read data: 5.187742233276367\n",
      "iter 787 (epoch 0), train_loss = 69.635, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.06579065322876\n",
      "iter 788 (epoch 0), train_loss = 70.498, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.123649597167969\n",
      "iter 789 (epoch 0), train_loss = 62.791, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.197680473327637\n",
      "iter 790 (epoch 0), train_loss = 72.432, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.1577513217926025\n",
      "iter 791 (epoch 0), train_loss = 80.589, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 4.8867645263671875\n",
      "iter 792 (epoch 0), train_loss = 66.648, time/batch = 0.500\n",
      "EVALUATING\n",
      "Read data: 5.165859699249268\n",
      "iter 793 (epoch 0), train_loss = 64.659, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.074132680892944\n",
      "iter 794 (epoch 0), train_loss = 77.283, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.195200204849243\n",
      "iter 795 (epoch 0), train_loss = 65.872, time/batch = 0.463\n",
      "EVALUATING\n",
      "Read data: 5.231255531311035\n",
      "iter 796 (epoch 0), train_loss = 72.016, time/batch = 0.503\n",
      "EVALUATING\n",
      "Read data: 4.9896626472473145\n",
      "iter 797 (epoch 0), train_loss = 65.188, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.163386821746826\n",
      "iter 798 (epoch 0), train_loss = 64.003, time/batch = 0.371\n",
      "EVALUATING\n",
      "Read data: 5.104620456695557\n",
      "iter 799 (epoch 0), train_loss = 63.226, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.108689069747925\n",
      "iter 800 (epoch 0), train_loss = 61.467, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.2758400440216064\n",
      "iter 801 (epoch 0), train_loss = 63.256, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 4.988085508346558\n",
      "iter 802 (epoch 0), train_loss = 71.467, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.153721332550049\n",
      "iter 803 (epoch 0), train_loss = 64.922, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.216751575469971\n",
      "iter 804 (epoch 0), train_loss = 71.740, time/batch = 0.523\n",
      "EVALUATING\n",
      "Read data: 5.230432510375977\n",
      "iter 805 (epoch 0), train_loss = 67.403, time/batch = 0.499\n",
      "EVALUATING\n",
      "Read data: 5.169587850570679\n",
      "iter 806 (epoch 0), train_loss = 68.051, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.268688678741455\n",
      "iter 807 (epoch 0), train_loss = 73.463, time/batch = 0.492\n",
      "EVALUATING\n",
      "Read data: 5.146302700042725\n",
      "iter 808 (epoch 0), train_loss = 69.372, time/batch = 0.459\n",
      "EVALUATING\n",
      "Read data: 5.138899564743042\n",
      "iter 809 (epoch 0), train_loss = 70.029, time/batch = 0.377\n",
      "EVALUATING\n",
      "Read data: 5.162451267242432\n",
      "iter 810 (epoch 0), train_loss = 72.789, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.068521022796631\n",
      "iter 811 (epoch 0), train_loss = 71.663, time/batch = 0.520\n",
      "EVALUATING\n",
      "Read data: 5.217149972915649\n",
      "iter 812 (epoch 0), train_loss = 65.899, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.158326864242554\n",
      "iter 813 (epoch 0), train_loss = 66.785, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 4.974463701248169\n",
      "iter 814 (epoch 0), train_loss = 62.980, time/batch = 0.385\n",
      "EVALUATING\n",
      "Read data: 5.12727689743042\n",
      "iter 815 (epoch 0), train_loss = 73.543, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.114504814147949\n",
      "iter 816 (epoch 0), train_loss = 73.554, time/batch = 0.464\n",
      "EVALUATING\n",
      "Read data: 4.982090950012207\n",
      "iter 817 (epoch 0), train_loss = 72.452, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 4.975918531417847\n",
      "iter 818 (epoch 0), train_loss = 71.709, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.095404148101807\n",
      "iter 819 (epoch 0), train_loss = 74.661, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.158380746841431\n",
      "iter 820 (epoch 0), train_loss = 69.936, time/batch = 0.369\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.179772138595581\n",
      "iter 821 (epoch 0), train_loss = 67.481, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.014463424682617\n",
      "iter 822 (epoch 0), train_loss = 63.780, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.1215925216674805\n",
      "iter 823 (epoch 0), train_loss = 69.279, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.1354453563690186\n",
      "iter 824 (epoch 0), train_loss = 70.498, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 4.903175592422485\n",
      "iter 825 (epoch 0), train_loss = 75.627, time/batch = 0.532\n",
      "EVALUATING\n",
      "Read data: 5.268589019775391\n",
      "iter 826 (epoch 0), train_loss = 66.426, time/batch = 0.419\n",
      "EVALUATING\n",
      "Read data: 4.9861414432525635\n",
      "iter 827 (epoch 0), train_loss = 61.284, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.152299642562866\n",
      "iter 828 (epoch 0), train_loss = 65.610, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.101354598999023\n",
      "iter 829 (epoch 0), train_loss = 71.484, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.139620542526245\n",
      "iter 830 (epoch 0), train_loss = 65.742, time/batch = 0.338\n",
      "EVALUATING\n",
      "Read data: 5.08711576461792\n",
      "iter 831 (epoch 0), train_loss = 69.293, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.274539232254028\n",
      "iter 832 (epoch 0), train_loss = 76.861, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.016429424285889\n",
      "iter 833 (epoch 0), train_loss = 70.558, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.072988986968994\n",
      "iter 834 (epoch 0), train_loss = 65.278, time/batch = 0.464\n",
      "EVALUATING\n",
      "Read data: 5.207935094833374\n",
      "iter 835 (epoch 0), train_loss = 63.990, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.120147943496704\n",
      "iter 836 (epoch 0), train_loss = 73.410, time/batch = 0.420\n",
      "EVALUATING\n",
      "Read data: 5.215937614440918\n",
      "iter 837 (epoch 0), train_loss = 67.254, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 4.964705467224121\n",
      "iter 838 (epoch 0), train_loss = 66.983, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 4.9793524742126465\n",
      "iter 839 (epoch 0), train_loss = 79.126, time/batch = 0.550\n",
      "EVALUATING\n",
      "Read data: 5.020892381668091\n",
      "iter 840 (epoch 0), train_loss = 65.940, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 4.984961986541748\n",
      "iter 841 (epoch 0), train_loss = 69.457, time/batch = 0.362\n",
      "EVALUATING\n",
      "Read data: 5.041174650192261\n",
      "iter 842 (epoch 0), train_loss = 72.799, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.252561330795288\n",
      "iter 843 (epoch 0), train_loss = 69.578, time/batch = 0.506\n",
      "EVALUATING\n",
      "Read data: 5.2795374393463135\n",
      "iter 844 (epoch 0), train_loss = 68.301, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.087973594665527\n",
      "iter 845 (epoch 0), train_loss = 66.165, time/batch = 0.400\n",
      "EVALUATING\n",
      "Read data: 5.0988733768463135\n",
      "iter 846 (epoch 0), train_loss = 74.415, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.1290295124053955\n",
      "iter 847 (epoch 0), train_loss = 68.939, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.042150259017944\n",
      "iter 848 (epoch 0), train_loss = 68.980, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.136756181716919\n",
      "iter 849 (epoch 0), train_loss = 58.692, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.055363178253174\n",
      "iter 850 (epoch 0), train_loss = 67.981, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.132578134536743\n",
      "iter 851 (epoch 0), train_loss = 71.667, time/batch = 0.459\n",
      "EVALUATING\n",
      "Read data: 5.09387469291687\n",
      "iter 852 (epoch 0), train_loss = 71.512, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.139942407608032\n",
      "iter 853 (epoch 0), train_loss = 64.005, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.014943838119507\n",
      "iter 854 (epoch 0), train_loss = 64.484, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.294251441955566\n",
      "iter 855 (epoch 0), train_loss = 67.400, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.046597003936768\n",
      "iter 856 (epoch 0), train_loss = 70.578, time/batch = 0.476\n",
      "EVALUATING\n",
      "Read data: 5.0033323764801025\n",
      "iter 857 (epoch 0), train_loss = 65.617, time/batch = 0.458\n",
      "EVALUATING\n",
      "Read data: 5.197117328643799\n",
      "iter 858 (epoch 0), train_loss = 72.790, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.039860010147095\n",
      "iter 859 (epoch 0), train_loss = 61.861, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.260436058044434\n",
      "iter 860 (epoch 0), train_loss = 68.427, time/batch = 0.548\n",
      "EVALUATING\n",
      "Read data: 5.213765621185303\n",
      "iter 861 (epoch 0), train_loss = 70.580, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 4.955024242401123\n",
      "iter 862 (epoch 0), train_loss = 62.000, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.300796270370483\n",
      "iter 863 (epoch 0), train_loss = 79.976, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.023175954818726\n",
      "iter 864 (epoch 0), train_loss = 68.874, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.216284275054932\n",
      "iter 865 (epoch 0), train_loss = 58.772, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.25965142250061\n",
      "iter 866 (epoch 0), train_loss = 66.609, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.3056676387786865\n",
      "iter 867 (epoch 0), train_loss = 76.830, time/batch = 0.394\n",
      "EVALUATING\n",
      "Read data: 5.213641166687012\n",
      "iter 868 (epoch 0), train_loss = 76.048, time/batch = 0.520\n",
      "EVALUATING\n",
      "Read data: 5.243350028991699\n",
      "iter 869 (epoch 0), train_loss = 73.137, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.088102579116821\n",
      "iter 870 (epoch 0), train_loss = 73.025, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.100945949554443\n",
      "iter 871 (epoch 0), train_loss = 67.355, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.074355840682983\n",
      "iter 872 (epoch 0), train_loss = 66.004, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.14798903465271\n",
      "iter 873 (epoch 0), train_loss = 62.114, time/batch = 0.389\n",
      "EVALUATING\n",
      "Read data: 5.062886476516724\n",
      "iter 874 (epoch 0), train_loss = 66.118, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.05388069152832\n",
      "iter 875 (epoch 0), train_loss = 83.115, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.06014609336853\n",
      "iter 876 (epoch 0), train_loss = 69.205, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.108233213424683\n",
      "iter 877 (epoch 0), train_loss = 71.439, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.1653008460998535\n",
      "iter 878 (epoch 0), train_loss = 75.276, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 4.98865818977356\n",
      "iter 879 (epoch 0), train_loss = 65.481, time/batch = 0.363\n",
      "EVALUATING\n",
      "Read data: 5.293312311172485\n",
      "iter 880 (epoch 0), train_loss = 70.248, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.091207265853882\n",
      "iter 881 (epoch 0), train_loss = 70.703, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 4.973459005355835\n",
      "iter 882 (epoch 0), train_loss = 64.323, time/batch = 0.379\n",
      "EVALUATING\n",
      "Read data: 5.104331016540527\n",
      "iter 883 (epoch 0), train_loss = 69.306, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.23220157623291\n",
      "iter 884 (epoch 0), train_loss = 57.356, time/batch = 0.331\n",
      "EVALUATING\n",
      "Read data: 5.027095079421997\n",
      "iter 885 (epoch 0), train_loss = 62.044, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.211028814315796\n",
      "iter 886 (epoch 0), train_loss = 74.731, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 4.815173149108887\n",
      "iter 887 (epoch 0), train_loss = 73.138, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 4.991098403930664\n",
      "iter 888 (epoch 0), train_loss = 68.348, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.217261552810669\n",
      "iter 889 (epoch 0), train_loss = 72.954, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.243761301040649\n",
      "iter 890 (epoch 0), train_loss = 62.320, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 4.978414297103882\n",
      "iter 891 (epoch 0), train_loss = 70.138, time/batch = 0.434\n",
      "EVALUATING\n",
      "Read data: 5.076517343521118\n",
      "iter 892 (epoch 0), train_loss = 60.795, time/batch = 0.464\n",
      "EVALUATING\n",
      "Read data: 5.297295808792114\n",
      "iter 893 (epoch 0), train_loss = 75.281, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.266399621963501\n",
      "iter 894 (epoch 0), train_loss = 71.448, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.15517258644104\n",
      "iter 895 (epoch 0), train_loss = 71.276, time/batch = 0.509\n",
      "EVALUATING\n",
      "Read data: 4.955008029937744\n",
      "iter 896 (epoch 0), train_loss = 69.672, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.260934591293335\n",
      "iter 897 (epoch 0), train_loss = 71.674, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.156342506408691\n",
      "iter 898 (epoch 0), train_loss = 62.706, time/batch = 0.511\n",
      "EVALUATING\n",
      "Read data: 4.959599494934082\n",
      "iter 899 (epoch 0), train_loss = 68.785, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.154198408126831\n",
      "iter 900 (epoch 0), train_loss = 72.407, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.221153974533081\n",
      "iter 901 (epoch 0), train_loss = 65.722, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.192104339599609\n",
      "iter 902 (epoch 0), train_loss = 72.074, time/batch = 0.472\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.17915153503418\n",
      "iter 903 (epoch 0), train_loss = 71.149, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.077527046203613\n",
      "iter 904 (epoch 0), train_loss = 66.221, time/batch = 0.382\n",
      "EVALUATING\n",
      "Read data: 5.179589748382568\n",
      "iter 905 (epoch 0), train_loss = 76.590, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.133744478225708\n",
      "iter 906 (epoch 0), train_loss = 68.602, time/batch = 0.366\n",
      "EVALUATING\n",
      "Read data: 4.973137855529785\n",
      "iter 907 (epoch 0), train_loss = 71.166, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.266362428665161\n",
      "iter 908 (epoch 0), train_loss = 72.763, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.08604097366333\n",
      "iter 909 (epoch 0), train_loss = 61.194, time/batch = 0.501\n",
      "EVALUATING\n",
      "Read data: 5.108527660369873\n",
      "iter 910 (epoch 0), train_loss = 63.926, time/batch = 0.386\n",
      "EVALUATING\n",
      "Read data: 4.996111869812012\n",
      "iter 911 (epoch 0), train_loss = 63.861, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.271395444869995\n",
      "iter 912 (epoch 0), train_loss = 68.888, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.121302604675293\n",
      "iter 913 (epoch 0), train_loss = 64.379, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.233450889587402\n",
      "iter 914 (epoch 0), train_loss = 74.766, time/batch = 0.537\n",
      "EVALUATING\n",
      "Read data: 5.111499786376953\n",
      "iter 915 (epoch 0), train_loss = 61.114, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.054143667221069\n",
      "iter 916 (epoch 0), train_loss = 65.170, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.222838878631592\n",
      "iter 917 (epoch 0), train_loss = 70.482, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 5.197768211364746\n",
      "iter 918 (epoch 0), train_loss = 65.898, time/batch = 0.366\n",
      "EVALUATING\n",
      "Read data: 5.0506157875061035\n",
      "iter 919 (epoch 0), train_loss = 78.609, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.224403142929077\n",
      "iter 920 (epoch 0), train_loss = 66.964, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.115002632141113\n",
      "iter 921 (epoch 0), train_loss = 64.218, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.091184616088867\n",
      "iter 922 (epoch 0), train_loss = 62.870, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.182299375534058\n",
      "iter 923 (epoch 0), train_loss = 73.084, time/batch = 0.507\n",
      "EVALUATING\n",
      "Read data: 4.977871417999268\n",
      "iter 924 (epoch 0), train_loss = 62.339, time/batch = 0.325\n",
      "EVALUATING\n",
      "Read data: 5.076760292053223\n",
      "iter 925 (epoch 0), train_loss = 71.853, time/batch = 0.359\n",
      "EVALUATING\n",
      "Read data: 5.040868043899536\n",
      "iter 926 (epoch 0), train_loss = 66.393, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.213041067123413\n",
      "iter 927 (epoch 0), train_loss = 71.072, time/batch = 0.386\n",
      "EVALUATING\n",
      "Read data: 5.165304899215698\n",
      "iter 928 (epoch 0), train_loss = 76.315, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.140118360519409\n",
      "iter 929 (epoch 0), train_loss = 75.549, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.185719966888428\n",
      "iter 930 (epoch 0), train_loss = 62.976, time/batch = 0.461\n",
      "EVALUATING\n",
      "Read data: 5.157292127609253\n",
      "iter 931 (epoch 0), train_loss = 61.332, time/batch = 0.349\n",
      "EVALUATING\n",
      "Read data: 5.299804925918579\n",
      "iter 932 (epoch 0), train_loss = 61.311, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 5.329283237457275\n",
      "iter 933 (epoch 0), train_loss = 74.525, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.3067638874053955\n",
      "iter 934 (epoch 0), train_loss = 71.192, time/batch = 0.449\n",
      "EVALUATING\n",
      "Read data: 5.203231334686279\n",
      "iter 935 (epoch 0), train_loss = 68.940, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 4.978567838668823\n",
      "iter 936 (epoch 0), train_loss = 62.592, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.111292600631714\n",
      "iter 937 (epoch 0), train_loss = 84.344, time/batch = 0.595\n",
      "EVALUATING\n",
      "Read data: 5.031228303909302\n",
      "iter 938 (epoch 0), train_loss = 67.998, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.068019151687622\n",
      "iter 939 (epoch 0), train_loss = 64.028, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.277910947799683\n",
      "iter 940 (epoch 0), train_loss = 71.166, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.298008441925049\n",
      "iter 941 (epoch 0), train_loss = 63.960, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 4.990943670272827\n",
      "iter 942 (epoch 0), train_loss = 65.232, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 4.96422553062439\n",
      "iter 943 (epoch 0), train_loss = 65.794, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.201953649520874\n",
      "iter 944 (epoch 0), train_loss = 65.887, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.099707126617432\n",
      "iter 945 (epoch 0), train_loss = 64.729, time/batch = 0.519\n",
      "EVALUATING\n",
      "Read data: 5.274139404296875\n",
      "iter 946 (epoch 0), train_loss = 70.818, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.146830320358276\n",
      "iter 947 (epoch 0), train_loss = 77.136, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.271743297576904\n",
      "iter 948 (epoch 0), train_loss = 72.266, time/batch = 0.373\n",
      "EVALUATING\n",
      "Read data: 5.175871133804321\n",
      "iter 949 (epoch 0), train_loss = 66.304, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.138620615005493\n",
      "iter 950 (epoch 0), train_loss = 70.154, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 4.920673131942749\n",
      "iter 951 (epoch 0), train_loss = 66.788, time/batch = 0.350\n",
      "EVALUATING\n",
      "Read data: 5.131455659866333\n",
      "iter 952 (epoch 0), train_loss = 68.681, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.171428680419922\n",
      "iter 953 (epoch 0), train_loss = 59.902, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.173543453216553\n",
      "iter 954 (epoch 0), train_loss = 71.707, time/batch = 0.460\n",
      "EVALUATING\n",
      "Read data: 5.184899568557739\n",
      "iter 955 (epoch 0), train_loss = 61.951, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.066792964935303\n",
      "iter 956 (epoch 0), train_loss = 67.531, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 4.979119300842285\n",
      "iter 957 (epoch 0), train_loss = 67.618, time/batch = 0.435\n",
      "EVALUATING\n",
      "Read data: 5.087907075881958\n",
      "iter 958 (epoch 0), train_loss = 61.585, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.203370094299316\n",
      "iter 959 (epoch 0), train_loss = 75.349, time/batch = 0.443\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 9.03975534439087\n",
      "iter 960 (epoch 0), train_loss = 83.214, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.190418243408203\n",
      "iter 961 (epoch 0), train_loss = 67.988, time/batch = 0.490\n",
      "EVALUATING\n",
      "Read data: 5.166249752044678\n",
      "iter 962 (epoch 0), train_loss = 75.755, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.227590560913086\n",
      "iter 963 (epoch 0), train_loss = 58.011, time/batch = 0.389\n",
      "EVALUATING\n",
      "Read data: 5.295430898666382\n",
      "iter 964 (epoch 0), train_loss = 79.026, time/batch = 0.536\n",
      "EVALUATING\n",
      "Read data: 5.2123939990997314\n",
      "iter 965 (epoch 0), train_loss = 67.365, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.386968374252319\n",
      "iter 966 (epoch 0), train_loss = 69.318, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.3425352573394775\n",
      "iter 967 (epoch 0), train_loss = 67.004, time/batch = 0.560\n",
      "EVALUATING\n",
      "Read data: 5.080990314483643\n",
      "iter 968 (epoch 0), train_loss = 62.593, time/batch = 0.409\n",
      "EVALUATING\n",
      "Read data: 5.21743369102478\n",
      "iter 969 (epoch 0), train_loss = 64.882, time/batch = 0.525\n",
      "EVALUATING\n",
      "Read data: 5.134260654449463\n",
      "iter 970 (epoch 0), train_loss = 63.987, time/batch = 0.435\n",
      "EVALUATING\n",
      "Read data: 5.044086694717407\n",
      "iter 971 (epoch 0), train_loss = 67.895, time/batch = 0.513\n",
      "EVALUATING\n",
      "Read data: 5.054762601852417\n",
      "iter 972 (epoch 0), train_loss = 66.679, time/batch = 0.362\n",
      "EVALUATING\n",
      "Read data: 4.938616752624512\n",
      "iter 973 (epoch 0), train_loss = 58.897, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.231276512145996\n",
      "iter 974 (epoch 0), train_loss = 64.290, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.1458823680877686\n",
      "iter 975 (epoch 0), train_loss = 68.442, time/batch = 0.392\n",
      "EVALUATING\n",
      "Read data: 5.267404079437256\n",
      "iter 976 (epoch 0), train_loss = 67.783, time/batch = 0.496\n",
      "EVALUATING\n",
      "Read data: 5.018009901046753\n",
      "iter 977 (epoch 0), train_loss = 70.482, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.076450824737549\n",
      "iter 978 (epoch 0), train_loss = 73.983, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.247575521469116\n",
      "iter 979 (epoch 0), train_loss = 65.184, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.280062198638916\n",
      "iter 980 (epoch 0), train_loss = 60.218, time/batch = 0.374\n",
      "EVALUATING\n",
      "Read data: 5.211762428283691\n",
      "iter 981 (epoch 0), train_loss = 74.112, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.067652225494385\n",
      "iter 982 (epoch 0), train_loss = 72.312, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.292956113815308\n",
      "iter 983 (epoch 0), train_loss = 67.371, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.029159069061279\n",
      "iter 984 (epoch 0), train_loss = 68.510, time/batch = 0.428\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.074425935745239\n",
      "iter 985 (epoch 0), train_loss = 69.987, time/batch = 0.519\n",
      "EVALUATING\n",
      "Read data: 5.219332933425903\n",
      "iter 986 (epoch 0), train_loss = 81.616, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.086144208908081\n",
      "iter 987 (epoch 0), train_loss = 64.282, time/batch = 0.526\n",
      "EVALUATING\n",
      "Read data: 5.054935455322266\n",
      "iter 988 (epoch 0), train_loss = 60.863, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.0699076652526855\n",
      "iter 989 (epoch 0), train_loss = 64.240, time/batch = 0.614\n",
      "EVALUATING\n",
      "Read data: 5.161253452301025\n",
      "iter 990 (epoch 0), train_loss = 67.810, time/batch = 0.410\n",
      "EVALUATING\n",
      "Read data: 5.0414628982543945\n",
      "iter 991 (epoch 0), train_loss = 69.035, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.214266777038574\n",
      "iter 992 (epoch 0), train_loss = 63.954, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.20705771446228\n",
      "iter 993 (epoch 0), train_loss = 63.955, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.0734288692474365\n",
      "iter 994 (epoch 0), train_loss = 63.640, time/batch = 0.386\n",
      "EVALUATING\n",
      "Read data: 5.212619304656982\n",
      "iter 995 (epoch 0), train_loss = 72.027, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.239711761474609\n",
      "iter 996 (epoch 0), train_loss = 65.135, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.128900527954102\n",
      "iter 997 (epoch 0), train_loss = 76.203, time/batch = 0.510\n",
      "EVALUATING\n",
      "Read data: 5.218780994415283\n",
      "iter 998 (epoch 0), train_loss = 74.933, time/batch = 0.537\n",
      "EVALUATING\n",
      "Read data: 5.163011074066162\n",
      "iter 999 (epoch 0), train_loss = 70.800, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.02010703086853\n",
      "iter 1000 (epoch 0), train_loss = 65.195, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.2195374965667725\n",
      "iter 1001 (epoch 0), train_loss = 63.881, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.063931226730347\n",
      "iter 1002 (epoch 0), train_loss = 59.589, time/batch = 0.351\n",
      "EVALUATING\n",
      "Read data: 5.154325485229492\n",
      "iter 1003 (epoch 0), train_loss = 69.310, time/batch = 0.576\n",
      "EVALUATING\n",
      "Read data: 5.1521172523498535\n",
      "iter 1004 (epoch 0), train_loss = 70.661, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.246924877166748\n",
      "iter 1005 (epoch 0), train_loss = 65.572, time/batch = 0.363\n",
      "EVALUATING\n",
      "Read data: 4.978080987930298\n",
      "iter 1006 (epoch 0), train_loss = 63.592, time/batch = 0.447\n",
      "EVALUATING\n",
      "Read data: 5.25618052482605\n",
      "iter 1007 (epoch 0), train_loss = 64.249, time/batch = 0.506\n",
      "EVALUATING\n",
      "Read data: 5.1363232135772705\n",
      "iter 1008 (epoch 0), train_loss = 63.453, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 4.8851478099823\n",
      "iter 1009 (epoch 0), train_loss = 69.439, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 4.998811721801758\n",
      "iter 1010 (epoch 0), train_loss = 61.350, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.005058526992798\n",
      "iter 1011 (epoch 0), train_loss = 69.487, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.209941864013672\n",
      "iter 1012 (epoch 0), train_loss = 65.769, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.136206150054932\n",
      "iter 1013 (epoch 0), train_loss = 72.595, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 4.985417604446411\n",
      "iter 1014 (epoch 0), train_loss = 64.070, time/batch = 0.342\n",
      "EVALUATING\n",
      "Read data: 5.152761936187744\n",
      "iter 1015 (epoch 0), train_loss = 62.768, time/batch = 0.365\n",
      "EVALUATING\n",
      "Read data: 5.260269641876221\n",
      "iter 1016 (epoch 0), train_loss = 69.853, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.249960899353027\n",
      "iter 1017 (epoch 0), train_loss = 73.848, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.197949647903442\n",
      "iter 1018 (epoch 0), train_loss = 66.452, time/batch = 0.409\n",
      "EVALUATING\n",
      "Read data: 5.220809698104858\n",
      "iter 1019 (epoch 0), train_loss = 66.629, time/batch = 0.538\n",
      "EVALUATING\n",
      "Read data: 4.988103628158569\n",
      "iter 1020 (epoch 0), train_loss = 67.614, time/batch = 0.423\n",
      "EVALUATING\n",
      "Read data: 5.1781158447265625\n",
      "iter 1021 (epoch 0), train_loss = 66.490, time/batch = 0.395\n",
      "EVALUATING\n",
      "Read data: 5.056842565536499\n",
      "iter 1022 (epoch 0), train_loss = 69.397, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.105489730834961\n",
      "iter 1023 (epoch 0), train_loss = 64.394, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.01888632774353\n",
      "iter 1024 (epoch 0), train_loss = 56.916, time/batch = 0.359\n",
      "EVALUATING\n",
      "Read data: 4.890082597732544\n",
      "iter 1025 (epoch 0), train_loss = 76.799, time/batch = 0.626\n",
      "EVALUATING\n",
      "Read data: 4.9321699142456055\n",
      "iter 1026 (epoch 0), train_loss = 62.278, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.010469436645508\n",
      "iter 1027 (epoch 0), train_loss = 66.491, time/batch = 0.434\n",
      "EVALUATING\n",
      "Read data: 5.174837112426758\n",
      "iter 1028 (epoch 0), train_loss = 74.681, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.157830238342285\n",
      "iter 1029 (epoch 0), train_loss = 63.468, time/batch = 0.347\n",
      "EVALUATING\n",
      "Read data: 5.159015655517578\n",
      "iter 1030 (epoch 0), train_loss = 73.539, time/batch = 0.462\n",
      "EVALUATING\n",
      "Read data: 5.051137924194336\n",
      "iter 1031 (epoch 0), train_loss = 76.807, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.124271631240845\n",
      "iter 1032 (epoch 0), train_loss = 66.802, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.1253132820129395\n",
      "iter 1033 (epoch 0), train_loss = 67.400, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.178011417388916\n",
      "iter 1034 (epoch 0), train_loss = 74.840, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.179686069488525\n",
      "iter 1035 (epoch 0), train_loss = 65.773, time/batch = 0.472\n",
      "EVALUATING\n",
      "Read data: 5.014112234115601\n",
      "iter 1036 (epoch 0), train_loss = 71.899, time/batch = 0.448\n",
      "EVALUATING\n",
      "Read data: 5.054954290390015\n",
      "iter 1037 (epoch 0), train_loss = 64.905, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.051182270050049\n",
      "iter 1038 (epoch 0), train_loss = 65.518, time/batch = 0.536\n",
      "EVALUATING\n",
      "Read data: 5.128113508224487\n",
      "iter 1039 (epoch 0), train_loss = 69.553, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.202733278274536\n",
      "iter 1040 (epoch 0), train_loss = 72.575, time/batch = 0.501\n",
      "EVALUATING\n",
      "Read data: 5.289931058883667\n",
      "iter 1041 (epoch 0), train_loss = 73.630, time/batch = 0.458\n",
      "EVALUATING\n",
      "Read data: 5.074537754058838\n",
      "iter 1042 (epoch 0), train_loss = 63.406, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.183966875076294\n",
      "iter 1043 (epoch 0), train_loss = 65.349, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 4.954339265823364\n",
      "iter 1044 (epoch 0), train_loss = 65.420, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.169483423233032\n",
      "iter 1045 (epoch 0), train_loss = 66.505, time/batch = 0.534\n",
      "EVALUATING\n",
      "Read data: 5.0451500415802\n",
      "iter 1046 (epoch 0), train_loss = 62.753, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.159666538238525\n",
      "iter 1047 (epoch 0), train_loss = 67.670, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.152281045913696\n",
      "iter 1048 (epoch 0), train_loss = 69.472, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.042062044143677\n",
      "iter 1049 (epoch 0), train_loss = 62.591, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 5.324327230453491\n",
      "iter 1050 (epoch 0), train_loss = 70.272, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.001481294631958\n",
      "iter 1051 (epoch 0), train_loss = 60.169, time/batch = 0.311\n",
      "EVALUATING\n",
      "Read data: 5.17124080657959\n",
      "iter 1052 (epoch 0), train_loss = 67.328, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.068005084991455\n",
      "iter 1053 (epoch 0), train_loss = 63.699, time/batch = 0.370\n",
      "EVALUATING\n",
      "Read data: 5.187864780426025\n",
      "iter 1054 (epoch 0), train_loss = 69.674, time/batch = 0.540\n",
      "EVALUATING\n",
      "Read data: 5.214492559432983\n",
      "iter 1055 (epoch 0), train_loss = 68.903, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 5.129242897033691\n",
      "iter 1056 (epoch 0), train_loss = 56.098, time/batch = 0.282\n",
      "EVALUATING\n",
      "Read data: 5.190635919570923\n",
      "iter 1057 (epoch 0), train_loss = 73.974, time/batch = 0.586\n",
      "EVALUATING\n",
      "Read data: 5.07961368560791\n",
      "iter 1058 (epoch 0), train_loss = 73.323, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.269887924194336\n",
      "iter 1059 (epoch 0), train_loss = 79.780, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.201112270355225\n",
      "iter 1060 (epoch 0), train_loss = 69.112, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.045726776123047\n",
      "iter 1061 (epoch 0), train_loss = 70.184, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.092734098434448\n",
      "iter 1062 (epoch 0), train_loss = 69.880, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.195278167724609\n",
      "iter 1063 (epoch 0), train_loss = 68.748, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.133646726608276\n",
      "iter 1064 (epoch 0), train_loss = 68.962, time/batch = 0.385\n",
      "EVALUATING\n",
      "Read data: 5.117031097412109\n",
      "iter 1065 (epoch 0), train_loss = 73.879, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.218888998031616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1066 (epoch 0), train_loss = 72.359, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.0836687088012695\n",
      "iter 1067 (epoch 0), train_loss = 62.449, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.034078359603882\n",
      "iter 1068 (epoch 0), train_loss = 69.454, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.118392467498779\n",
      "iter 1069 (epoch 0), train_loss = 69.546, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.0567522048950195\n",
      "iter 1070 (epoch 0), train_loss = 70.347, time/batch = 0.447\n",
      "EVALUATING\n",
      "Read data: 5.054718017578125\n",
      "iter 1071 (epoch 0), train_loss = 71.797, time/batch = 0.535\n",
      "EVALUATING\n",
      "Read data: 5.162661552429199\n",
      "iter 1072 (epoch 0), train_loss = 66.876, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 5.068893194198608\n",
      "iter 1073 (epoch 0), train_loss = 71.647, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.043900728225708\n",
      "iter 1074 (epoch 0), train_loss = 68.870, time/batch = 0.530\n",
      "EVALUATING\n",
      "Read data: 5.121396064758301\n",
      "iter 1075 (epoch 0), train_loss = 69.748, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.073085308074951\n",
      "iter 1076 (epoch 0), train_loss = 60.940, time/batch = 0.492\n",
      "EVALUATING\n",
      "Read data: 5.138938665390015\n",
      "iter 1077 (epoch 0), train_loss = 67.313, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.089263200759888\n",
      "iter 1078 (epoch 0), train_loss = 80.432, time/batch = 0.642\n",
      "EVALUATING\n",
      "Read data: 5.1500489711761475\n",
      "iter 1079 (epoch 0), train_loss = 69.858, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 5.01850962638855\n",
      "iter 1080 (epoch 0), train_loss = 67.403, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.211506605148315\n",
      "iter 1081 (epoch 0), train_loss = 68.475, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 4.993922233581543\n",
      "iter 1082 (epoch 0), train_loss = 66.885, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 5.144036531448364\n",
      "iter 1083 (epoch 0), train_loss = 60.211, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.179844617843628\n",
      "iter 1084 (epoch 0), train_loss = 64.493, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.080141544342041\n",
      "iter 1085 (epoch 0), train_loss = 70.240, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.0671210289001465\n",
      "iter 1086 (epoch 0), train_loss = 73.194, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.118606090545654\n",
      "iter 1087 (epoch 0), train_loss = 69.644, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.242923259735107\n",
      "iter 1088 (epoch 0), train_loss = 63.157, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 5.026453256607056\n",
      "iter 1089 (epoch 0), train_loss = 67.171, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.367192983627319\n",
      "iter 1090 (epoch 0), train_loss = 77.629, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 5.1399219036102295\n",
      "iter 1091 (epoch 0), train_loss = 72.956, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 5.023721218109131\n",
      "iter 1092 (epoch 0), train_loss = 74.494, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.269758939743042\n",
      "iter 1093 (epoch 0), train_loss = 73.978, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 5.338496923446655\n",
      "iter 1094 (epoch 0), train_loss = 70.038, time/batch = 0.511\n",
      "EVALUATING\n",
      "Read data: 5.1756751537323\n",
      "iter 1095 (epoch 0), train_loss = 72.632, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.214269161224365\n",
      "iter 1096 (epoch 0), train_loss = 64.352, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.02885627746582\n",
      "iter 1097 (epoch 0), train_loss = 62.481, time/batch = 0.366\n",
      "EVALUATING\n",
      "Read data: 5.001972436904907\n",
      "iter 1098 (epoch 0), train_loss = 64.081, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 4.989289283752441\n",
      "iter 1099 (epoch 0), train_loss = 69.074, time/batch = 0.379\n",
      "EVALUATING\n",
      "Read data: 5.206209659576416\n",
      "iter 1100 (epoch 0), train_loss = 81.773, time/batch = 0.985\n",
      "EVALUATING\n",
      "Read data: 5.038882732391357\n",
      "iter 1101 (epoch 0), train_loss = 63.840, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.021733283996582\n",
      "iter 1102 (epoch 0), train_loss = 64.343, time/batch = 0.505\n",
      "EVALUATING\n",
      "Read data: 5.179640293121338\n",
      "iter 1103 (epoch 0), train_loss = 72.636, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.157402515411377\n",
      "iter 1104 (epoch 0), train_loss = 66.451, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.021226406097412\n",
      "iter 1105 (epoch 0), train_loss = 69.655, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.156679153442383\n",
      "iter 1106 (epoch 0), train_loss = 66.983, time/batch = 0.459\n",
      "EVALUATING\n",
      "Read data: 5.2645909786224365\n",
      "iter 1107 (epoch 0), train_loss = 65.252, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.271354675292969\n",
      "iter 1108 (epoch 0), train_loss = 67.589, time/batch = 0.477\n",
      "EVALUATING\n",
      "Read data: 5.100622892379761\n",
      "iter 1109 (epoch 0), train_loss = 58.224, time/batch = 0.329\n",
      "EVALUATING\n",
      "Read data: 5.114485740661621\n",
      "iter 1110 (epoch 0), train_loss = 63.239, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.220704555511475\n",
      "iter 1111 (epoch 0), train_loss = 69.670, time/batch = 0.461\n",
      "EVALUATING\n",
      "Read data: 5.085797071456909\n",
      "iter 1112 (epoch 0), train_loss = 64.260, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 4.92320990562439\n",
      "iter 1113 (epoch 0), train_loss = 67.386, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.097937107086182\n",
      "iter 1114 (epoch 0), train_loss = 70.548, time/batch = 0.419\n",
      "EVALUATING\n",
      "Read data: 5.005443811416626\n",
      "iter 1115 (epoch 0), train_loss = 59.168, time/batch = 0.336\n",
      "EVALUATING\n",
      "Read data: 5.159381866455078\n",
      "iter 1116 (epoch 0), train_loss = 73.735, time/batch = 0.520\n",
      "EVALUATING\n",
      "Read data: 5.267670631408691\n",
      "iter 1117 (epoch 0), train_loss = 77.290, time/batch = 0.427\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 8.393813371658325\n",
      "iter 1118 (epoch 0), train_loss = 77.964, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.135662078857422\n",
      "iter 1119 (epoch 0), train_loss = 73.854, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.206201791763306\n",
      "iter 1120 (epoch 0), train_loss = 67.838, time/batch = 0.450\n",
      "EVALUATING\n",
      "Read data: 5.162762641906738\n",
      "iter 1121 (epoch 0), train_loss = 70.858, time/batch = 0.419\n",
      "EVALUATING\n",
      "Read data: 5.143440246582031\n",
      "iter 1122 (epoch 0), train_loss = 71.504, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.220667839050293\n",
      "iter 1123 (epoch 0), train_loss = 70.364, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.191566467285156\n",
      "iter 1124 (epoch 0), train_loss = 74.435, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 4.9843456745147705\n",
      "iter 1125 (epoch 0), train_loss = 66.062, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.075366973876953\n",
      "iter 1126 (epoch 0), train_loss = 67.188, time/batch = 0.406\n",
      "EVALUATING\n",
      "Read data: 5.071740627288818\n",
      "iter 1127 (epoch 0), train_loss = 64.637, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.335675477981567\n",
      "iter 1128 (epoch 0), train_loss = 71.684, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.174936056137085\n",
      "iter 1129 (epoch 0), train_loss = 68.471, time/batch = 0.529\n",
      "EVALUATING\n",
      "Read data: 5.099184274673462\n",
      "iter 1130 (epoch 0), train_loss = 77.903, time/batch = 0.574\n",
      "EVALUATING\n",
      "Read data: 5.186726331710815\n",
      "iter 1131 (epoch 0), train_loss = 78.200, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.042919158935547\n",
      "iter 1132 (epoch 0), train_loss = 60.173, time/batch = 0.359\n",
      "EVALUATING\n",
      "Read data: 4.958553791046143\n",
      "iter 1133 (epoch 0), train_loss = 64.078, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.235257387161255\n",
      "iter 1134 (epoch 0), train_loss = 67.167, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.04430365562439\n",
      "iter 1135 (epoch 0), train_loss = 61.745, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.070429086685181\n",
      "iter 1136 (epoch 0), train_loss = 62.891, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.280076742172241\n",
      "iter 1137 (epoch 0), train_loss = 69.406, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.202286005020142\n",
      "iter 1138 (epoch 0), train_loss = 71.628, time/batch = 0.420\n",
      "EVALUATING\n",
      "Read data: 5.055492401123047\n",
      "iter 1139 (epoch 0), train_loss = 73.705, time/batch = 0.533\n",
      "EVALUATING\n",
      "Read data: 5.181968927383423\n",
      "iter 1140 (epoch 0), train_loss = 64.085, time/batch = 0.371\n",
      "EVALUATING\n",
      "Read data: 5.16122579574585\n",
      "iter 1141 (epoch 0), train_loss = 68.388, time/batch = 0.510\n",
      "EVALUATING\n",
      "Read data: 5.191603899002075\n",
      "iter 1142 (epoch 0), train_loss = 66.341, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.049881935119629\n",
      "iter 1143 (epoch 0), train_loss = 70.078, time/batch = 0.459\n",
      "EVALUATING\n",
      "Read data: 5.201069593429565\n",
      "iter 1144 (epoch 0), train_loss = 68.109, time/batch = 0.423\n",
      "EVALUATING\n",
      "Read data: 5.231896162033081\n",
      "iter 1145 (epoch 0), train_loss = 67.513, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.033155679702759\n",
      "iter 1146 (epoch 0), train_loss = 64.673, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 5.135665416717529\n",
      "iter 1147 (epoch 0), train_loss = 65.753, time/batch = 0.499\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.129973649978638\n",
      "iter 1148 (epoch 0), train_loss = 64.674, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.006691217422485\n",
      "iter 1149 (epoch 0), train_loss = 52.746, time/batch = 0.283\n",
      "EVALUATING\n",
      "Read data: 5.19535756111145\n",
      "iter 1150 (epoch 0), train_loss = 70.097, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.186309576034546\n",
      "iter 1151 (epoch 0), train_loss = 70.006, time/batch = 0.394\n",
      "EVALUATING\n",
      "Read data: 4.910539150238037\n",
      "iter 1152 (epoch 0), train_loss = 69.742, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.215275287628174\n",
      "iter 1153 (epoch 0), train_loss = 69.975, time/batch = 0.367\n",
      "EVALUATING\n",
      "Read data: 5.286861419677734\n",
      "iter 1154 (epoch 0), train_loss = 69.854, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.143535852432251\n",
      "iter 1155 (epoch 0), train_loss = 67.341, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.2042248249053955\n",
      "iter 1156 (epoch 0), train_loss = 69.297, time/batch = 0.605\n",
      "EVALUATING\n",
      "Read data: 5.103093862533569\n",
      "iter 1157 (epoch 0), train_loss = 61.407, time/batch = 0.389\n",
      "EVALUATING\n",
      "Read data: 5.067220449447632\n",
      "iter 1158 (epoch 0), train_loss = 74.907, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 4.829748153686523\n",
      "iter 1159 (epoch 0), train_loss = 65.071, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.198601722717285\n",
      "iter 1160 (epoch 0), train_loss = 56.977, time/batch = 0.337\n",
      "EVALUATING\n",
      "Read data: 4.947940111160278\n",
      "iter 1161 (epoch 0), train_loss = 64.407, time/batch = 0.424\n",
      "EVALUATING\n",
      "Read data: 5.120460271835327\n",
      "iter 1162 (epoch 0), train_loss = 68.112, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.242622137069702\n",
      "iter 1163 (epoch 0), train_loss = 70.096, time/batch = 0.408\n",
      "EVALUATING\n",
      "Read data: 5.128300189971924\n",
      "iter 1164 (epoch 0), train_loss = 71.644, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.085939645767212\n",
      "iter 1165 (epoch 0), train_loss = 58.865, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.087112665176392\n",
      "iter 1166 (epoch 0), train_loss = 64.919, time/batch = 0.477\n",
      "EVALUATING\n",
      "Read data: 5.240364074707031\n",
      "iter 1167 (epoch 0), train_loss = 63.643, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.115354776382446\n",
      "iter 1168 (epoch 0), train_loss = 65.172, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.1622912883758545\n",
      "iter 1169 (epoch 0), train_loss = 68.926, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.192293882369995\n",
      "iter 1170 (epoch 0), train_loss = 65.694, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.102003812789917\n",
      "iter 1171 (epoch 0), train_loss = 67.276, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 4.951582908630371\n",
      "iter 1172 (epoch 0), train_loss = 61.510, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.224306344985962\n",
      "iter 1173 (epoch 0), train_loss = 68.112, time/batch = 0.400\n",
      "EVALUATING\n",
      "Read data: 5.094907999038696\n",
      "iter 1174 (epoch 0), train_loss = 59.545, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.186781167984009\n",
      "iter 1175 (epoch 0), train_loss = 68.112, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.087904214859009\n",
      "iter 1176 (epoch 0), train_loss = 76.935, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.179782390594482\n",
      "iter 1177 (epoch 0), train_loss = 66.301, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.117536306381226\n",
      "iter 1178 (epoch 0), train_loss = 69.328, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 5.064579486846924\n",
      "iter 1179 (epoch 0), train_loss = 63.303, time/batch = 0.437\n",
      "EVALUATING\n",
      "Read data: 5.045821905136108\n",
      "iter 1180 (epoch 0), train_loss = 68.917, time/batch = 0.385\n",
      "EVALUATING\n",
      "Read data: 5.064677715301514\n",
      "iter 1181 (epoch 0), train_loss = 57.742, time/batch = 0.382\n",
      "EVALUATING\n",
      "Read data: 5.151434898376465\n",
      "iter 1182 (epoch 0), train_loss = 65.876, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.013180255889893\n",
      "iter 1183 (epoch 0), train_loss = 65.534, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 5.1040215492248535\n",
      "iter 1184 (epoch 0), train_loss = 60.221, time/batch = 0.400\n",
      "EVALUATING\n",
      "Read data: 5.139845371246338\n",
      "iter 1185 (epoch 0), train_loss = 70.592, time/batch = 0.491\n",
      "EVALUATING\n",
      "Read data: 5.106928586959839\n",
      "iter 1186 (epoch 0), train_loss = 73.896, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.125669002532959\n",
      "iter 1187 (epoch 0), train_loss = 65.955, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.098715305328369\n",
      "iter 1188 (epoch 0), train_loss = 68.093, time/batch = 0.450\n",
      "EVALUATING\n",
      "Read data: 5.198772430419922\n",
      "iter 1189 (epoch 0), train_loss = 78.064, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.192839860916138\n",
      "iter 1190 (epoch 0), train_loss = 59.878, time/batch = 0.364\n",
      "EVALUATING\n",
      "Read data: 5.3444249629974365\n",
      "iter 1191 (epoch 0), train_loss = 63.526, time/batch = 0.348\n",
      "EVALUATING\n",
      "Read data: 5.169882535934448\n",
      "iter 1192 (epoch 0), train_loss = 57.771, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.11442756652832\n",
      "iter 1193 (epoch 0), train_loss = 67.604, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.183316946029663\n",
      "iter 1194 (epoch 0), train_loss = 73.498, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.103714466094971\n",
      "iter 1195 (epoch 0), train_loss = 63.345, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 4.958544731140137\n",
      "iter 1196 (epoch 0), train_loss = 59.095, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.1063408851623535\n",
      "iter 1197 (epoch 0), train_loss = 68.304, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.237910509109497\n",
      "iter 1198 (epoch 0), train_loss = 61.855, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.11539888381958\n",
      "iter 1199 (epoch 0), train_loss = 62.800, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.195011854171753\n",
      "iter 1200 (epoch 0), train_loss = 70.180, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.016181707382202\n",
      "iter 1201 (epoch 0), train_loss = 69.979, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.1480607986450195\n",
      "iter 1202 (epoch 0), train_loss = 67.444, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 4.982977867126465\n",
      "iter 1203 (epoch 0), train_loss = 72.784, time/batch = 0.510\n",
      "EVALUATING\n",
      "Read data: 5.089517831802368\n",
      "iter 1204 (epoch 0), train_loss = 69.384, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 5.192375898361206\n",
      "iter 1205 (epoch 0), train_loss = 66.027, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.0817296504974365\n",
      "iter 1206 (epoch 0), train_loss = 71.571, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.078121662139893\n",
      "iter 1207 (epoch 0), train_loss = 70.306, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.177438020706177\n",
      "iter 1208 (epoch 0), train_loss = 62.385, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.227485418319702\n",
      "iter 1209 (epoch 0), train_loss = 65.148, time/batch = 0.445\n",
      "EVALUATING\n",
      "Read data: 5.132227897644043\n",
      "iter 1210 (epoch 0), train_loss = 67.482, time/batch = 0.446\n",
      "EVALUATING\n",
      "Read data: 4.942979097366333\n",
      "iter 1211 (epoch 0), train_loss = 68.149, time/batch = 0.395\n",
      "EVALUATING\n",
      "Read data: 5.160025119781494\n",
      "iter 1212 (epoch 0), train_loss = 70.669, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 4.960604667663574\n",
      "iter 1213 (epoch 0), train_loss = 63.496, time/batch = 0.511\n",
      "EVALUATING\n",
      "Read data: 5.255465030670166\n",
      "iter 1214 (epoch 0), train_loss = 65.432, time/batch = 0.434\n",
      "EVALUATING\n",
      "Read data: 5.2556235790252686\n",
      "iter 1215 (epoch 0), train_loss = 73.732, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 5.1986494064331055\n",
      "iter 1216 (epoch 0), train_loss = 67.598, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.107995271682739\n",
      "iter 1217 (epoch 0), train_loss = 66.249, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.157729864120483\n",
      "iter 1218 (epoch 0), train_loss = 54.870, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.092807769775391\n",
      "iter 1219 (epoch 0), train_loss = 68.529, time/batch = 0.577\n",
      "EVALUATING\n",
      "Read data: 5.329241514205933\n",
      "iter 1220 (epoch 0), train_loss = 71.355, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.140882253646851\n",
      "iter 1221 (epoch 0), train_loss = 70.199, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.222591400146484\n",
      "iter 1222 (epoch 0), train_loss = 67.486, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.082937240600586\n",
      "iter 1223 (epoch 0), train_loss = 72.825, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.287538051605225\n",
      "iter 1224 (epoch 0), train_loss = 75.062, time/batch = 0.555\n",
      "EVALUATING\n",
      "Read data: 4.995147943496704\n",
      "iter 1225 (epoch 0), train_loss = 67.951, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 4.944107532501221\n",
      "iter 1226 (epoch 0), train_loss = 62.033, time/batch = 0.352\n",
      "EVALUATING\n",
      "Read data: 5.170379877090454\n",
      "iter 1227 (epoch 0), train_loss = 76.147, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.169586896896362\n",
      "iter 1228 (epoch 0), train_loss = 60.151, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 5.129132986068726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1229 (epoch 0), train_loss = 66.069, time/batch = 0.523\n",
      "EVALUATING\n",
      "Read data: 5.246976375579834\n",
      "iter 1230 (epoch 0), train_loss = 66.926, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.212045907974243\n",
      "iter 1231 (epoch 0), train_loss = 69.490, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.108752012252808\n",
      "iter 1232 (epoch 0), train_loss = 66.099, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.2727837562561035\n",
      "iter 1233 (epoch 0), train_loss = 69.846, time/batch = 0.499\n",
      "EVALUATING\n",
      "Read data: 5.0680036544799805\n",
      "iter 1234 (epoch 0), train_loss = 69.749, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.210981607437134\n",
      "iter 1235 (epoch 0), train_loss = 68.942, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.200709104537964\n",
      "iter 1236 (epoch 0), train_loss = 65.961, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.236387252807617\n",
      "iter 1237 (epoch 0), train_loss = 72.701, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.168431997299194\n",
      "iter 1238 (epoch 0), train_loss = 65.966, time/batch = 0.366\n",
      "EVALUATING\n",
      "Read data: 5.136445760726929\n",
      "iter 1239 (epoch 0), train_loss = 63.837, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.223979711532593\n",
      "iter 1240 (epoch 0), train_loss = 65.141, time/batch = 0.364\n",
      "EVALUATING\n",
      "Read data: 5.256576776504517\n",
      "iter 1241 (epoch 0), train_loss = 62.871, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.160791873931885\n",
      "iter 1242 (epoch 0), train_loss = 68.069, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.395829677581787\n",
      "iter 1243 (epoch 0), train_loss = 70.065, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.143907785415649\n",
      "iter 1244 (epoch 0), train_loss = 55.892, time/batch = 0.339\n",
      "EVALUATING\n",
      "Read data: 5.155722379684448\n",
      "iter 1245 (epoch 0), train_loss = 63.657, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.321109056472778\n",
      "iter 1246 (epoch 0), train_loss = 72.036, time/batch = 0.433\n",
      "EVALUATING\n",
      "Read data: 5.183549880981445\n",
      "iter 1247 (epoch 1), train_loss = 72.253, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.341673851013184\n",
      "iter 1248 (epoch 1), train_loss = 71.400, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 5.212977886199951\n",
      "iter 1249 (epoch 1), train_loss = 64.636, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.062316417694092\n",
      "iter 1250 (epoch 1), train_loss = 70.022, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.073057651519775\n",
      "iter 1251 (epoch 1), train_loss = 62.437, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.074469804763794\n",
      "iter 1252 (epoch 1), train_loss = 67.604, time/batch = 0.486\n",
      "EVALUATING\n",
      "Read data: 4.8578901290893555\n",
      "iter 1253 (epoch 1), train_loss = 63.300, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.009053945541382\n",
      "iter 1254 (epoch 1), train_loss = 61.434, time/batch = 0.433\n",
      "EVALUATING\n",
      "Read data: 4.954589128494263\n",
      "iter 1255 (epoch 1), train_loss = 64.010, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.235828399658203\n",
      "iter 1256 (epoch 1), train_loss = 60.291, time/batch = 0.505\n",
      "EVALUATING\n",
      "Read data: 5.191330671310425\n",
      "iter 1257 (epoch 1), train_loss = 65.694, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.049077987670898\n",
      "iter 1258 (epoch 1), train_loss = 68.264, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.157378911972046\n",
      "iter 1259 (epoch 1), train_loss = 70.840, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.269354820251465\n",
      "iter 1260 (epoch 1), train_loss = 72.314, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.240488290786743\n",
      "iter 1261 (epoch 1), train_loss = 70.581, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.017831802368164\n",
      "iter 1262 (epoch 1), train_loss = 69.657, time/batch = 0.475\n",
      "EVALUATING\n",
      "Read data: 5.0571229457855225\n",
      "iter 1263 (epoch 1), train_loss = 72.223, time/batch = 0.510\n",
      "EVALUATING\n",
      "Read data: 5.171131610870361\n",
      "iter 1264 (epoch 1), train_loss = 74.059, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 4.946912050247192\n",
      "iter 1265 (epoch 1), train_loss = 58.397, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.060944557189941\n",
      "iter 1266 (epoch 1), train_loss = 74.992, time/batch = 0.445\n",
      "EVALUATING\n",
      "Read data: 4.9948930740356445\n",
      "iter 1267 (epoch 1), train_loss = 64.310, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.106128692626953\n",
      "iter 1268 (epoch 1), train_loss = 70.140, time/batch = 0.521\n",
      "EVALUATING\n",
      "Read data: 5.2976179122924805\n",
      "iter 1269 (epoch 1), train_loss = 74.850, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.138510704040527\n",
      "iter 1270 (epoch 1), train_loss = 67.514, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.153517961502075\n",
      "iter 1271 (epoch 1), train_loss = 70.039, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.209713935852051\n",
      "iter 1272 (epoch 1), train_loss = 63.296, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.164395332336426\n",
      "iter 1273 (epoch 1), train_loss = 64.431, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.145168781280518\n",
      "iter 1274 (epoch 1), train_loss = 72.885, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 4.984693288803101\n",
      "iter 1275 (epoch 1), train_loss = 66.494, time/batch = 0.523\n",
      "EVALUATING\n",
      "Read data: 5.173580646514893\n",
      "iter 1276 (epoch 1), train_loss = 67.515, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 4.9914751052856445\n",
      "iter 1277 (epoch 1), train_loss = 59.128, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.201350927352905\n",
      "iter 1278 (epoch 1), train_loss = 72.609, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.237897157669067\n",
      "iter 1279 (epoch 1), train_loss = 62.978, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.101238012313843\n",
      "iter 1280 (epoch 1), train_loss = 68.112, time/batch = 0.361\n",
      "EVALUATING\n",
      "Read data: 4.943822145462036\n",
      "iter 1281 (epoch 1), train_loss = 66.012, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.13585901260376\n",
      "iter 1282 (epoch 1), train_loss = 63.599, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.2049806118011475\n",
      "iter 1283 (epoch 1), train_loss = 65.842, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 4.94729471206665\n",
      "iter 1284 (epoch 1), train_loss = 59.279, time/batch = 0.389\n",
      "EVALUATING\n",
      "Read data: 5.212425947189331\n",
      "iter 1285 (epoch 1), train_loss = 56.253, time/batch = 0.296\n",
      "EVALUATING\n",
      "Read data: 4.909008502960205\n",
      "iter 1286 (epoch 1), train_loss = 65.944, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.082225322723389\n",
      "iter 1287 (epoch 1), train_loss = 68.261, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.066528797149658\n",
      "iter 1288 (epoch 1), train_loss = 63.914, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.091482162475586\n",
      "iter 1289 (epoch 1), train_loss = 66.391, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 4.9622437953948975\n",
      "iter 1290 (epoch 1), train_loss = 73.099, time/batch = 0.611\n",
      "EVALUATING\n",
      "Read data: 5.055428504943848\n",
      "iter 1291 (epoch 1), train_loss = 67.054, time/batch = 0.425\n",
      "EVALUATING\n",
      "Read data: 5.010223150253296\n",
      "iter 1292 (epoch 1), train_loss = 65.717, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.354703187942505\n",
      "iter 1293 (epoch 1), train_loss = 74.365, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.0897393226623535\n",
      "iter 1294 (epoch 1), train_loss = 64.872, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.14475417137146\n",
      "iter 1295 (epoch 1), train_loss = 71.926, time/batch = 0.461\n",
      "EVALUATING\n",
      "Read data: 5.142317771911621\n",
      "iter 1296 (epoch 1), train_loss = 69.832, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 4.9938883781433105\n",
      "iter 1297 (epoch 1), train_loss = 63.762, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 5.2109901905059814\n",
      "iter 1298 (epoch 1), train_loss = 68.026, time/batch = 0.345\n",
      "EVALUATING\n",
      "Read data: 5.098476409912109\n",
      "iter 1299 (epoch 1), train_loss = 64.487, time/batch = 0.524\n",
      "EVALUATING\n",
      "Read data: 5.078967571258545\n",
      "iter 1300 (epoch 1), train_loss = 61.437, time/batch = 0.559\n",
      "EVALUATING\n",
      "Read data: 5.109407901763916\n",
      "iter 1301 (epoch 1), train_loss = 72.151, time/batch = 0.422\n",
      "EVALUATING\n",
      "Read data: 5.1892454624176025\n",
      "iter 1302 (epoch 1), train_loss = 78.246, time/batch = 0.449\n",
      "EVALUATING\n",
      "Read data: 5.140094518661499\n",
      "iter 1303 (epoch 1), train_loss = 70.977, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.064319610595703\n",
      "iter 1304 (epoch 1), train_loss = 66.232, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.172939300537109\n",
      "iter 1305 (epoch 1), train_loss = 67.202, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.094850301742554\n",
      "iter 1306 (epoch 1), train_loss = 59.738, time/batch = 0.496\n",
      "EVALUATING\n",
      "Read data: 5.139537334442139\n",
      "iter 1307 (epoch 1), train_loss = 69.278, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.136746168136597\n",
      "iter 1308 (epoch 1), train_loss = 63.383, time/batch = 0.521\n",
      "EVALUATING\n",
      "Read data: 5.095778226852417\n",
      "iter 1309 (epoch 1), train_loss = 70.762, time/batch = 0.538\n",
      "EVALUATING\n",
      "Read data: 5.248714447021484\n",
      "iter 1310 (epoch 1), train_loss = 69.389, time/batch = 0.385\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.130650997161865\n",
      "iter 1311 (epoch 1), train_loss = 61.209, time/batch = 0.524\n",
      "EVALUATING\n",
      "Read data: 5.064707279205322\n",
      "iter 1312 (epoch 1), train_loss = 72.707, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.191277265548706\n",
      "iter 1313 (epoch 1), train_loss = 69.173, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.31105637550354\n",
      "iter 1314 (epoch 1), train_loss = 66.983, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 4.90643835067749\n",
      "iter 1315 (epoch 1), train_loss = 68.759, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.144270420074463\n",
      "iter 1316 (epoch 1), train_loss = 64.618, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.290613412857056\n",
      "iter 1317 (epoch 1), train_loss = 68.521, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.111786842346191\n",
      "iter 1318 (epoch 1), train_loss = 71.288, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.199856281280518\n",
      "iter 1319 (epoch 1), train_loss = 73.256, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.172048568725586\n",
      "iter 1320 (epoch 1), train_loss = 67.163, time/batch = 0.445\n",
      "EVALUATING\n",
      "Read data: 5.244665622711182\n",
      "iter 1321 (epoch 1), train_loss = 65.354, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.227533340454102\n",
      "iter 1322 (epoch 1), train_loss = 69.595, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.179844617843628\n",
      "iter 1323 (epoch 1), train_loss = 65.070, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.168164014816284\n",
      "iter 1324 (epoch 1), train_loss = 66.842, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.2285168170928955\n",
      "iter 1325 (epoch 1), train_loss = 69.370, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.143667697906494\n",
      "iter 1326 (epoch 1), train_loss = 69.111, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.072900295257568\n",
      "iter 1327 (epoch 1), train_loss = 65.580, time/batch = 0.480\n",
      "EVALUATING\n",
      "Read data: 5.248391389846802\n",
      "iter 1328 (epoch 1), train_loss = 63.735, time/batch = 0.436\n",
      "EVALUATING\n",
      "Read data: 5.066054821014404\n",
      "iter 1329 (epoch 1), train_loss = 73.889, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.165503740310669\n",
      "iter 1330 (epoch 1), train_loss = 71.857, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.149069786071777\n",
      "iter 1331 (epoch 1), train_loss = 70.967, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.198227882385254\n",
      "iter 1332 (epoch 1), train_loss = 62.954, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 4.996758222579956\n",
      "iter 1333 (epoch 1), train_loss = 63.416, time/batch = 0.449\n",
      "EVALUATING\n",
      "Read data: 5.009862184524536\n",
      "iter 1334 (epoch 1), train_loss = 63.481, time/batch = 0.364\n",
      "EVALUATING\n",
      "Read data: 5.144689559936523\n",
      "iter 1335 (epoch 1), train_loss = 63.931, time/batch = 0.371\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 7.1586480140686035\n",
      "iter 1336 (epoch 1), train_loss = 68.328, time/batch = 0.433\n",
      "EVALUATING\n",
      "Read data: 5.152967691421509\n",
      "iter 1337 (epoch 1), train_loss = 68.611, time/batch = 0.399\n",
      "EVALUATING\n",
      "Read data: 5.123493432998657\n",
      "iter 1338 (epoch 1), train_loss = 67.618, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.021350860595703\n",
      "iter 1339 (epoch 1), train_loss = 69.697, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.125274419784546\n",
      "iter 1340 (epoch 1), train_loss = 70.855, time/batch = 0.496\n",
      "EVALUATING\n",
      "Read data: 5.179346323013306\n",
      "iter 1341 (epoch 1), train_loss = 68.806, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.075732707977295\n",
      "iter 1342 (epoch 1), train_loss = 65.238, time/batch = 0.572\n",
      "EVALUATING\n",
      "Read data: 5.057622194290161\n",
      "iter 1343 (epoch 1), train_loss = 65.852, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.086742877960205\n",
      "iter 1344 (epoch 1), train_loss = 58.513, time/batch = 0.334\n",
      "EVALUATING\n",
      "Read data: 5.1921021938323975\n",
      "iter 1345 (epoch 1), train_loss = 67.089, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.16790246963501\n",
      "iter 1346 (epoch 1), train_loss = 67.107, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.122914791107178\n",
      "iter 1347 (epoch 1), train_loss = 70.178, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.091982126235962\n",
      "iter 1348 (epoch 1), train_loss = 60.408, time/batch = 0.312\n",
      "EVALUATING\n",
      "Read data: 5.14172625541687\n",
      "iter 1349 (epoch 1), train_loss = 63.743, time/batch = 0.367\n",
      "EVALUATING\n",
      "Read data: 5.199924945831299\n",
      "iter 1350 (epoch 1), train_loss = 68.043, time/batch = 0.420\n",
      "EVALUATING\n",
      "Read data: 5.14152717590332\n",
      "iter 1351 (epoch 1), train_loss = 59.052, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 5.100200414657593\n",
      "iter 1352 (epoch 1), train_loss = 64.423, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.153671026229858\n",
      "iter 1353 (epoch 1), train_loss = 69.592, time/batch = 0.374\n",
      "EVALUATING\n",
      "Read data: 5.292060136795044\n",
      "iter 1354 (epoch 1), train_loss = 75.124, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 5.083473443984985\n",
      "iter 1355 (epoch 1), train_loss = 70.269, time/batch = 0.533\n",
      "EVALUATING\n",
      "Read data: 4.99820613861084\n",
      "iter 1356 (epoch 1), train_loss = 61.478, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.181983470916748\n",
      "iter 1357 (epoch 1), train_loss = 63.131, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.136900901794434\n",
      "iter 1358 (epoch 1), train_loss = 75.193, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.10522985458374\n",
      "iter 1359 (epoch 1), train_loss = 69.575, time/batch = 0.405\n",
      "EVALUATING\n",
      "Read data: 5.227583885192871\n",
      "iter 1360 (epoch 1), train_loss = 72.338, time/batch = 0.489\n",
      "EVALUATING\n",
      "Read data: 5.108970403671265\n",
      "iter 1361 (epoch 1), train_loss = 73.702, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 4.986181735992432\n",
      "iter 1362 (epoch 1), train_loss = 67.027, time/batch = 0.462\n",
      "EVALUATING\n",
      "Read data: 5.1878821849823\n",
      "iter 1363 (epoch 1), train_loss = 65.254, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.311473369598389\n",
      "iter 1364 (epoch 1), train_loss = 68.882, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 4.938079118728638\n",
      "iter 1365 (epoch 1), train_loss = 67.933, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.016182899475098\n",
      "iter 1366 (epoch 1), train_loss = 67.906, time/batch = 0.341\n",
      "EVALUATING\n",
      "Read data: 5.006570100784302\n",
      "iter 1367 (epoch 1), train_loss = 68.194, time/batch = 0.346\n",
      "EVALUATING\n",
      "Read data: 5.207515001296997\n",
      "iter 1368 (epoch 1), train_loss = 67.000, time/batch = 0.360\n",
      "EVALUATING\n",
      "Read data: 5.189918518066406\n",
      "iter 1369 (epoch 1), train_loss = 68.643, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.282912969589233\n",
      "iter 1370 (epoch 1), train_loss = 70.008, time/batch = 0.797\n",
      "EVALUATING\n",
      "Read data: 5.156384706497192\n",
      "iter 1371 (epoch 1), train_loss = 67.727, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 4.986567258834839\n",
      "iter 1372 (epoch 1), train_loss = 63.364, time/batch = 0.368\n",
      "EVALUATING\n",
      "Read data: 5.0750792026519775\n",
      "iter 1373 (epoch 1), train_loss = 70.626, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.220200777053833\n",
      "iter 1374 (epoch 1), train_loss = 79.750, time/batch = 0.503\n",
      "EVALUATING\n",
      "Read data: 5.031355142593384\n",
      "iter 1375 (epoch 1), train_loss = 67.163, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.081789493560791\n",
      "iter 1376 (epoch 1), train_loss = 61.480, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.254856824874878\n",
      "iter 1377 (epoch 1), train_loss = 61.892, time/batch = 0.533\n",
      "EVALUATING\n",
      "Read data: 5.134565830230713\n",
      "iter 1378 (epoch 1), train_loss = 59.849, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.290897369384766\n",
      "iter 1379 (epoch 1), train_loss = 80.900, time/batch = 0.445\n",
      "EVALUATING\n",
      "Read data: 5.1401543617248535\n",
      "iter 1380 (epoch 1), train_loss = 62.169, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.236774444580078\n",
      "iter 1381 (epoch 1), train_loss = 70.930, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.365333795547485\n",
      "iter 1382 (epoch 1), train_loss = 64.526, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.290162563323975\n",
      "iter 1383 (epoch 1), train_loss = 74.218, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 5.342143535614014\n",
      "iter 1384 (epoch 1), train_loss = 67.815, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.122750759124756\n",
      "iter 1385 (epoch 1), train_loss = 62.383, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.098068475723267\n",
      "iter 1386 (epoch 1), train_loss = 57.769, time/batch = 0.367\n",
      "EVALUATING\n",
      "Read data: 5.227221727371216\n",
      "iter 1387 (epoch 1), train_loss = 67.137, time/batch = 0.363\n",
      "EVALUATING\n",
      "Read data: 4.95200777053833\n",
      "iter 1388 (epoch 1), train_loss = 64.381, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.030232667922974\n",
      "iter 1389 (epoch 1), train_loss = 66.320, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.105300188064575\n",
      "iter 1390 (epoch 1), train_loss = 64.132, time/batch = 0.359\n",
      "EVALUATING\n",
      "Read data: 5.1556174755096436\n",
      "iter 1391 (epoch 1), train_loss = 58.923, time/batch = 0.548\n",
      "EVALUATING\n",
      "Read data: 5.206728458404541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1392 (epoch 1), train_loss = 62.137, time/batch = 0.421\n",
      "EVALUATING\n",
      "Read data: 5.10453462600708\n",
      "iter 1393 (epoch 1), train_loss = 82.074, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.1506736278533936\n",
      "iter 1394 (epoch 1), train_loss = 61.676, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.0653533935546875\n",
      "iter 1395 (epoch 1), train_loss = 70.961, time/batch = 0.461\n",
      "EVALUATING\n",
      "Read data: 5.00235652923584\n",
      "iter 1396 (epoch 1), train_loss = 72.661, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 4.9617085456848145\n",
      "iter 1397 (epoch 1), train_loss = 61.417, time/batch = 0.328\n",
      "EVALUATING\n",
      "Read data: 5.1158127784729\n",
      "iter 1398 (epoch 1), train_loss = 66.771, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.2822253704071045\n",
      "iter 1399 (epoch 1), train_loss = 65.546, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.303804159164429\n",
      "iter 1400 (epoch 1), train_loss = 64.584, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.080500602722168\n",
      "iter 1401 (epoch 1), train_loss = 72.712, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.209630250930786\n",
      "iter 1402 (epoch 1), train_loss = 67.261, time/batch = 0.411\n",
      "EVALUATING\n",
      "Read data: 4.999624490737915\n",
      "iter 1403 (epoch 1), train_loss = 62.477, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.1769654750823975\n",
      "iter 1404 (epoch 1), train_loss = 72.949, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.127865314483643\n",
      "iter 1405 (epoch 1), train_loss = 64.916, time/batch = 0.430\n",
      "EVALUATING\n",
      "Read data: 5.09702205657959\n",
      "iter 1406 (epoch 1), train_loss = 60.191, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.233620643615723\n",
      "iter 1407 (epoch 1), train_loss = 61.016, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.005393743515015\n",
      "iter 1408 (epoch 1), train_loss = 66.722, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 5.085118532180786\n",
      "iter 1409 (epoch 1), train_loss = 59.712, time/batch = 0.398\n",
      "EVALUATING\n",
      "Read data: 5.159495830535889\n",
      "iter 1410 (epoch 1), train_loss = 66.323, time/batch = 0.596\n",
      "EVALUATING\n",
      "Read data: 5.1894896030426025\n",
      "iter 1411 (epoch 1), train_loss = 69.509, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.164269685745239\n",
      "iter 1412 (epoch 1), train_loss = 74.371, time/batch = 0.393\n",
      "EVALUATING\n",
      "Read data: 5.307000160217285\n",
      "iter 1413 (epoch 1), train_loss = 65.028, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.015077590942383\n",
      "iter 1414 (epoch 1), train_loss = 67.369, time/batch = 0.534\n",
      "EVALUATING\n",
      "Read data: 5.264426231384277\n",
      "iter 1415 (epoch 1), train_loss = 64.323, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.24057674407959\n",
      "iter 1416 (epoch 1), train_loss = 69.028, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.279070138931274\n",
      "iter 1417 (epoch 1), train_loss = 60.112, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.1082353591918945\n",
      "iter 1418 (epoch 1), train_loss = 63.025, time/batch = 0.384\n",
      "EVALUATING\n",
      "Read data: 5.0507025718688965\n",
      "iter 1419 (epoch 1), train_loss = 64.401, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 4.9018518924713135\n",
      "iter 1420 (epoch 1), train_loss = 66.292, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.09784460067749\n",
      "iter 1421 (epoch 1), train_loss = 62.705, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.022952556610107\n",
      "iter 1422 (epoch 1), train_loss = 62.693, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.165045261383057\n",
      "iter 1423 (epoch 1), train_loss = 68.121, time/batch = 0.479\n",
      "EVALUATING\n",
      "Read data: 5.089731454849243\n",
      "iter 1424 (epoch 1), train_loss = 70.966, time/batch = 0.530\n",
      "EVALUATING\n",
      "Read data: 5.246264934539795\n",
      "iter 1425 (epoch 1), train_loss = 61.865, time/batch = 0.461\n",
      "EVALUATING\n",
      "Read data: 5.211477279663086\n",
      "iter 1426 (epoch 1), train_loss = 63.034, time/batch = 0.434\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 9.856786727905273\n",
      "iter 1427 (epoch 1), train_loss = 94.813, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.06682562828064\n",
      "iter 1428 (epoch 1), train_loss = 70.605, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.183741569519043\n",
      "iter 1429 (epoch 1), train_loss = 64.271, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.218867778778076\n",
      "iter 1430 (epoch 1), train_loss = 71.006, time/batch = 0.581\n",
      "EVALUATING\n",
      "Read data: 5.156104564666748\n",
      "iter 1431 (epoch 1), train_loss = 73.374, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 5.330911874771118\n",
      "iter 1432 (epoch 1), train_loss = 73.223, time/batch = 0.456\n",
      "EVALUATING\n",
      "Read data: 5.078165531158447\n",
      "iter 1433 (epoch 1), train_loss = 63.329, time/batch = 0.348\n",
      "EVALUATING\n",
      "Read data: 5.059493780136108\n",
      "iter 1434 (epoch 1), train_loss = 69.625, time/batch = 0.518\n",
      "EVALUATING\n",
      "Read data: 5.114633798599243\n",
      "iter 1435 (epoch 1), train_loss = 63.585, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 5.019902944564819\n",
      "iter 1436 (epoch 1), train_loss = 75.015, time/batch = 0.445\n",
      "EVALUATING\n",
      "Read data: 5.023339509963989\n",
      "iter 1437 (epoch 1), train_loss = 66.471, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.221374988555908\n",
      "iter 1438 (epoch 1), train_loss = 67.128, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.219400644302368\n",
      "iter 1439 (epoch 1), train_loss = 66.240, time/batch = 0.457\n",
      "EVALUATING\n",
      "Read data: 5.226944208145142\n",
      "iter 1440 (epoch 1), train_loss = 75.227, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.063615083694458\n",
      "iter 1441 (epoch 1), train_loss = 66.360, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.113559722900391\n",
      "iter 1442 (epoch 1), train_loss = 63.680, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.24139404296875\n",
      "iter 1443 (epoch 1), train_loss = 64.905, time/batch = 0.433\n",
      "EVALUATING\n",
      "Read data: 5.073622941970825\n",
      "iter 1444 (epoch 1), train_loss = 64.154, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.1118927001953125\n",
      "iter 1445 (epoch 1), train_loss = 66.755, time/batch = 0.577\n",
      "EVALUATING\n",
      "Read data: 5.157306671142578\n",
      "iter 1446 (epoch 1), train_loss = 72.762, time/batch = 0.394\n",
      "EVALUATING\n",
      "Read data: 5.2502148151397705\n",
      "iter 1447 (epoch 1), train_loss = 82.889, time/batch = 0.513\n",
      "EVALUATING\n",
      "Read data: 5.07682728767395\n",
      "iter 1448 (epoch 1), train_loss = 61.088, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.18702244758606\n",
      "iter 1449 (epoch 1), train_loss = 66.167, time/batch = 0.465\n",
      "EVALUATING\n",
      "Read data: 5.255533933639526\n",
      "iter 1450 (epoch 1), train_loss = 61.661, time/batch = 0.507\n",
      "EVALUATING\n",
      "Read data: 5.293830633163452\n",
      "iter 1451 (epoch 1), train_loss = 76.967, time/batch = 0.438\n",
      "EVALUATING\n",
      "Read data: 5.176941633224487\n",
      "iter 1452 (epoch 1), train_loss = 71.771, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 5.187445163726807\n",
      "iter 1453 (epoch 1), train_loss = 71.896, time/batch = 0.495\n",
      "EVALUATING\n",
      "Read data: 5.238511085510254\n",
      "iter 1454 (epoch 1), train_loss = 66.280, time/batch = 0.391\n",
      "EVALUATING\n",
      "Read data: 5.023863077163696\n",
      "iter 1455 (epoch 1), train_loss = 56.367, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.165362119674683\n",
      "iter 1456 (epoch 1), train_loss = 66.257, time/batch = 0.366\n",
      "EVALUATING\n",
      "Read data: 5.078685522079468\n",
      "iter 1457 (epoch 1), train_loss = 61.430, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.163043260574341\n",
      "iter 1458 (epoch 1), train_loss = 66.106, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.1302337646484375\n",
      "iter 1459 (epoch 1), train_loss = 70.770, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.221123218536377\n",
      "iter 1460 (epoch 1), train_loss = 70.101, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.081460952758789\n",
      "iter 1461 (epoch 1), train_loss = 60.366, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.127161502838135\n",
      "iter 1462 (epoch 1), train_loss = 63.317, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.158327579498291\n",
      "iter 1463 (epoch 1), train_loss = 66.470, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.208607912063599\n",
      "iter 1464 (epoch 1), train_loss = 70.938, time/batch = 0.415\n",
      "EVALUATING\n",
      "Read data: 5.224494695663452\n",
      "iter 1465 (epoch 1), train_loss = 67.089, time/batch = 0.562\n",
      "EVALUATING\n",
      "Read data: 5.175210475921631\n",
      "iter 1466 (epoch 1), train_loss = 61.132, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 5.326450347900391\n",
      "iter 1467 (epoch 1), train_loss = 65.954, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 4.998941421508789\n",
      "iter 1468 (epoch 1), train_loss = 61.960, time/batch = 0.509\n",
      "EVALUATING\n",
      "Read data: 5.123957395553589\n",
      "iter 1469 (epoch 1), train_loss = 65.198, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.018202781677246\n",
      "iter 1470 (epoch 1), train_loss = 66.093, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.2457826137542725\n",
      "iter 1471 (epoch 1), train_loss = 71.443, time/batch = 0.461\n",
      "EVALUATING\n",
      "Read data: 5.184207916259766\n",
      "iter 1472 (epoch 1), train_loss = 66.648, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.107924461364746\n",
      "iter 1473 (epoch 1), train_loss = 75.920, time/batch = 0.403\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.300338983535767\n",
      "iter 1474 (epoch 1), train_loss = 68.621, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.204888582229614\n",
      "iter 1475 (epoch 1), train_loss = 72.934, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.259723424911499\n",
      "iter 1476 (epoch 1), train_loss = 66.316, time/batch = 0.476\n",
      "EVALUATING\n",
      "Read data: 5.1887431144714355\n",
      "iter 1477 (epoch 1), train_loss = 69.320, time/batch = 0.483\n",
      "EVALUATING\n",
      "Read data: 5.088981628417969\n",
      "iter 1478 (epoch 1), train_loss = 75.929, time/batch = 0.539\n",
      "EVALUATING\n",
      "Read data: 4.953866243362427\n",
      "iter 1479 (epoch 1), train_loss = 76.194, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.2458343505859375\n",
      "iter 1480 (epoch 1), train_loss = 68.532, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.214127779006958\n",
      "iter 1481 (epoch 1), train_loss = 63.085, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.296900272369385\n",
      "iter 1482 (epoch 1), train_loss = 65.859, time/batch = 0.497\n",
      "EVALUATING\n",
      "Read data: 5.0610268115997314\n",
      "iter 1483 (epoch 1), train_loss = 64.347, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.231780767440796\n",
      "iter 1484 (epoch 1), train_loss = 58.030, time/batch = 0.443\n",
      "EVALUATING\n",
      "Read data: 5.154793739318848\n",
      "iter 1485 (epoch 1), train_loss = 67.498, time/batch = 0.447\n",
      "EVALUATING\n",
      "Read data: 5.042007684707642\n",
      "iter 1486 (epoch 1), train_loss = 61.408, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.181650161743164\n",
      "iter 1487 (epoch 1), train_loss = 71.263, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.2610859870910645\n",
      "iter 1488 (epoch 1), train_loss = 62.997, time/batch = 0.392\n",
      "EVALUATING\n",
      "Read data: 4.966590404510498\n",
      "iter 1489 (epoch 1), train_loss = 62.734, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.208697319030762\n",
      "iter 1490 (epoch 1), train_loss = 73.348, time/batch = 0.455\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 6.070179462432861\n",
      "iter 1491 (epoch 1), train_loss = 66.283, time/batch = 0.440\n",
      "EVALUATING\n",
      "Read data: 5.058094024658203\n",
      "iter 1492 (epoch 1), train_loss = 63.060, time/batch = 0.454\n",
      "EVALUATING\n",
      "Read data: 5.2010602951049805\n",
      "iter 1493 (epoch 1), train_loss = 66.088, time/batch = 0.444\n",
      "EVALUATING\n",
      "Read data: 5.2315545082092285\n",
      "iter 1494 (epoch 1), train_loss = 66.767, time/batch = 0.452\n",
      "EVALUATING\n",
      "Read data: 5.105676889419556\n",
      "iter 1495 (epoch 1), train_loss = 69.476, time/batch = 0.522\n",
      "EVALUATING\n",
      "Read data: 5.101568222045898\n",
      "iter 1496 (epoch 1), train_loss = 67.810, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.084561347961426\n",
      "iter 1497 (epoch 1), train_loss = 60.815, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.054301500320435\n",
      "iter 1498 (epoch 1), train_loss = 56.959, time/batch = 0.533\n",
      "EVALUATING\n",
      "Read data: 5.014649391174316\n",
      "iter 1499 (epoch 1), train_loss = 59.913, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.228096008300781\n",
      "iter 1500 (epoch 1), train_loss = 63.175, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.18544340133667\n",
      "iter 1501 (epoch 1), train_loss = 63.480, time/batch = 0.460\n",
      "EVALUATING\n",
      "Read data: 5.135891914367676\n",
      "iter 1502 (epoch 1), train_loss = 66.539, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.07323145866394\n",
      "iter 1503 (epoch 1), train_loss = 66.907, time/batch = 0.427\n",
      "EVALUATING\n",
      "Read data: 5.113534450531006\n",
      "iter 1504 (epoch 1), train_loss = 65.923, time/batch = 0.433\n",
      "EVALUATING\n",
      "Read data: 5.02454400062561\n",
      "iter 1505 (epoch 1), train_loss = 63.794, time/batch = 0.508\n",
      "EVALUATING\n",
      "Read data: 5.087584495544434\n",
      "iter 1506 (epoch 1), train_loss = 69.245, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.305118083953857\n",
      "iter 1507 (epoch 1), train_loss = 67.396, time/batch = 0.403\n",
      "EVALUATING\n",
      "Read data: 5.157257318496704\n",
      "iter 1508 (epoch 1), train_loss = 59.645, time/batch = 0.336\n",
      "EVALUATING\n",
      "Read data: 5.207893133163452\n",
      "iter 1509 (epoch 1), train_loss = 60.051, time/batch = 0.351\n",
      "EVALUATING\n",
      "Read data: 5.0704193115234375\n",
      "iter 1510 (epoch 1), train_loss = 58.295, time/batch = 0.401\n",
      "EVALUATING\n",
      "Read data: 5.1330413818359375\n",
      "iter 1511 (epoch 1), train_loss = 68.988, time/batch = 0.514\n",
      "EVALUATING\n",
      "Read data: 5.330111503601074\n",
      "iter 1512 (epoch 1), train_loss = 79.550, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.125356674194336\n",
      "iter 1513 (epoch 1), train_loss = 59.294, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 5.094186305999756\n",
      "iter 1514 (epoch 1), train_loss = 68.452, time/batch = 0.451\n",
      "EVALUATING\n",
      "Read data: 5.099660873413086\n",
      "iter 1515 (epoch 1), train_loss = 67.350, time/batch = 0.388\n",
      "EVALUATING\n",
      "Read data: 5.18944525718689\n",
      "iter 1516 (epoch 1), train_loss = 65.141, time/batch = 0.498\n",
      "EVALUATING\n",
      "Read data: 5.1491124629974365\n",
      "iter 1517 (epoch 1), train_loss = 62.624, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.159082651138306\n",
      "iter 1518 (epoch 1), train_loss = 65.173, time/batch = 0.393\n",
      "EVALUATING\n",
      "Read data: 5.110529661178589\n",
      "iter 1519 (epoch 1), train_loss = 64.210, time/batch = 0.418\n",
      "EVALUATING\n",
      "Read data: 5.279276371002197\n",
      "iter 1520 (epoch 1), train_loss = 66.847, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.308409214019775\n",
      "iter 1521 (epoch 1), train_loss = 67.867, time/batch = 0.471\n",
      "EVALUATING\n",
      "Read data: 5.013689756393433\n",
      "iter 1522 (epoch 1), train_loss = 59.486, time/batch = 0.420\n",
      "EVALUATING\n",
      "Read data: 5.065795660018921\n",
      "iter 1523 (epoch 1), train_loss = 66.031, time/batch = 0.402\n",
      "EVALUATING\n",
      "Read data: 5.091202020645142\n",
      "iter 1524 (epoch 1), train_loss = 67.229, time/batch = 0.387\n",
      "EVALUATING\n",
      "Read data: 5.0863869190216064\n",
      "iter 1525 (epoch 1), train_loss = 57.386, time/batch = 0.376\n",
      "EVALUATING\n",
      "Read data: 4.944679260253906\n",
      "iter 1526 (epoch 1), train_loss = 54.864, time/batch = 0.494\n",
      "EVALUATING\n",
      "Read data: 5.094454288482666\n",
      "iter 1527 (epoch 1), train_loss = 66.572, time/batch = 0.442\n",
      "EVALUATING\n",
      "Read data: 5.161959648132324\n",
      "iter 1528 (epoch 1), train_loss = 57.886, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.146254777908325\n",
      "iter 1529 (epoch 1), train_loss = 64.159, time/batch = 0.419\n",
      "EVALUATING\n",
      "Read data: 5.243516445159912\n",
      "iter 1530 (epoch 1), train_loss = 60.516, time/batch = 0.390\n",
      "EVALUATING\n",
      "Read data: 5.11153244972229\n",
      "iter 1531 (epoch 1), train_loss = 72.529, time/batch = 0.545\n",
      "EVALUATING\n",
      "Read data: 5.118440866470337\n",
      "iter 1532 (epoch 1), train_loss = 60.439, time/batch = 0.417\n",
      "EVALUATING\n",
      "Read data: 5.1890881061553955\n",
      "iter 1533 (epoch 1), train_loss = 64.525, time/batch = 0.385\n",
      "EVALUATING\n",
      "Read data: 5.071137189865112\n",
      "iter 1534 (epoch 1), train_loss = 67.209, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.118963003158569\n",
      "iter 1535 (epoch 1), train_loss = 60.159, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.132892370223999\n",
      "iter 1536 (epoch 1), train_loss = 63.038, time/batch = 0.548\n",
      "EVALUATING\n",
      "Read data: 5.09274435043335\n",
      "iter 1537 (epoch 1), train_loss = 59.776, time/batch = 0.478\n",
      "EVALUATING\n",
      "Read data: 5.061606407165527\n",
      "iter 1538 (epoch 1), train_loss = 65.269, time/batch = 0.466\n",
      "EVALUATING\n",
      "Read data: 5.058505296707153\n",
      "iter 1539 (epoch 1), train_loss = 68.467, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 4.954156398773193\n",
      "iter 1540 (epoch 1), train_loss = 60.189, time/batch = 0.371\n",
      "EVALUATING\n",
      "Read data: 5.143435001373291\n",
      "iter 1541 (epoch 1), train_loss = 58.278, time/batch = 0.470\n",
      "EVALUATING\n",
      "Read data: 5.337141036987305\n",
      "iter 1542 (epoch 1), train_loss = 68.273, time/batch = 0.426\n",
      "EVALUATING\n",
      "Read data: 5.184465646743774\n",
      "iter 1543 (epoch 1), train_loss = 56.076, time/batch = 0.396\n",
      "EVALUATING\n",
      "Read data: 5.195300102233887\n",
      "iter 1544 (epoch 1), train_loss = 67.930, time/batch = 0.576\n",
      "EVALUATING\n",
      "Read data: 5.136528253555298\n",
      "iter 1545 (epoch 1), train_loss = 71.998, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.260805606842041\n",
      "iter 1546 (epoch 1), train_loss = 72.536, time/batch = 0.378\n",
      "EVALUATING\n",
      "Read data: 5.255755424499512\n",
      "iter 1547 (epoch 1), train_loss = 73.293, time/batch = 0.412\n",
      "EVALUATING\n",
      "Read data: 5.109841585159302\n",
      "iter 1548 (epoch 1), train_loss = 77.223, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 4.985064268112183\n",
      "iter 1549 (epoch 1), train_loss = 61.572, time/batch = 0.366\n",
      "EVALUATING\n",
      "Read data: 5.141872406005859\n",
      "iter 1550 (epoch 1), train_loss = 68.249, time/batch = 0.473\n",
      "EVALUATING\n",
      "Read data: 5.117457151412964\n",
      "iter 1551 (epoch 1), train_loss = 62.300, time/batch = 0.429\n",
      "EVALUATING\n",
      "Read data: 4.946920871734619\n",
      "iter 1552 (epoch 1), train_loss = 60.672, time/batch = 0.485\n",
      "EVALUATING\n",
      "Read data: 5.182631254196167\n",
      "iter 1553 (epoch 1), train_loss = 72.487, time/batch = 0.432\n",
      "EVALUATING\n",
      "Read data: 5.0843329429626465\n",
      "iter 1554 (epoch 1), train_loss = 67.397, time/batch = 0.375\n",
      "EVALUATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data: 5.28473973274231\n",
      "iter 1555 (epoch 1), train_loss = 70.601, time/batch = 0.477\n",
      "EVALUATING\n",
      "Read data: 5.348208904266357\n",
      "iter 1556 (epoch 1), train_loss = 67.590, time/batch = 0.511\n",
      "EVALUATING\n",
      "Read data: 5.047255277633667\n",
      "iter 1557 (epoch 1), train_loss = 59.634, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.245269775390625\n",
      "iter 1558 (epoch 1), train_loss = 71.939, time/batch = 0.469\n",
      "EVALUATING\n",
      "Read data: 5.180579900741577\n",
      "iter 1559 (epoch 1), train_loss = 67.883, time/batch = 0.467\n",
      "EVALUATING\n",
      "Read data: 5.145096302032471\n",
      "iter 1560 (epoch 1), train_loss = 67.064, time/batch = 0.468\n",
      "EVALUATING\n",
      "Read data: 5.193490982055664\n",
      "iter 1561 (epoch 1), train_loss = 64.112, time/batch = 0.413\n",
      "EVALUATING\n",
      "Read data: 5.0354249477386475\n",
      "iter 1562 (epoch 1), train_loss = 71.372, time/batch = 0.431\n",
      "EVALUATING\n",
      "Read data: 5.160248041152954\n",
      "iter 1563 (epoch 1), train_loss = 53.749, time/batch = 0.376\n",
      "EVALUATING\n",
      "Read data: 5.1528000831604\n",
      "iter 1564 (epoch 1), train_loss = 68.651, time/batch = 0.416\n",
      "EVALUATING\n",
      "Read data: 5.305535316467285\n",
      "iter 1565 (epoch 1), train_loss = 70.841, time/batch = 0.363\n",
      "EVALUATING\n",
      "Read data: 5.286073207855225\n",
      "iter 1566 (epoch 1), train_loss = 61.233, time/batch = 0.330\n",
      "EVALUATING\n",
      "Read data: 5.036591291427612\n",
      "iter 1567 (epoch 1), train_loss = 69.962, time/batch = 0.441\n",
      "EVALUATING\n",
      "Read data: 5.015422105789185\n",
      "iter 1568 (epoch 1), train_loss = 71.858, time/batch = 0.493\n",
      "EVALUATING\n",
      "Read data: 5.002029895782471\n",
      "iter 1569 (epoch 1), train_loss = 69.511, time/batch = 0.369\n",
      "EVALUATING\n",
      "Read data: 5.20801043510437\n",
      "iter 1570 (epoch 1), train_loss = 70.439, time/batch = 0.439\n",
      "EVALUATING\n",
      "Read data: 5.188790321350098\n",
      "iter 1571 (epoch 1), train_loss = 67.251, time/batch = 0.482\n",
      "EVALUATING\n",
      "Read data: 4.988145351409912\n",
      "iter 1572 (epoch 1), train_loss = 60.210, time/batch = 0.414\n",
      "EVALUATING\n",
      "Read data: 4.987169027328491\n",
      "iter 1573 (epoch 1), train_loss = 65.605, time/batch = 0.499\n",
      "EVALUATING\n",
      "Read data: 5.248791933059692\n",
      "iter 1574 (epoch 1), train_loss = 68.633, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.3084940910339355\n",
      "iter 1575 (epoch 1), train_loss = 66.475, time/batch = 0.381\n",
      "EVALUATING\n",
      "Read data: 5.075709581375122\n",
      "iter 1576 (epoch 1), train_loss = 58.017, time/batch = 0.404\n",
      "EVALUATING\n",
      "Read data: 5.223737001419067\n",
      "iter 1577 (epoch 1), train_loss = 73.471, time/batch = 0.532\n",
      "EVALUATING\n",
      "Read data: 5.000084161758423\n",
      "iter 1578 (epoch 1), train_loss = 63.758, time/batch = 0.516\n",
      "EVALUATING\n",
      "Read data: 5.147688865661621\n",
      "iter 1579 (epoch 1), train_loss = 67.885, time/batch = 0.481\n",
      "EVALUATING\n",
      "Read data: 5.100968360900879\n",
      "iter 1580 (epoch 1), train_loss = 65.230, time/batch = 0.455\n",
      "EVALUATING\n",
      "rescuing\n",
      "Read data: 5.890394449234009\n",
      "iter 1581 (epoch 1), train_loss = 68.336, time/batch = 0.448\n",
      "EVALUATING\n",
      "Read data: 5.044945240020752\n",
      "iter 1582 (epoch 1), train_loss = 62.384, time/batch = 0.375\n",
      "EVALUATING\n",
      "Read data: 5.240339517593384\n",
      "iter 1583 (epoch 1), train_loss = 64.723, time/batch = 0.455\n",
      "EVALUATING\n",
      "Read data: 5.094698905944824\n",
      "iter 1584 (epoch 1), train_loss = 63.594, time/batch = 0.428\n",
      "EVALUATING\n",
      "Read data: 5.255139589309692\n",
      "iter 1585 (epoch 1), train_loss = 74.725, time/batch = 0.453\n",
      "EVALUATING\n",
      "Read data: 4.910956621170044\n",
      "iter 1586 (epoch 1), train_loss = 58.707, time/batch = 0.484\n",
      "EVALUATING\n",
      "Read data: 5.127017974853516\n",
      "iter 1587 (epoch 1), train_loss = 64.544, time/batch = 0.431\n",
      "EVALUATING\n",
      "rescuing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'JpegImageFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/workspace/ImageCaptioning.pytorch/dataloaderraw.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, split, batch_size)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mfc_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_fc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0matt_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_att\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-d586a547f38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-cbb50b17d7ba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Load data from train split (0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Read data:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/ImageCaptioning.pytorch/dataloaderraw.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, split, batch_size)\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'JpegImageFile'"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/ubuntu/workspace/ImageCaptioning.pytorch/data/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "\n",
    "import opts\n",
    "import models\n",
    "from dataloader import *\n",
    "import eval_utils\n",
    "import misc.utils as utils\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    print(\"Tensorflow not installed; No tensorboard logging.\")\n",
    "    tf = None\n",
    "\n",
    "def add_summary_value(writer, key, value, iteration):\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag=key, simple_value=value)])\n",
    "    writer.add_summary(summary, iteration)\n",
    "\n",
    "def test(opt):\n",
    "    opt.use_att = utils.if_use_att(opt.caption_model)\n",
    "    loader = DataLoaderRaw(opt)\n",
    "#     opt.vocab_size = loader.vocab_size\n",
    "#     opt.seq_length = loader.seq_length\n",
    "\n",
    "    tf_summary_writer = tf and tf.summary.FileWriter(opt.checkpoint_path)\n",
    "\n",
    "    infos = {}\n",
    "    histories = {}\n",
    "    if opt.start_from is not None:\n",
    "        # open old infos and check if models are compatible\n",
    "        with open(os.path.join(opt.start_from, 'infos_'+opt.id+'.pkl')) as f:\n",
    "            infos = cPickle.load(f)\n",
    "            saved_model_opt = infos['opt']\n",
    "            need_be_same=[\"caption_model\", \"rnn_type\", \"rnn_size\", \"num_layers\"]\n",
    "            for checkme in need_be_same:\n",
    "                assert vars(saved_model_opt)[checkme] == vars(opt)[checkme], \"Command line argument and saved model disagree on '%s' \" % checkme\n",
    "\n",
    "        if os.path.isfile(os.path.join(opt.start_from, 'histories_'+opt.id+'.pkl')):\n",
    "            with open(os.path.join(opt.start_from, 'histories_'+opt.id+'.pkl')) as f:\n",
    "                histories = cPickle.load(f)\n",
    "\n",
    "    iteration = infos.get('iter', 0)\n",
    "    epoch = infos.get('epoch', 0)\n",
    "\n",
    "    val_result_history = histories.get('val_result_history', {})\n",
    "    loss_history = histories.get('loss_history', {})\n",
    "    lr_history = histories.get('lr_history', {})\n",
    "    ss_prob_history = histories.get('ss_prob_history', {})\n",
    "\n",
    "#     loader.iterators = infos.get('iterators', loader.iterators)\n",
    "#     loader.split_ix = infos.get('split_ix', loader.split_ix)\n",
    "    if opt.load_best_score == 1:\n",
    "        best_val_score = infos.get('best_val_score', None)\n",
    "\n",
    "#     model = models.setup(opt)\n",
    "#     model.cuda()\n",
    "\n",
    "    update_lr_flag = True\n",
    "    # Assure in training mode\n",
    "#     model.train()\n",
    "\n",
    "    crit = utils.LanguageModelCriterion()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=opt.learning_rate, weight_decay=opt.weight_decay)\n",
    "\n",
    "    # Load the optimizer\n",
    "    if vars(opt).get('start_from', None) is not None:\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(opt.start_from, 'optimizer.pth')))\n",
    "\n",
    "    while True:\n",
    "        if update_lr_flag:\n",
    "                # Assign the learning rate\n",
    "            if epoch > opt.learning_rate_decay_start and opt.learning_rate_decay_start >= 0:\n",
    "                frac = (epoch - opt.learning_rate_decay_start) // opt.learning_rate_decay_every\n",
    "                decay_factor = opt.learning_rate_decay_rate  ** frac\n",
    "                opt.current_lr = opt.learning_rate * decay_factor\n",
    "                utils.set_lr(optimizer, opt.current_lr) # set the decayed rate\n",
    "            else:\n",
    "                opt.current_lr = opt.learning_rate\n",
    "            # Assign the scheduled sampling prob\n",
    "            if epoch > opt.scheduled_sampling_start and opt.scheduled_sampling_start >= 0:\n",
    "                frac = (epoch - opt.scheduled_sampling_start) // opt.scheduled_sampling_increase_every\n",
    "                opt.ss_prob = min(opt.scheduled_sampling_increase_prob  * frac, opt.scheduled_sampling_max_prob)\n",
    "                model.ss_prob = opt.ss_prob\n",
    "            update_lr_flag = False\n",
    "                \n",
    "        start = time.time()\n",
    "        # Load data from train split (0)\n",
    "        data = loader.get_batch('test')\n",
    "        print('Read data:', time.time() - start)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        tmp = [data['fc_feats'], data['att_feats'], data['labels']]\n",
    "        tmp = [Variable(torch.from_numpy(_), requires_grad=False).cuda() for _ in tmp]\n",
    "        fc_feats, att_feats, labels = tmp\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = crit(model(fc_feats, att_feats, labels), labels[:,1:])\n",
    "        loss.backward()\n",
    "        utils.clip_gradient(optimizer, opt.grad_clip)\n",
    "        optimizer.step()\n",
    "        train_loss = loss.data[0] / batch_size\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        print(\"iter {} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "            .format(iteration, epoch, train_loss, end - start))\n",
    "\n",
    "        # Update the iteration and epoch\n",
    "        iteration += 1\n",
    "        if data['bounds']['wrapped']:\n",
    "            epoch += 1\n",
    "            update_lr_flag = True\n",
    "\n",
    "        # Write the training loss summary\n",
    "        if (iteration % opt.losses_log_every == 0):\n",
    "            if tf is not None:\n",
    "                add_summary_value(tf_summary_writer, 'train_loss', train_loss, iteration)\n",
    "                add_summary_value(tf_summary_writer, 'learning_rate', opt.current_lr, iteration)\n",
    "                add_summary_value(tf_summary_writer, 'scheduled_sampling_prob', model.ss_prob, iteration)\n",
    "                tf_summary_writer.flush()\n",
    "\n",
    "            loss_history[iteration] = train_loss\n",
    "            lr_history[iteration] = opt.current_lr\n",
    "            ss_prob_history[iteration] = model.ss_prob\n",
    "\n",
    "        # make evaluation on validation set, and save model\n",
    "        if (iteration % opt.save_checkpoint_every == 0):\n",
    "            # eval model\n",
    "            eval_kwargs = {'split': 'val',\n",
    "                            'dataset': opt.input_json}\n",
    "            eval_kwargs.update(vars(opt))\n",
    "            val_loss, predictions, lang_stats = eval_utils.eval_split(model, crit, loader, eval_kwargs)\n",
    "\n",
    "            # Write validation result into summary\n",
    "            if tf is not None:\n",
    "                add_summary_value(tf_summary_writer, 'validation loss', val_loss, iteration)\n",
    "                for k,v in lang_stats.items():\n",
    "                    add_summary_value(tf_summary_writer, k, v, iteration)\n",
    "                tf_summary_writer.flush()\n",
    "            val_result_history[iteration] = {'loss': val_loss, 'lang_stats': lang_stats, 'predictions': predictions}\n",
    "\n",
    "            # Save model if is improving on validation result\n",
    "            if opt.language_eval == 1:\n",
    "                current_score = lang_stats['CIDEr']\n",
    "            else:\n",
    "                current_score = - val_loss\n",
    "\n",
    "            best_flag = False\n",
    "            if True: # if true\n",
    "                if best_val_score is None or current_score > best_val_score:\n",
    "                    best_val_score = current_score\n",
    "                    best_flag = True\n",
    "                checkpoint_path = os.path.join(opt.checkpoint_path, 'model.pth')\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(\"model saved to {}\".format(checkpoint_path))\n",
    "                optimizer_path = os.path.join(opt.checkpoint_path, 'optimizer.pth')\n",
    "                torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "                # Dump miscalleous informations\n",
    "                infos['iter'] = iteration\n",
    "                infos['epoch'] = epoch\n",
    "                infos['iterators'] = loader.iterators\n",
    "                infos['split_ix'] = loader.split_ix\n",
    "                infos['best_val_score'] = best_val_score\n",
    "                infos['opt'] = opt\n",
    "                infos['vocab'] = loader.get_vocab()\n",
    "\n",
    "                histories['val_result_history'] = val_result_history\n",
    "                histories['loss_history'] = loss_history\n",
    "                histories['lr_history'] = lr_history\n",
    "                histories['ss_prob_history'] = ss_prob_history\n",
    "                with open(os.path.join(opt.checkpoint_path, 'infos_'+opt.id+'.pkl'), 'wb') as f:\n",
    "                    cPickle.dump(infos, f)\n",
    "                with open(os.path.join(opt.checkpoint_path, 'histories_'+opt.id+'.pkl'), 'wb') as f:\n",
    "                    cPickle.dump(histories, f)\n",
    "\n",
    "                if best_flag:\n",
    "                    checkpoint_path = os.path.join(opt.checkpoint_path, 'model-best.pth')\n",
    "                    torch.save(model.state_dict(), checkpoint_path)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "                    with open(os.path.join(opt.checkpoint_path, 'infos_'+opt.id+'-best.pkl'), 'wb') as f:\n",
    "                        cPickle.dump(infos, f)\n",
    "\n",
    "        # Stop if reaching max epochs\n",
    "        if epoch >= opt.max_epochs and opt.max_epochs != -1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.get_batch('val')\n",
    "optimizer = optim.Adam(model.parameters(), lr=opt.learning_rate, weight_decay=opt.weight_decay)\n",
    "\n",
    "# Load the optimizer\n",
    "if vars(opt).get('start_from', None) is not None:\n",
    "    optimizer.load_state_dict(torch.load(os.path.join(opt.start_from, 'optimizer.pth')))\n",
    "tmp = [data['fc_feats'], data['att_feats'], data['labels']]\n",
    "tmp = [Variable(torch.from_numpy(_), requires_grad=False).cuda() for _ in tmp]\n",
    "fc_feats, att_feats, labels = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCModel(\n",
       "  (img_embed): Linear(in_features=2048, out_features=512)\n",
       "  (core): LSTMCore(\n",
       "    (i2h): Linear(in_features=512, out_features=2560)\n",
       "    (h2h): Linear(in_features=512, out_features=2560)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (embed): Embedding(13016, 512)\n",
       "  (logit): Linear(in_features=512, out_features=13016)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "# torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crit = utils.LanguageModelCriterion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaderRaw loading images from folder:  /home/ubuntu/workspace/ImageCaptioning.pytorch/images/\n",
      "0\n",
      "listing all images in directory /home/ubuntu/workspace/ImageCaptioning.pytorch/images/\n",
      "DataLoaderRaw found  80088  images\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoaderRaw(opt)\n",
    "data = loader.get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/ImageCaptioning.pytorch/models/FCModel.py:111: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.log_softmax(self.logit(output))\n"
     ]
    }
   ],
   "source": [
    "tmp = [data['fc_feats'], data['att_feats'], data['labels']]\n",
    "tmp = [Variable(torch.from_numpy(_), requires_grad=False).cuda() for _ in tmp]\n",
    "fc_feats, att_feats, labels = tmp\n",
    "\n",
    "optimizer.zero_grad()\n",
    "results = model(fc_feats, att_feats, labels)\n",
    "loss = crit(results, labels[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/ImageCaptioning.pytorch/models/FCModel.py:198: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logprobs = F.log_softmax(self.logit(output))\n"
     ]
    }
   ],
   "source": [
    "seq, _ = model.sample(fc_feats,att_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  1.7569   0.2160   0.2002  ...    0.6460   0.0493   0.0862\n",
       "  0.5674   0.0000   0.0000  ...    0.0226   0.0621   0.0165\n",
       "  0.2495   0.0069   0.1452  ...    0.0000   0.1048   0.0418\n",
       "           ...               ⋱              ...            \n",
       "  1.2170   0.0457   0.0000  ...    0.0531   0.0029   0.0024\n",
       "  0.5394   0.0687   0.0408  ...    0.0447   0.1923   0.0514\n",
       "  1.7932   0.0255   0.0869  ...    0.4511   0.0259   0.0122\n",
       "[torch.cuda.FloatTensor of size 64x2048 (GPU 0)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for label in data['labels']:\n",
    "    words = \"\"\n",
    "    for i in label:\n",
    "        words += idx_to_word.get(i, \"UNKNOWN\") + \" \"\n",
    "    labels.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_prediction = []\n",
    "for label in seq:\n",
    "    words = set()\n",
    "    for i in label:\n",
    "        words.add(idx_to_word[i])\n",
    "    labels_prediction.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['B003F6EI2C', 'B01IQ60ZFE', 'B075V3WC2F', 'B06XFPQ3ZN', 'B0071URUW8', 'B0759MW7L3', 'B01M15PLEH', 'B01N43BBIO', 'B01H6Y1DM6', 'B000F3FVTC', 'B00PJIQ5A2', 'B007SNADYA', 'B00MR9T9ZY', 'B074W5LM1K', 'B06Y6HKCWW', 'B014W1C262', 'B00MC7TC6M', 'B075MBGLS1', 'B072JTX7KK', 'B00AGNMRIO', 'B00Q7DETQU', 'B00S6ZDFJE', 'B0778WNJTT', 'B01AV86K72', 'B01N5F1OM3', 'B00SX5ULXK', 'B01ES9VFF2', 'B0081SKI28', 'B0761SG5DN', 'B017PEGG0G', 'B01HR215FM', 'B074PV1GJM', 'B06XTQJ1BV', 'B01M0SASZO', 'B01MY0AZHI', 'B01J4EIRTI', 'B075R13JLG', 'B01M098VFG', 'B00ZBJVH9I', 'B01ISL7P6E', 'B075MWDHSF', 'B06XSKSPLW', 'B011TR7FSC', 'B076J9HQBP', 'B075HTQZ26', 'B071GP64JM', 'B0761SCQZ7', 'B00X6L4J70', 'B073HPCH9D', 'B00NARZQKY', 'B01N5BNROP', 'B075BNFRGP', 'B01N29Q7LA', 'B00PVLHHRC', 'B01N2W2BMW', 'B07476XJC7', 'B00BJOGIDE', 'B076VGYX9H', 'B071H476Z4', 'B071LPWM11', 'B074TKPNQZ', 'B01N31DPE5', 'B001TJWZPY', 'B072P6CTV4'])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['asins_to_label'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pattern, Tinted Smile Cynthia Liquid 2.0 18.5 EXCEED Market Saddle',\n",
       " 'McAlister Tinted Smile are (BLACK) Cynthia 2.0 18.5 Mount Saddle HNNATTA',\n",
       " 'Pattern, Tinted Knot, Sparkling Cynthia 2.0 Spatula Height 18.5 BGSD',\n",
       " 'McAlister Tinted Smile (BLACK) Cynthia 2.0 Underpants 18.5 15-Ounce Saddle',\n",
       " 'Underpants Port, Sparkling Smile',\n",
       " 'McAlister Smile Sparkling Underpants Port, 15-Ounce',\n",
       " 'Shears Multi-Color Smile Tinted 8-Inch grinderPUNCH 2.0 Cynthia Height 18.5 EXCEED Market Saddle',\n",
       " 'Anne Tinted Cynthia 2.0 Underpants 18.5 not Saddle',\n",
       " 'Madonna Smile 18X18 Underpants Port, 15-Ounce',\n",
       " 'Pattern, Smile Sparkling Underpants Port, 15-Ounce',\n",
       " 'Curb Pattern, Joseph Tinted Cynthia Square) 2.0 18.5',\n",
       " 'Puzzle Port, Smile',\n",
       " 'Pattern, Tinted Cynthia 2.0 (2, 18.5 Hosiery',\n",
       " 'Shears best Tinted Smile 8-Inch Cynthia 2.0 (2, 18.5 EXCEED Hosiery BIA Market Astylish Saddle',\n",
       " 'Smile Boys Underpants Port, 15-Ounce',\n",
       " '15-Ounce Port, Patina Smile',\n",
       " 'Tinted Smile Cynthia 2.0 Height 18.5 EXCEED Market BGSD Astylish Saddle',\n",
       " 'Patina Smile Tinted Cynthia 2.0 18.5 EXCEED Market Saddle',\n",
       " 'McAlister Patina Smile Tinted (BLACK) Cynthia 2.0 18.5 Saddle',\n",
       " 'Height Port, String Smile',\n",
       " 'Port, Patina Smile',\n",
       " 'McAlister Shears Tinted Smile various 8-Inch Cynthia 2.0 Height 18.5 EXCEED Market Saddle',\n",
       " 'Shears best Tinted Smile 8-Inch Cynthia 2.0 18.5 EXCEED BIA Dress,IEason Market Astylish Saddle',\n",
       " 'Port, String Smile',\n",
       " 'Shears Tinted Smile 8-Inch Cynthia 2.0 Height 18.5 EXCEED Market Elm Saddle',\n",
       " 'Port, Beverages, Patina Smile',\n",
       " 'McAlister Tinted Smile (BLACK) Cynthia Liquid 2.0 18.5 Wavy Saddle',\n",
       " 'Pattern, Tinted Sparkling Cynthia 2.0 18.5 Biodegradable,',\n",
       " 'Tinted Smile Regular Cynthia 2.0 Height 18.5 Saddle',\n",
       " 'best Tinted Lodge, EZ Cynthia 2.0 stone 18.5 Saddle',\n",
       " 'di Dear',\n",
       " 'McAlister Shears Tinted Smile 8-Inch EZ 2.0 Cynthia 18.5 EXCEED Market Saddle HNNATTA',\n",
       " 'Tinted Smile (BLACK) Cynthia 2.0 Height 18.5 Mount Saddle',\n",
       " 'PJ Tinted Smile Cynthia 2.0 (2, 18.5 EXCEED Hosiery Market Saddle',\n",
       " 'String Shears Tinted Smile 8-Inch Cynthia 2.0 18.5 EXCEED Market Elm Saddle',\n",
       " 'Shears Tinted Smile Freestanding 8-Inch Cynthia 2.0 18.5 EXCEED Market Astylish Saddle',\n",
       " 'McAlister Tinted Smile (BLACK) Cynthia 2.0 Height 18.5 Mount Saddle',\n",
       " 'PJ Tinted Smile Cynthia 2.0 (2, 18.5 EXCEED Hosiery Market Saddle',\n",
       " 'Curb Pattern, Tinted Sparkling Cynthia 2.0 18.5 Wavy',\n",
       " 'Tinted Cynthia EZ 2.0 18.5 Body-Con HNNATTA',\n",
       " 'Backpacking di',\n",
       " 'PJ Tinted Smile Regular Cynthia 2.0 (2, Height 18.5 EXCEED Hosiery Market Saddle',\n",
       " 'Curb Shears Tinted Smile Overall 8-Inch making Cynthia 2.0 Height 18.5 EXCEED Market Saddle',\n",
       " 'Pattern, Tinted Cynthia 2.0 18.5 3000K',\n",
       " 'Height Port, party Smile',\n",
       " 'Curb Pattern, Tinted Bug Cynthia 2.0 18.5',\n",
       " 'Port, Patina Smile',\n",
       " 'party Tinted making Cynthia 2.0 18.5 Fundamental',\n",
       " 'Smile Flattering various Regular Port, Unlisted 6-',\n",
       " 'Tinted 8-Inch Regular Cynthia 2.0 18.5 Saddle Flattering',\n",
       " 'Collectible Shears Pattern, FAKE Tinted Smile 8-Inch Cynthia 2.0 Strauss 18.5 EXCEED Market Saddle Sports',\n",
       " 'McAlister Pattern, Tinted Cynthia 2.0 Strauss 18.5 Drying',\n",
       " 'Pattern, Tinted Cynthia 2.0 18.5 Hosiery',\n",
       " 'Pattern, Tinted Sparkling Cynthia Liquid 2.0 18.5',\n",
       " 'Curb Pattern, Tinted Overall Cynthia 2.0 18.5',\n",
       " 'Patina Smile Tinted Cynthia 2.0 18.5 EXCEED Market Brief(FBA) Saddle',\n",
       " 'Pattern, Tinted Cynthia 2.0 18.5 3000K',\n",
       " 'Curb Tinted Smile Cynthia 2.0 18.5 EXCEED Market Saddle Handcrafted',\n",
       " 'Pattern, Tinted making Cynthia 2.0 18.5 Body-Con',\n",
       " 'Tinted Smile are Sparkling grinderPUNCH Cynthia Underpants 2.0 18.5 Saddle',\n",
       " 'di',\n",
       " 'Curb Patina Smile Tinted Cynthia 2.0 18.5 EXCEED Market Saddle',\n",
       " 'Smile Liquid Underpants Handles Port, 15-Ounce',\n",
       " 'McAlister Pattern, Tinted Knot, Sparkling Cynthia 2.0 18.5']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prodyne Prodyne Acrylic Salad Bowl with Servers <EOS>                                                                    ',\n",
       " '22 Spacious Durable Large Canvas Tote Bag Pocket Pool Beach Shopping Travel Tote Bag Eco-Friendly (1, <EOS>                                                           ',\n",
       " 'Women Hat Lace Head Cover Bonnet Cap Scarf Muslim Hat <EOS>                                                                 ',\n",
       " 'Aluminum 1.2L Outdoor Camping Hiking Kettle Coffee Pot Portable Teapot Kettle, Compact and Lightweight with Silicon Handle <EOS>                                                          ',\n",
       " 'Brand New World Upholstered Toddler Sofa <EOS>                                                                     ',\n",
       " \"Cambridge Select Women's Classic Round Toe Mid Heel Dress Pump <EOS>                                                                 \",\n",
       " 'MODERNO Mens Checkered Shirts Dress Slim Fit Long Sleeve Button down <EOS>                                                                ',\n",
       " \"Montana Men's Quartz Brass and Dress Watch, Tone (Model: <EOS>                                                                  \",\n",
       " 'Cool Touch 2-Slice Toaster, White <EOS>                                                                      ',\n",
       " 'Everest Gym Bag with Wet Pocket <EOS>                                                                     ',\n",
       " '(TM) Tribal Jewelry Flat Round Dangle Coin Tassel Pendant Chain Necklace For Women <EOS>                                                              ',\n",
       " '1:12 Scale Champagne in Silver ICE Bucket <EOS>                                                                    ',\n",
       " 'KINDWER Square Hammered Aluminum Bowl, 14-Inch, Silver <EOS>                                                                    ',\n",
       " \"Vintage Tea Dress 1950's Floral Spring Garden Retro Swing Prom Party Cocktail Dress For Women <EOS>                                                            \",\n",
       " 'Swarovski Crystal Leverback Earrings - 8mm Dangling Bezel Rhodium White Gold Finish <EOS>                                                               ',\n",
       " \"Cuisinart Chef's Classic - 16 Stainless Steel Roaster Pan, Silver <EOS>                                                                 \",\n",
       " 'Melissa & Doug Deluxe Magnetic Standing Art Easel With Chalkboard, Board, and 39 Letter and Number Magnets <EOS>                                                          ',\n",
       " \"MrWonder Men's Casual Slim Fit Long Sleeve Button Down Shirts 100% Cotton Printed Dress Shirts <EOS>                                                            \",\n",
       " \"Daily Ritual Women's Jersey Sleeveless V-Neck Dress <EOS>                                                                    \",\n",
       " \"Shephe Men's 4 Ply Turtleneck Cashmere Sweater <EOS>                                                                    \",\n",
       " \"Michael Kors Women's Silver-Tone Watch <EOS>                                                                      \",\n",
       " 'Screwdriver Bits Precision Set for Watch Clock, Jewelry 11 Piece <EOS>                                                                 ',\n",
       " 'City MVE Shoes Cute Womens Pointed Toe Slip on Ankle Boot Zip Up Low Heel <EOS>                                                            ',\n",
       " 'BIADANI Women Long Sleeve Soft Knit and Open Front Cardigan Sweater <EOS>                                                                ',\n",
       " 'Arctic Cubic Sequin Shiny Glitter Sparkle Gradient Color Black Silver One-Button Blazer Suit Top <EOS>                                                             ',\n",
       " \"Nine West Women's Madelyn Dress <EOS>                                                                      \",\n",
       " 'All-Clad Hard Anodized Nonstick Dishwasher Safe PFOA Free Soup Pot / Stock Pot Cookware, 4-Quart, Black <EOS>                                                           ',\n",
       " 'Flexfit Garment Washed Cap (Assorted Colors) <EOS>                                                                     ',\n",
       " \"Men's Winter Woolen Long Trench Coat Business Outfit Down Jacket <EOS>                                                                 \",\n",
       " 'Fashion Checkered Microfiber Skinny Tie Gift For Holy Saturday By Dan Smith <EOS>                                                               ',\n",
       " \"Alizeal Men's Vintage Smart Casual Designed Knit Tie Necktie-Various Color <EOS>                                                                 \",\n",
       " 'Lapis Blue Glass Accent Table Lamp with USB Port Set of 2 <EOS>                                                               ',\n",
       " \"Women's Retro Slim 3/4 sleeve 8 Colors Business Workwear Cocktail Midi Pencil Dress With <EOS>                                                             \",\n",
       " \"Verdamo Men's Classic Dress Shirt With Convertible Cuffs - Many Colors Available <EOS>                                                               \",\n",
       " \"Women's Sexy Vintage Lace Up High Waist Bodycon Faux Suede Mini Skirt <EOS>                                                               \",\n",
       " 'Zappy 2.5 x 5 Elegant Petite Mini Rectangle Tray Appetizer Dessert Plates - Disposable Hard Plastic Miniature Tasting Sushi Trays Sample Dish Party Plates (White) <EOS>                                                  ',\n",
       " 'Women Sexy Teddy Lingerie One Piece Lace Bodysuit Deep V-Neck Jumpsuit for Women Plus Size <EOS>                                                            ',\n",
       " \"Esteez Women's Knee Length Button down Stretch Denim Jean Skirt <EOS>                                                                 \",\n",
       " 'EVER FAITH Austrian Crystal Flower Ribbon Teardrop Brooch Pendant <EOS>                                                                  ',\n",
       " 'BLACK Oversized Large XL Big Sunglasses Kim Square Flat Aviator Wayfarer Womens <EOS>                                                               ',\n",
       " 'Men Paisley Necktie and Handkerchief Set Jacquard Woven Tie Pocket Square <EOS>                                                                ',\n",
       " \"Men's 2-Button Single-Breasted Dress Blazer Buttons <EOS>                                                                     \",\n",
       " 'Shlax&Wing Ties Necktie Checkered Blue Green Handmade Wedding Brand New Long <EOS>                                                                ',\n",
       " \"Haola Women's Sexy Casual Long Sleeve Short Dress Slim Party Club Mini Dress <EOS>                                                              \",\n",
       " 'Disney Mickey Mouse Distressed Classic T-Shirt <EOS>                                                                     ',\n",
       " \"DREAM PAIRS Women's Over The Knee Thigh High Low Block Heel Boots <EOS>                                                               \",\n",
       " \"Froomer Women's Winter Thick Outerwear Warm Long Fox Faux Fur Coat <EOS>                                                                \",\n",
       " \"Women's Solid Color Classic French Style Beret Beanie Hat <EOS>                                                                  \",\n",
       " \"U.S. Polo Assn. Men's Regular Fit Solid Semi Spread Collar Dress Shirt <EOS>                                                               \",\n",
       " \"Perry Ellis Men's Suit Separate Jacket <EOS>                                                                     \",\n",
       " 'Blue Buffalo Variety Pack Dog Food 12.5 oz Bundle Beige <EOS>                                                                 ',\n",
       " 'Koloa Surf Ladies Living the Wave Satin Jersey Tank Top Size S-XL <EOS>                                                               ',\n",
       " 'Golden State Art, 8.5x11 inches Certificate / Diploma Frame, Color: Black Gold & Burgundy, with Easel Stand for and Real Glass <EOS>                                                      ',\n",
       " \"Dahlia Women's Winter Knit Headband - Button Accented <EOS>                                                                   \",\n",
       " 'Copper Compression Copper Arch Support - 2 Plantar Fasciitis Braces / Sleeves. Highest Copper Foot Care, Heel Feet Pain, Flat (1 Black - One Size Fits <EOS>                                                 ',\n",
       " 'Women Chiffon Long Skirt with Drawstring Floral Maxi Skirts <EOS>                                                                  ',\n",
       " 'Alexander Del Rossa Womens Satin Pajamas, Cami Top Pj Set with Sleep Mask <EOS>                                                              ',\n",
       " \"Goddessvan Men's Autumn Slim Patchwork Sweater Plus Size Hooded Sweatshirts Tops Blouse <EOS>                                                               \",\n",
       " '12mm Green Amethyst Sterling Silver Stacking Ring, size 9 <EOS>                                                                  ',\n",
       " 'Small White Porcelain Serving Bowls - Set of 6 6 - For Ceviche, Appetizers, Snacks, Tapas, Dips, - <EOS>                                                         ',\n",
       " 'Cremation Urn Necklace for Ashes With Beautiful Gift Box Urn Pendant Memorial Keepsake Cremation Jewelry / <EOS>                                                           ',\n",
       " 'Womens Solid Knit Lace Full Slip Dress Extender With Adjustable Straps <EOS>                                                                ',\n",
       " 'Dale Tiffany Butterfly and Peony Tiffany Table Lamp, <EOS>                                                                   ',\n",
       " 'J. Adams Classic Pointed Toe Pumps - Slip On Comfortable Work High Heel - Closed Toe Kitten Heel - Kiera <EOS>                                                       ']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"/home/ubuntu/workspace/ImageCaptioning.pytorch/images/B01FTCX14A.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "img = Image(url='/home/ubuntu/workspace/ImageCaptioning.pytorch/images/B01FTCX14A.jpg')\n",
    "\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 13016])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results[:][:][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
